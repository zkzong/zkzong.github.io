<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[双注册双订阅模式]]></title>
    <url>%2F2022%2F01%2F23%2FSpring%20Cloud%2F%E5%8F%8C%E6%B3%A8%E5%86%8C%E5%8F%8C%E8%AE%A2%E9%98%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[双注册双订阅表示一个Provider应用可以将自身的实例信息注册到多个注册中心上，一个Consumer应用可以订阅到多个注册中心上的服务实例信息。 如图所示，Provider可以把自身的服务实例信息注册到Nacos和Eureka集群上，Consumer发起服务订阅的时候可以从Nacos和Eureka上订阅服务。 1. 双注册双订阅模式分析Spring Cloud自身的编程模型是支持双注册双订阅模式的。在服务注册侧，Spring Cloud各个注册中心都有AutoServiceRegistration的实现类，比如，NacosAutoServiceRegistration和EurekaAutoServiceRegistration实现在类内部完成服务的注册。这些AutoServiceRegistration的实现类都实现了 Lifecycle接口，在start过程中完成服务注册操作。 在服务订阅侧，DiscoveryClient 统一了Spring Cloud服务发现的操作。其中，CompositeDiscoveryClient是一个特殊的DiscoveryClient实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package org.springframework.cloud.client.discovery.composite;import java.util.ArrayList;import java.util.Collections;import java.util.Iterator;import java.util.LinkedHashSet;import java.util.List;import org.springframework.cloud.client.ServiceInstance;import org.springframework.cloud.client.discovery.DiscoveryClient;import org.springframework.core.annotation.AnnotationAwareOrderComparator;public class CompositeDiscoveryClient implements DiscoveryClient &#123; private final List&lt;DiscoveryClient&gt; discoveryClients; public CompositeDiscoveryClient(List&lt;DiscoveryClient&gt; discoveryClients) &#123; AnnotationAwareOrderComparator.sort(discoveryClients); this.discoveryClients = discoveryClients; &#125; public String description() &#123; return "Composite Discovery Client"; &#125; public List&lt;ServiceInstance&gt; getInstances(String serviceId) &#123; if (this.discoveryClients != null) &#123; Iterator var2 = this.discoveryClients.iterator(); while(var2.hasNext()) &#123; DiscoveryClient discoveryClient = (DiscoveryClient)var2.next(); List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(serviceId); if (instances != null &amp;&amp; !instances.isEmpty()) &#123; return instances; &#125; &#125; &#125; return Collections.emptyList(); &#125; public List&lt;String&gt; getServices() &#123; LinkedHashSet&lt;String&gt; services = new LinkedHashSet(); if (this.discoveryClients != null) &#123; Iterator var2 = this.discoveryClients.iterator(); while(var2.hasNext()) &#123; DiscoveryClient discoveryClient = (DiscoveryClient)var2.next(); List&lt;String&gt; serviceForClient = discoveryClient.getServices(); if (serviceForClient != null) &#123; services.addAll(serviceForClient); &#125; &#125; &#125; return new ArrayList(services); &#125; public List&lt;DiscoveryClient&gt; getDiscoveryClients() &#123; return this.discoveryClients; &#125;&#125; 在 getInstances 方法中，会聚合所有的 DiscoveryClient 实现类找到的服务名，也会遍历每个DiscoveryClient查询服务名对应的实例信息。 下面在一个应用里分别加上 Nacos （com.alibaba.cloud:spring-cloud-starter-alibaba-nacos-discovery）和 Eureka（org.springframework.cloud:spring-cloud-starter-netfix-eureka-client）依赖，用来完成双注册双订阅。 应用启动后，会出现以下报错信息： 1234567891011121314***************************APPLICATION FAILED TO START***************************Description:Field autoServiceRegistration in org.springframework.cloud.client.serviceregistry.AutoServiceRegistrationAutoConfiguration required a single bean, but 2 were found: - nacosAutoServiceRegistration: defined by method 'nacosAutoServiceRegistration' in class path resource [com/alibaba/cloud/nacos/registry/NacosServiceRegistryAutoConfiguration.class] - eurekaAutoServiceRegistration: defined by method 'eurekaAutoServiceRegistration' in class path resource [org/springframework/cloud/netflix/eureka/EurekaClientAutoConfiguration.class]Action:Consider marking one of the beans as @Primary, updating the consumer to accept multiple beans, or using @Qualifier to identify the bean that should be consumed 从这个报错信息可以很明显地看出，ServiceRegistryAutoConfiguration 自动化配置类的内部类ServiceRegistryEndpointConfiguration内部依赖一个RegistrationBean，但是在Nacos和Eureka依赖内部分别会构造 NacosRegistration 和EurekaRegistration，这样会出现 ServiceRegistry-EndpointConfiguration 并不知道要注入哪个 Registration Bean 的问题。同理，AutoService-RegistrationAutoConfiguration内部的AutoServiceRegistration Bean也会引起一样的问题。 为了解决这个问题，可以在配置文件里过滤这两个自动化配置类： 1spring.autoconfigure.exclude=org.springframework.cloud.client.serviceregistry.ServiceRegistryAutoConfiguration,org.springframework.cloud.client.serviceregistry.AutoServiceRegistrationAutoConfiguration 加上该配置之后，还需要通过@EnableConfigurationProperties 注解让AutoServiceRegistration-Properties 配置类生效。这是因为所有的AutoServiceRegistration 实现类在构造过程中都需要这个配置类Bean。 有了这两个条件之后，即可享受双注册双订阅模式。 2. 案例：使用双注册双订阅模式将Eureka注册中心迁移到Nacos注册中心假设某公司原先使用 Eureka 作为注册中心，Nacos 开源之后，该公司想把Eureka替换成Nacos注册中心，要求在这个过程中对客户没有任何影响，也不能造成业务损失。 对于这个场景，可以使用双注册双订阅方案来完成任务。如图所示，这是一个3个阶段的过程图。 第1阶段：Eureka作为注册中心，Provider完成服务注册，Consumer完成服务发现。 第2阶段：双注册双订阅的核心阶段，该阶段内部包括以下4个操作。 上线新的Provider（拥有双注册能力），这时Eureka 注册中心的Provider有两个实例。 下线旧的 Provider，下线之后由于新 Provider 也会注册到 Eureka 上，这时旧的Consumer可以找到新Provider的实例。 上线新的 Consumer（拥有双订阅能力），新 Consumer 可以订阅 Nacos 和Eureka 集群的服务实例，这时可以订阅到Nacos上的服务实例。 下线旧的Consumer。 第3阶段：Eureka下线，使用Nacos替换Eureka作为新的注册中心，Provider和Consumer的服务注册和服务发现操作只与Nacos交互。]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何同时启动多个Tomcat服务器]]></title>
    <url>%2F2022%2F01%2F23%2FTomcat%2F%E5%A6%82%E4%BD%95%E5%90%8C%E6%97%B6%E5%90%AF%E5%8A%A8%E5%A4%9A%E4%B8%AATomcat%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[修改conf目录下的server.xml文件的三处： 修改http访问端口（默认为8080端口） 123&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; 修改Shutdown端口（默认为8005端口） 1&lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; 修改JVM启动端口（默认为8009端口） 1&lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; 如果使用IDEA集成tomcat开发项目，需要同时部署两个项目到不同的tomcat并启动，则还需要修改JMXport。 参考：http://www.blogjava.net/allen-zhe/archive/2007/01/15/93981.html]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2022%2F01%2F06%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[JSP自定义标签]]></title>
    <url>%2F2021%2F02%2F02%2FSpring%2FJSP%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A0%87%E7%AD%BE%2F</url>
    <content type="text"><![CDATA[1. JSP自定义标签尽管JSP中内置了许多标签，但有时还是需要根据需求自定义标签。如用逗号和空格格式化数字：&lt;mytags:formatNumber number=&quot;100050.574&quot; format=&quot;#,###.00&quot;/&gt;根据传入的number和format，在JSP页面上格式化这个数字，上面的数字应该被输出为100,050.57。JSTL中没有提供内置的标签，所以只能自定义标签实现。 1.1 JSP自定义标签处理器首先创建一个类，它继承自javax.servlet.jsp.tagext.SimpleTagSupport，并重写doTag()方法。重点是需要为标签的属性设置setter方法。所以定义两个setter方法–setFormat(String format)和setNumber(String number)。SimpleTagSupport类提供方法使我们能获取JspWriter对象并输出数据到response。我们使用DecimalFormat类来生成格式化的字符串并输出到response。12345678910111213141516171819202122232425262728293031323334353637383940414243package com.zkzong.format;import java.io.IOException;import java.text.DecimalFormat;import javax.servlet.jsp.JspException;import javax.servlet.jsp.SkipPageException;import javax.servlet.jsp.tagext.SimpleTagSupport;public class NumberFormatterTag extends SimpleTagSupport &#123; private String format; private String number; public NumberFormatterTag() &#123; &#125; public void setFormat(String format) &#123; this.format = format; &#125; public void setNumber(String number) &#123; this.number = number; &#125; @Override public void doTag() throws JspException, IOException &#123; System.out.println("Number is:" + number); System.out.println("Format is:" + format); try &#123; double amount = Double.parseDouble(number); DecimalFormat formatter = new DecimalFormat(format); String formattedNumber = formatter.format(amount); getJspContext().getOut().write(formattedNumber); &#125; catch (Exception e) &#123; e.printStackTrace(); // stop page from loading further by throwing SkipPageException throw new SkipPageException("Exception in formatting " + number + " with format " + format); &#125; &#125;&#125; 1.2 创建TLD（Tag Library Descriptor）文件一旦标签处理器创建完成，就需要在WEB-INF文件中定义一个TLD文件。当应用部署后容器就会加载它。1234567891011121314151617181920212223&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;taglib xmlns="http://java.sun.com/xml/ns/j2ee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-jsptaglibrary_2_0.xsd" version="2.0"&gt; &lt;description&gt;Number Formatter Custom Tag&lt;/description&gt; &lt;tlib-version&gt;2.1&lt;/tlib-version&gt; &lt;short-name&gt;mytags&lt;/short-name&gt; &lt;uri&gt;http://zkzong.com/jsp/tlds/mytags&lt;/uri&gt; &lt;tag&gt; &lt;name&gt;formatNumber&lt;/name&gt; &lt;tag-class&gt;com.zkzong.format.NumberFormatterTag&lt;/tag-class&gt; &lt;body-content&gt;empty&lt;/body-content&gt; &lt;attribute&gt; &lt;name&gt;format&lt;/name&gt; &lt;required&gt;true&lt;/required&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;number&lt;/name&gt; &lt;required&gt;true&lt;/required&gt; &lt;/attribute&gt; &lt;/tag&gt;&lt;/taglib&gt; 注意：URI必须在tld文件中定义，而且format和number属性也使必需的。 1.3 描述符配置在web.xml文件中配置如下内容：1234567891011121314151617&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app version="2.5" xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd"&gt; &lt;display-name&gt;&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;/welcome-file-list&gt; &lt;jsp-config&gt; &lt;taglib&gt; &lt;taglib-uri&gt;http://zkzong.com/jsp/tlds/mytags&lt;/taglib-uri&gt; &lt;taglib-location&gt;/WEB-INF/numberformatter.tld&lt;/taglib-location&gt; &lt;/taglib&gt; &lt;/jsp-config&gt;&lt;/web-app&gt; taglib-uri的值要和TLD文件中定义的一样。 1.4 在JSP页面中使用自定义标签JSP页面如下：1234567891011121314151617181920212223242526&lt;%@ page language="java" contentType="text/html; charset=US-ASCII" pageEncoding="US-ASCII"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=US-ASCII"&gt;&lt;title&gt;Custom Tag Example&lt;/title&gt;&lt;%@ taglib uri="http://zkzong.com/jsp/tlds/mytags" prefix="mytags"%&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;Number Formatting Example&lt;/h2&gt; &lt;mytags:formatNumber number="100050.574" format="#,###.00" /&gt; &lt;br&gt; &lt;br&gt; &lt;mytags:formatNumber number="1234.567" format="$# ###.00" /&gt; &lt;br&gt; &lt;br&gt; &lt;p&gt; &lt;strong&gt;Thanks You!!&lt;/strong&gt; &lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 如果有多个标签处理器类可以提供JAR文件供使用。只需在JAR文件的META-INF目录下包含TLD文件就可以了，然后把它放在web应用的lib目录下。这样就不用再web.xml中配置了，因为JSP容器会自动处理。这也是为什么我们使用JSP内置标签时不需要再web.xml中配置。 源码下载 参考文章：http://www.journaldev.com/2099/jsp-custom-tags-example-tutorialhttp://www.codeproject.com/Articles/31614/JSP-JSTL-Custom-Tag-Library 2. 如何创建JSTL自定义函数2.1 在/WEB-INF文件夹下创建a.tld文件12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;taglib version="2.1" xmlns="http://java.sun.com/xml/ns/javaee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-jsptaglibrary_2_1.xsd"&gt; &lt;tlib-version&gt;1.0&lt;/tlib-version&gt; &lt;short-name&gt;mytlds&lt;/short-name&gt; &lt;uri&gt;http://www.zkzong.com/myTlds&lt;/uri&gt; &lt;function&gt; &lt;name&gt;charAt&lt;/name&gt; &lt;function-class&gt;com.zkzong.jstl.Functions&lt;/function-class&gt; &lt;function-signature&gt;char charAt(java.lang.String, int)&lt;/function-signature&gt; &lt;/function&gt;&lt;/taglib&gt; 2.2 创建类文件1234567package com.zkzong.jstl;public class Functions &#123; public static char charAt(String input, int index) &#123; return input.charAt(index); &#125;&#125; 2.3 在jsp页面中使用1234567891011121314&lt;%@page contentType="text/html" pageEncoding="UTF-8"%&gt;&lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;%@taglib uri="http://java.sun.com/jsp/jstl/core" prefix="c" %&gt;&lt;%@taglib uri="http://www.zkzong.com/myTlds" prefix="mt" %&gt;&lt;html&gt; &lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;title&gt;JSP Page&lt;/title&gt; &lt;/head&gt; &lt;body&gt; $&#123;mt:charAt("AB",0)&#125; &lt;!-- It will give you A--&gt; &lt;/body&gt;&lt;/html&gt; 源码下载 参考文章：http://www.noppanit.com/how-to-create-a-custom-function-for-jstl/http://findnerd.com/list/view/How-to-create-a-custom-Function-for-JSTL/2869/ tld文件中的uri可以不用配置。如果没有配置，在jsp页面中引入标签时，uri的值就是tld文件的路径，如&lt;%@ taglib prefix=&quot;ex&quot; uri=&quot;WEB-INF/custom.tld&quot;%&gt;。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[自定义标签库]]></title>
    <url>%2F2021%2F02%2F02%2FSpring%2F%E8%87%AA%E5%AE%9A%E4%B9%89%E6%A0%87%E7%AD%BE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[自定义标签是用户定义的JSP元素。当包含自定义标签的JSP页面被解释成Servlet，标签被转化为叫做标签处理器的对象来操作。当JSP页面的servlet执行时，Web容器调用这些操作。由于代码重用的灵活性，它加速了web应用的部署。自定义标签可以访问所有JSP页面中可用的对象。自定义标签可以修改调用页面生成的响应。自定义标签可以嵌套。自定义标签库有一个或多个叫做标签处理器的Java类和一个XML标签库描述文件组成。标签处理器的类需要实现Tag接口或IterationTag接口或BodyTag接口，也可以继承TagSupport类或BodyTagSupport类。所有支持自定义标签的类都在javax.servlet.jsp.tagext包中。 实现一个对输入字符串获取子串的用例。 1. 首先创建标签处理器类。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package com.zkzong.customtag;import javax.servlet.jsp.JspException;import javax.servlet.jsp.JspWriter;import javax.servlet.jsp.tagext.TagSupport;import java.io.IOException;/** * Created by Zong on 2015/6/20. */public class SubstrTagHandler extends TagSupport &#123; private String input; private int start; private int end; @Override public int doStartTag() throws JspException &#123; try &#123; // Get the writer object for output. JspWriter out = pageContext.getOut(); // Perform substr operation on string. out.println(input.substring(start, end)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return SKIP_BODY; &#125; public String getInput() &#123; return input; &#125; public void setInput(String input) &#123; this.input = input; &#125; public int getStart() &#123; return start; &#125; public void setStart(int start) &#123; this.start = start; &#125; public int getEnd() &#123; return end; &#125; public void setEnd(int end) &#123; this.end = end; &#125;&#125; 上面代码我们创建了三个变量：input、start和end。当自定义标签被调用时这些输入可以从JSP文件中得到。 注意：这些参数有getter和setter方法用来设置变量值。 2. 创建标签描述文件（TLD）。在WEB-INF目录下创建文件SubstrDescriptor.tld。123456789101112131415161718192021222324&lt;taglib&gt; &lt;tlib-version&gt;1.0&lt;/tlib-version&gt; &lt;jsp-version&gt;1.1&lt;/jsp-version&gt; &lt;short-name&gt;substr&lt;/short-name&gt; &lt;description&gt;Sample taglib for Substr operation&lt;/description&gt; &lt;uri&gt;http://viralpatel.net/blogs/jsp/taglib/substr&lt;/uri&gt; &lt;tag&gt; &lt;name&gt;substring&lt;/name&gt; &lt;tag-class&gt;com.zkzong.customtag.SubstrTagHandler&lt;/tag-class&gt; &lt;description&gt;Substring function.&lt;/description&gt; &lt;attribute&gt; &lt;name&gt;input&lt;/name&gt; &lt;required&gt;true&lt;/required&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;start&lt;/name&gt; &lt;required&gt;true&lt;/required&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;end&lt;/name&gt; &lt;required&gt;true&lt;/required&gt; &lt;/attribute&gt; &lt;/tag&gt;&lt;/taglib&gt; 上面这个tld文件是用来定义自定义标签的。每个新标签有自己的标签处理类，这个类在标签中配置。标签的name属性是在jsp文件中使用的名字。我们提供了三个标签属性：input、start和end。input是需要解析子串的字符串。start和end分别是开始和结束索引。 3. 根据以上步骤我们已经创建了自定义标签。现在我们在jsp文件中使用它。在web应用中创建indexjsp文件并添加如下代码：12345678&lt;%@ taglib prefix="test" uri="/WEB-INF/SubstrDescriptor.tld" %&gt;&lt;title&gt;JSP Custom Taglib example: Substr function&lt;/title&gt;SUBSTR(GOODMORNING, 1, 6) is&lt;font color="blue"&gt; &lt;test:substring input="GOODMORNING" start="1" end="6"&gt;&lt;/test:substring&gt;&lt;/font&gt; 参考文章：http://viralpatel.net/blogs/tutorial-create-custom-tag-library-taglib-in-jsp/http://www.javatpoint.com/attributes-in-jsp-custom-taghttp://www.devmanuals.com/tutorials/java/jsp/TagSupport.html]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Eclipse快捷键]]></title>
    <url>%2F2021%2F02%2F02%2FIDE%2Feclipse%2FEclipse%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[快捷键 描述 Ctrl+L 快速定位某一行 Alt+← 前一个编辑的页面 Alt+→ 下一个编辑的页面 Alt+↑ 上移当前行 Alt+↓ 下移当前行 Shift+Tab 往回缩进 Ctrl+K 快速向下查找选定的内容 Ctrl+Shift+K 快速向上查找选定的内容 Ctrl+H 全局查找 Ctrl+1 快速创建方法 Ctrl+T 快速显示当前类的继承结构/查找实现该接口的类 Ctrl+O 快速显示OutLine Ctrl+Shift+T 打开Open Type，查找类文件 Ctrl+Shift+R 打开Open Resource，查找文件 Ctrl+Shift+G 查看变量或方法被调用的地方 shift+alt+s，再按r 生成set、get方法 shift+ctrl+o 导入所有没导入的包 shift+ctrl+m 导入你鼠标当前所在的地方的未导入的包 main+Alt+/ main方法 syso+Alt+/ System.out.println();]]></content>
      <categories>
        <category>eclipse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Eclipse中SVN版本比较或查看jar包源码时中文乱码问题]]></title>
    <url>%2F2021%2F02%2F02%2FIDE%2Feclipse%2FEclipse%E4%B8%ADSVN%E7%89%88%E6%9C%AC%E6%AF%94%E8%BE%83%E6%88%96%E6%9F%A5%E7%9C%8Bjar%E5%8C%85%E6%BA%90%E7%A0%81%E6%97%B6%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[解决办法：Window -&gt; Preferences -&gt; General -&gt; Workspace修改Text file encoding为：UTF-8 参考文献：http://blog.csdn.net/stevenprime/article/details/7666418http://jingyan.baidu.com/article/76a7e409d05f58fc3a6e1572.html]]></content>
      <categories>
        <category>eclipse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Eclipse MyEclipse安装FindBugs]]></title>
    <url>%2F2021%2F02%2F02%2FIDE%2Feclipse%2FEclipse%20MyEclipse%E5%AE%89%E8%A3%85FindBugs%2F</url>
    <content type="text"><![CDATA[FindBugs是一个静态分析工具，用来查找Java代码中存在的bugs。 1. 安装FindBugsFindBugs有两种安装方式： 在线安装（Eclipse建议使用此安装方式） 离线安装：下载FingdBugs插件，放入plugins文件夹（MyEclipse建议使用此安装方式） 2. Eclipse在线安装FindBugs 打开Help -&gt; Install New Software，点击Add按钮，弹出如下对话框Location的值：http://findbugs.cs.umd.edu/eclipse按照提示安装，完成之后重启即可。 3. MyEclipse使用插件安装FindBugs 首先下载FindBugs插件，本文提供一个下载链接：http://downloads.sourceforge.net/project/findbugs/findbugs%20eclipse%20plugin/1.3.9/edu.umd.cs.findbugs.plugin.eclipse_1.3.9.20090821.zip?use_mirror=ncu，也可去官网下载。 解压下载的文件，获取如下文件夹 把该文件夹拷贝到MyEclipse安装路径的Common/plugins目录下。 修改bundles.info文件，该文件位于D:\MyEclipse\MyEclipse 10\configuration\org.eclipse.equinox.simpleconfigurator目录下。在bundles.info最后一行添加edu.umd.cs.findbugs.plugin.eclipse,1.3.9.20090821,file:/D:/MyEclipse/Common/plugins/edu.umd.cs.findbugs.plugin.eclipse_1.3.9.20090821/,4,false 完成之后重启即可。 4. 使用 在需要查找bug的java文件、包或项目上点击右键，选择Find Bugs。 在Bug Explorer中查看相关的bug情况。如果没有找到Bug Explorer可以通过Window -&gt; Show View -&gt; Other打开。 参考文章：http://chenzhou123520.iteye.com/blog/1313565http://luckykapok918.blog.163.com/blog/static/2058650432012101394245604/http://www.cnblogs.com/kayfans/archive/2012/06/18/2554022.html]]></content>
      <categories>
        <category>eclipse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[eclipse提交代码至GitHub]]></title>
    <url>%2F2021%2F02%2F02%2FIDE%2Feclipse%2Feclipse%E6%8F%90%E4%BA%A4%E4%BB%A3%E7%A0%81%E8%87%B3GitHub%2F</url>
    <content type="text"><![CDATA[作为一名程序员，自己在学习时经常需要写代码，但是由于换电脑或其他原因这些代码可能丢失，不方便以后的查看和复习。如果有一个版本服务器，不仅能把上传代码，在需要是可以随时下载，而且能实现版本控制，查看每个版本做了哪些修改。这时GitHub是个不错的选择。 要使用GitHub首先需要注册一个GitHub账号，并创建一个Repository。这已基本成为每个程序员的必备技能，在此就不赘述了。 在eclipse上安装git插件首先选择Help -&gt; Install New Software：弹出如下窗口，点击Add按钮：弹出如下窗口，输入相应内容：Name的值可以任意输入，建议见名知义；Location的值为http://download.eclipse.org/egit/updates。往下选择默认的就ok了。安装完成之后需要重启eclipse。需要在Window -&gt; Preferences -&gt; Team -&gt; Git -&gt; Configuration中配置GitHub的用户信息。 在eclipse中创建Java项目（本文以Java项目为例，其他项目与此类似）。在项目名字上右键选择Team -&gt; Share Project：选择Git，点击下一步：第一次时需要勾选Use or create repository in parent folder of project选中项目，点击Create Repository完成后就在本机上创建了一个Git仓库。此时项目中文件会显示问号小图标。此时就可以把代码提交到本地仓库了，在项目上右键选择Team -&gt; Commit可以选择某个文件提交，也可以选择全部提交。Commit message为必填项。点击Commit按钮就可以把代码提交到本地仓库。当然也可以点击Commit and Push按钮提交代码到本地仓库并上传至GitHub。如果点击的是Commit按钮，接下来就要把代码Push到GitHub上。右键项目选择Team -&gt; Remote -&gt; Push输入之前在GitHub上创建的Repository的URIHost和Repository path会自动生成，不需要输入。User和Password需要输入。下一步选择分支，此处选择master而不是HEAD。然后点击Add Spec点击完成，上传成功。此时在GitHub上查看代码是否已经上传。如果没有上传成功，可能是上一步没有勾选Force Update。建议每次上传都勾选。至此全部完成。 从GitHub上clone代码File -&gt; Import -&gt; Git -&gt; Projects from Git -&gt; Clone URI]]></content>
      <categories>
        <category>eclipse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyEclipse web项目导入IntelliJ并部署运行]]></title>
    <url>%2F2021%2F02%2F02%2FIDE%2Feclipse%2FMyEclipse%20web%E9%A1%B9%E7%9B%AE%E5%AF%BC%E5%85%A5IntelliJ%E5%B9%B6%E9%83%A8%E7%BD%B2%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[1. 导入MyEclipse项目File ｜ Project from Existing Sources... | 选择项目 | Import project from external model | Eclipse或File | Open 2. 配置artifactFile | Project Structure | Artifacts | + | Web Application | From Modules 3. 部署到Tomcat Run | Edit Configurations | + | Tomcat Server | Local or Remote Server | Application server Deployment | + | Artifact 支持热部署，修改如下配置： 参考文章：http://my.oschina.net/tsl0922/blog/94621http://www.php-note.com/article/detail/854]]></content>
      <categories>
        <category>eclipse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[eclipse插件]]></title>
    <url>%2F2021%2F02%2F02%2FIDE%2Feclipse%2Feclipse%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[Spket JavaScript EditorPoperties Editor http://propedit.sourceforge.jp/eclipse/updates/findbugschechstyle]]></content>
      <categories>
        <category>eclipse</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在IntelliJ上操作GitHub]]></title>
    <url>%2F2020%2F12%2F31%2FIDE%2FIDEA%2F%E5%9C%A8IntelliJ%E4%B8%8A%E6%93%8D%E4%BD%9CGitHub%2F</url>
    <content type="text"><![CDATA[IntelliJ IDEA集成了对GitHub的支持，使上传代码到GitHub和从GitHub下载代码更加方便快捷。 1. 分享代码到GitHub 首先需要在IntelliJ配置Git，如果没有正确配置会出现如下错误： 通过File-&gt;Settings打开设置面板进行设置，如图： 第一次上传代码到GitHub操作如下： 其间需要输入用户名和密码。 非第一次上传代码，需要像使用Git命令一样，遵循Add-&gt;Commit-&gt;Push的方式。如图： 其中Add这一步可以省略，直接Commit-&gt;Push。 2. 从GitHub上clone代码 首先选择File-&gt;New-&gt;Project from Version Control-&gt;GitHub 上步操作会打开如下界面： 在Git Repository URL输入需要clone的项目的url即可。]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA集成svn]]></title>
    <url>%2F2020%2F12%2F31%2FIDE%2FIDEA%2FIntelliJ%20IDEA%E9%9B%86%E6%88%90svn%2F</url>
    <content type="text"><![CDATA[Eclipse集成svn相信大家已经非常熟悉了，但是IntelliJ IDEA如何集成svn呢？下面简单介绍一下（其他的版本控制工具的集成类似）： 首先配置下载并配置svn软件，推荐使用SlikSvn。 在IntelliJ IDEA集成svn，选择Subversion。 配置完成之后就可以从svn服务器上checkout代码。 修改代码之后，在该文件或目录上右键点击，提交代码。]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis基础篇]]></title>
    <url>%2F2020%2F12%2F02%2FRedis%2FRedis%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[关系型数据库的特点： 1、它以表格的形式，基于行存储数据，是一个二维的模式。2、它存储的是结构化的数据，数据存储有固定的模式（schema），数据需要适应表结构。3、表与表之间存在关联（Relationship）。4、大部分关系型数据库都支持 SQL（结构化查询语言）的操作，支持复杂的关联查询。5、通过支持事务（ACID 酸）来提供严格或者实时的数据一致性。 但是使用关系型数据库也存在一些限制，比如：1、要实现扩容的话，只能向上（垂直）扩展，比如磁盘限制了数据的存储，就要扩 大磁盘容量，通过堆硬件的方式，不支持动态的扩缩容。水平扩容需要复杂的技术来实 现，比如分库分表。2、表结构修改困难，因此存储的数据格式也受到限制。3、在高并发和高数据量的情况下，我们的关系型数据库通常会把数据持久化到磁盘， 基于磁盘的读写压力比较大。 非关系型数据库的特点： 1、存储非结构化的数据，比如文本、图片、音频、视频。2、表与表之间没有关联，可扩展性强。3、保证数据的最终一致性。遵循 BASE（碱）理论。 Basically Available（基本 可用）； Soft-state（软状态）； Eventually Consistent（最终一致性）。4、支持海量数据的存储和高并发的高效读写。5、支持分布式，能够对数据进行分片存储，扩缩容简单。 对于不同的存储类型，我们又有各种各样的非关系型数据库，比如有几种常见的类型：1、KV 存储，用 Key Value 的形式来存储数据。比较常见的有 Redis 和 MemcacheDB。2、文档存储，MongoDB。3、列存储，HBase。4、图存储，这个图（Graph）是数据结构，不是文件格式。Neo4j。5、对象存储。6、XML 存储等等等等。 Redis 的特性： 1）更丰富的数据类型2）进程内与跨进程；单机与分布式3）功能丰富：持久化机制、过期策略4）支持多种编程语言5）高可用，集群 默认有 16 个库（0-15），可以在配置文件中修改，默认使用第一个 db0。因为没有完全隔离，不像数据库的 database，不适合把不同的库分配给不同的业务使用。 Redis 是字典结构的存储方式，采用 key-value 存储。key 和 value 的最大长度限制 是 512M。 数据类型Redis 一共有几种数据类型？（注意是数据类型不是数据结构）String、Hash、Set、List、Zset、Hyperloglog、Geo、Streams String 字符串可以用来存储字符串、整数、浮点数。 存储（实现）原理以set hello word 为例，因为 Redis 是 KV 的数据库，它是通过 hashtable 实现的（我 们把这个叫做外层的哈希）。所以每个键值对都会有一个 dictEntry（源码位置：dict.h）， 里面指向了 key 和 value 的指针。next 指向下一个 dictEntry。key 是字符串，但是 Redis 没有直接使用 C 的字符数组，而是存储在自定义的 SDS 中。value 既不是直接作为字符串存储，也不是直接存储在 SDS 中，而是存储在 redisObject 中。实际上五种常用的数据类型的任何一种，都是通过 redisObject 来存储 的。 字符串类型的内部编码有三种：1、int，存储 8 个字节的长整型（long，2^63-1）。2、embstr， 代表 embstr 格式的 SDS（Simple Dynamic String 简单动态字符串）， 存储小于 44 个字节的字符串。3、raw，存储大于 44 个字节的字符串（3.2 版本之前是 39 字节）。 应用场景缓存 数据共享分布式（spring-session-data-redis） 分布式锁（setnx） 全局 ID（INCRBY） 计数器（INCR） 限流（INCR） 位统计 Hash 哈希包含键值对的无序散列表。value 只能是字符串，不能嵌套其他类型。 同样是存储字符串，Hash 与 String 的主要区别？1、把所有相关的值聚集到一个 key 中，节省内存空间2、只使用一个 key，减少 key 冲突3、当需要批量获取值的时候，只需要使用一个命令，减少内存/IO/CPU 的消耗 Hash 不适合的场景：1、Field 不能单独设置过期时间2、没有 bit 操作3、需要考虑数据量分布的问题（value 值非常大的时候，无法分布到多个节点）]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Lombok常用注解]]></title>
    <url>%2F2019%2F07%2F21%2FLombok%2FLombok%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[和其他语言相比， Java经常因为不必要的冗长被批评。 Lombok提供了一系列注解用以在后台生成模板代码，将其从你的类中删除，从而有助于保持你的代码整洁。较少的模板意味着更简洁的代码，更易于阅读和维护。在本文中，我将涉及我经常使用的 Lombok功能，并向你展示如何使用他们生产更清晰、更简洁的代码。 1. @NonNull对方法参数进行 null 检查通常不是一个坏主意，特别是如果该方法形成的 API被其他开发者使用。虽然这些检查很简单，但是他们可能变得冗长，特别是当你有多个参数时。如下所示，额外的代码无助于可读性，并且可能从方法的主要目的分散注意力。 123456789101112public void nonNullDemo(Employee employee, Account account) &#123; if (employee == null) &#123; throw new IllegalArgumentException("Employee is marked @NonNull but is null"); &#125; if (account == null) &#123; throw new IllegalArgumentException("Account is marked @NonNull but is null"); &#125; // do stuff&#125; 理想情况下，你需要 null 检查——没有干扰的那种。这就是 @NonNull发挥作用的地方。通过用@NonNull标记参数，Lombok替你为该参数生成 null 检查。你的方法突然变得更加简洁，但没有丢失那些安全性的 null 检查。 12345public void nonNullDemo(@NonNull Employee employee, @NonNull Account account) &#123; // just do stuff&#125; 默认情况下， Lombok会抛出 NullPointerException，如果你愿意，可以配置 Lombok抛出 IllegalArgumentException。我个人更喜欢 IllegalArgumentException，因为我认为它更适合于对参数检查。 2. 更简洁的数据类数据类是 Lombok 真正有助于减少模板代码的领域。在查看该选项前，思考一下我们经常需要处理的模板种类。数据类通常包括以下一种或全部： 构造函数（有或没有参数） 私有成员变量的 getter 方法 私有非 final 成员变量的 setter 方法 帮助记录日志的 toString 方法 equals 和 hashCode（处理相等/集合） 可以通过 IDE 生成以上内容，因此问题不在于编写他们花费的时间。问题是带有少量成员变量的简单类很快会变得非常冗长。让我们看看 Lombok 如何通过处理上述的每一项来减少混乱。 3.1 @Getter 和 @Setter想想下面的 Car 类。当生成 getter 和 setter 时，我们会得到接近 50 行代码来描述一个包含 5 个成员变量的类。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class Car &#123; private String make; private String model; private String bodyType; private int yearOfManufacture; private int cubicCapacity; public String getMake() &#123; return make; &#125; public void setMake(String make) &#123; this.make = make; &#125; public String getModel() &#123; return model; &#125; public void setModel(String model) &#123; this.model = model; &#125; public String getBodyType() &#123; return bodyType; &#125; public void setBodyType(String bodyType) &#123; this.bodyType = bodyType; &#125; public int getYearOfManufacture() &#123; return yearOfManufacture; &#125; public void setYearOfManufacture(int yearOfManufacture) &#123; this.yearOfManufacture = yearOfManufacture; &#125; public int getCubicCapacity() &#123; return cubicCapacity; &#125; public void setCubicCapacity(int cubicCapacity) &#123; this.cubicCapacity = cubicCapacity; &#125;&#125; Lombok可以替你生成 getter和 setter模板。通过对每个成员变量使用 @Getter和 @Setter注解，你最终得到一个等效的类，如下所示： 1234567891011121314151617181920212223public class Car &#123; @Getter @Setter private String make; @Getter @Setter private String model; @Getter @Setter private String bodyType; @Getter @Setter private int yearOfManufacture; @Getter @Setter private int cubicCapacity;&#125; 注意，你只能在非 final 成员变量上使用 @Setter，在 final成员变量上使用将导致编译错误。 如果你需要为每个成员变量生成 getter和 setter，你也可以在类级别使用 @Getter和 @Setter，如下所示。 123456789101112131415@Getter@Setterpublic class Car &#123; private String make; private String model; private String bodyType; private int yearOfManufacture; private int cubicCapacity;&#125; 3.2 @AllArgsConstructor数据类通常包含一个构造函数，它为每个成员变量接受参数。IDE 为 Car 生成的构造函数如下所示： 1234567891011121314151617181920212223242526272829303132public class Car &#123; @Getter @Setter private String make; @Getter @Setter private String model; @Getter @Setter private String bodyType; @Getter @Setter private int yearOfManufacture; @Getter @Setter private int cubicCapacity; public Car(String make, String model, String bodyType, int yearOfManufacture, int cubicCapacity) &#123; super(); this.make = make; this.model = model; this.bodyType = bodyType; this.yearOfManufacture = yearOfManufacture; this.cubicCapacity = cubicCapacity; &#125;&#125; 我们可以使用 @AllArgsConstructor 注解实现同样功能。 使用 @Getter 和 @Setter、 @AllArgsConstructor可以减少模板，保持类更干净且更简洁。 123456789101112131415161718192021222324@AllArgsConstructorpublic class Car &#123; @Getter @Setter private String make; @Getter @Setter private String model; @Getter @Setter private String bodyType; @Getter @Setter private int yearOfManufacture; @Getter @Setter private int cubicCapacity;&#125; 还有其他选项用于生成构造函数。 @RequiredArgsConstructor 将创建带有每个 final成员变量参数的构造函数， @NoArgsConstructor将创建没有参数的构造函数。 3.3 @ToString在你的数据类上覆盖 toString方法是有助于记录日志的良好实践。IDE 为 Car类生成的 toString方法如下所示： 1234567891011121314151617181920212223242526272829303132@AllArgsConstructorpublic class Car &#123; @Getter @Setter private String make; @Getter @Setter private String model; @Getter @Setter private String bodyType; @Getter @Setter private int yearOfManufacture; @Getter @Setter private int cubicCapacity; @Override public String toString() &#123; return "Car [make=" + make + ", model=" + model + ", bodyType=" + bodyType + ", yearOfManufacture=" + yearOfManufacture + ", cubicCapacity=" + cubicCapacity + "]"; &#125;&#125; 我们可以使用 ToString注解替换这个，如下所示： 12345678910111213141516171819@ToString@AllArgsConstructorpublic class Car &#123; @Getter @Setter private String make; @Getter @Setter private String model; @Getter @Setter private String bodyType; @Getter @Setter private int yearOfManufacture; @Getter @Setter private int cubicCapacity;&#125; 默认情况下，Lombok 生成包含所有成员变量的 toString 方法。可以通过 exclude 属性 @ToString(exclude={&quot;someField&quot;},&quot;someOtherField&quot;}) 覆盖行为将某些成员变量排除。 3.4 @EqualsAndHashCode如果你正在将你的数据类和任何类型的对象比较，则需要覆盖 equals和 hashCode 方法。对象的相等是基于业务规则定义的。举个例子，在 Car类中，如果两个对象有相同的 make、 model和 bodyType，我可能认为他们是相等的。如果我使用 IDE 生成 equals 方法检查 make、 model 和 bodyType，它看起来会是这样： 1234567891011121314151617181920212223242526@Overridepublic boolean equals(Object obj) &#123; if (this == obj) return true; if (obj == null) return false; if (getClass() != obj.getClass()) return false; Car other = (Car) obj; if (bodyType == null) &#123; if (other.bodyType != null) return false; &#125; else if (!bodyType.equals(other.bodyType)) return false; if (make == null) &#123; if (other.make != null) return false; &#125; else if (!make.equals(other.make)) return false; if (model == null) &#123; if (other.model != null) return false; &#125; else if (!model.equals(other.model)) return false; return true;&#125; 等价的 hashCode 实现如下所示： 123456789@Overridepublic int hashCode() &#123; final int prime = 31; int result = 1; result = prime * result + ((bodyType == null) ? 0 : bodyType.hashCode()); result = prime * result + ((make == null) ? 0 : make.hashCode()); result = prime * result + ((model == null) ? 0 : model.hashCode()); return result;&#125; 虽然 IDE 处理了繁重的工作，但我们在类中仍然有大量的模板代码。 Lombok允许我们使用 @EqualsAndHashCode 类注解实现相同的功能，如下所示： 1234567891011121314151617181920@ToString@AllArgsConstructor@EqualsAndHashCode(exclude = &#123;"yearOfManufacture", "cubicCapacity"&#125;)public class Car &#123; @Getter @Setter private String make; @Getter @Setter private String model; @Getter @Setter private String bodyType; @Getter @Setter private int yearOfManufacture; @Getter @Setter private int cubicCapacity;&#125; 默认情况下，@EqualsAndHashCode 会创建包含所有成员变量的 equals 和 hashCode 方法。 exclude选项可用于通知 Lombok排除某些成员变量。在上面的代码片段中。我已经从生成的 equals和 hashCode方法中排除了 yearOfManuFacture 和 cubicCapacity。 3.5 @Data如果你想使数据类尽可能精简，可以使用 @Data 注解。 @Data 是 @Getter、 @Setter、 @ToString、 @EqualsAndHashCode 和 @RequiredArgsConstructor 的快捷方式。 1234567891011121314151617181920@ToString@RequiredArgsConstructor@EqualsAndHashCode(exclude = &#123;"yearOfManufacture", "cubicCapacity"&#125;)public class Car &#123; @Getter @Setter private String make; @Getter @Setter private String model; @Getter @Setter private String bodyType; @Getter @Setter private int yearOfManufacture; @Getter @Setter private int cubicCapacity;&#125; 通过使用 @Data，我们可以将上面的类精简如下： 12345678@Datapublic class Car &#123; private String make; private String model; private String bodyType; private int yearOfManufacture; private int cubicCapacity;&#125; 4. 使用 @Buidler 创建对象建造者设计模式描述了一种灵活的创建对象的方式。 Lombok可以帮你轻松的实现该模式。看一个使用简单 Car 类的示例。假设我们希望可以创建各种 Car对象，但我们希望在创建时设置的属性具有灵活性。 123456789@AllArgsConstructorpublic class Car &#123; private String make; private String model; private String bodyType; private int yearOfManufacture; private int cubicCapacity; private List&lt;LocalDate&gt; serviceDate;&#125; 假设我们要创建一个 Car，但只想设置 make和 model。在 Car上使用标准的全参数构造函数意味着我们只提供 make和 model并设置其他参数为 null。 1Car2 car2 = new Car2("Ford", "Mustang", null, null, null, null); 这可行但并不理想，我们必须为我们不感兴趣的参数传递 null。我们可以创建一个只接受 make和 model的构造函数来避开这个问题。这是一个合理的解决方法，但不够灵活。如果我们有许多不同的字段排列，我们怎么来创建一个新 Car？最终我们得到了一堆不同的构造函数，代表了我们可以实例化 Car的所有可能方式。 解决该问题的一种干净、灵活的方式是使用建造者模式。 Lombok通过 @Builder 注解帮你实现建造者模式。当你使用 @Builder 注解 Car 类时， Lombok会执行以下操作： 添加一个私有构造函数到 Car 创建一个静态的 CarBuilder 类 在 CarBuilder 中为 Car 中的每个成员创建一个 setter风格方法 在 CarBuilder中添加创建 Car 的新实例的建造方法 CarBuilder上的每个 setter 风格方法返回自身的实例（ CarBuilder）。这允许你进行方法链式调用并为对象创建提供流畅的 API。让我们看看它如何使用。 1234Car muscleCar = Car.builder().make("Ford") .model("mustang") .bodyType("coupe") .build(); 现在只使用 make 和 model 创建 Car 比之前更简洁了。只需在 Car 上简单的调用生成的 builder 方法获取 CarBuilder 实例，然后调用任何我们感兴趣的 setter 风格方法。最后，调用 build 创建 Car 的新实例。 另一个值得一提的方便的注解是 @Singular。默认情况下，Lombok 为集合创建使用集合参数的标准的 setter 风格方法。在下面的例子中，我们创建了新的 Car 并设置了服务日期列表。 12345678910@Builderpublic class Car &#123; private String make; private String model; private String bodyType; private int yearOfManufacture; private int cubicCapacity; @Singular private List&lt;LocalDate&gt; serviceDate;&#125; 向集合成员变量添加 @Singular 将提供一个额外的方法，允许你向集合添加单个项。 12345Car muscleCar3 = Car.builder() .make("Ford") .model("mustang") .serviceDate(LocalDate.of(2016, 5, 4)) .build(); 现在我们可以添加单个服务日期，如下所示： 12345Car muscleCar3 = Car.builder() .make("Ford") .model("mustang") .serviceDate(LocalDate.of(2016, 5, 4)) .build(); 这是一个有助于在创建对象期间处理集合时保持代码简洁的快捷方法。 5. 日志Lombok另一个伟大的功能是日志记录器。如果没有 Lombok，要实例化标准的 SLF4J日志记录器，通常会有以下内容： 12345678public class SomeService &#123; private static final org.slf4j.Logger log = org.slf4j.LoggerFactory.getLogger(LogExample.class); public void doStuff() &#123; log.debug("doing stuff...."); &#125;&#125; 这些日志记录器很沉重，并为每个需要日志记录的类添加了不必要的混乱。值得庆幸的是 Lombok提供了一个为你创建日志记录器的注解。你要做的所有事情就是在类上添加注解，这样就可以了。 123456@Slf4jpublic class SomeService &#123; public void doStuff() &#123; log.debug("doing stuff...."); &#125;&#125; 我在这里使用了 @SLF4J注解，但 Lombok能为几乎所有通用 Java日志框架生成日志记录器。有关更多日志记录器的选项，请参阅文档。 我非常喜欢 Lombok 的一点是它的不侵入性。如果你决定在使用如 @Getter、 @Setter 或 @ToString 时也想要自己的方法实现，你的方法将总是优先于 Lombok。它允许你在大多数时间使用 Lombok，但在你需要的时候仍有控制权。]]></content>
      <categories>
        <category>Lombok</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[全解史上最快的JOSN解析库 - alibaba Fastjson]]></title>
    <url>%2F2019%2F07%2F11%2FJSON%2F%E5%85%A8%E8%A7%A3%E5%8F%B2%E4%B8%8A%E6%9C%80%E5%BF%AB%E7%9A%84JOSN%E8%A7%A3%E6%9E%90%E5%BA%93%20-%20alibaba%20Fastjson%2F</url>
    <content type="text"><![CDATA[JSON，全称：JavaScript Object Notation，作为一个常见的轻量级的数据交换格式，应该在一个程序员的开发生涯中是常接触的。简洁和清晰的层次结构使得 JSON 成为理想的数据交换语言。 易于阅读和编写，同时也易于机器解析和生成，并有效地提升网络传输效率。 Java是面向对象的语言，所以我们更多的在项目中是以对象的形式处理业务的，但是在传输的时候我们却要将对象转换为 JSON 格式便于传输，而且 JSON 格式一般能解析为大多数的对象格式，而不在乎编程语言。 现在主流的对象与 JSON 互转的工具很多，我们主要介绍今天的主角，阿里巴巴的开源库 - Fastjson。Fastjson是一个Java库，可用于将Java对象转换为其JSON表示。它还可用于将JSON字符串转换为等效的Java对象。Fastjson可以处理任意Java对象，包括您没有源代码的预先存在的对象。 1. 什么是 Fastjson?阿里官方给的定义是， fastjson 是阿里巴巴的开源JSON解析库，它可以解析 JSON 格式的字符串，支持将 Java Bean 序列化为 JSON 字符串，也可以从 JSON 字符串反序列化到 JavaBean。 2. Fastjson 的优点 速度快fastjson相对其他JSON库的特点是快，从2011年fastjson发布1.1.x版本之后，其性能从未被其他Java实现的JSON库超越。 使用广泛fastjson在阿里巴巴大规模使用，在数万台服务器上部署，fastjson在业界被广泛接受。在2012年被开源中国评选为最受欢迎的国产开源软件之一。 测试完备fastjson有非常多的testcase，在1.2.11版本中，testcase超过3321个。每次发布都会进行回归测试，保证质量稳定。 使用简单fastjson的 API 十分简洁。 功能完备支持泛型，支持流处理超大文本，支持枚举，支持序列化和反序列化扩展。 3. 怎么获得 Fastjson你可以通过如下地方下载fastjson: maven中央仓库: http://central.maven.org/maven2/com/alibaba/fastjson/ Sourceforge.net : https://sourceforge.net/projects/fastjson/files/ 在maven项目的pom文件中直接配置fastjson依赖 fastjson最新版本都会发布到maven中央仓库，你可以直接依赖。 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;x.x.x&lt;/version&gt;&lt;/dependency&gt; 其中x.x.x是版本号，根据需要使用特定版本，建议使用最新版本。 4. Fastjson 主要的APIFastjson入口类是 com.alibaba.fastjson.JSON，主要的 API 是 JSON.toJSONString 和 parseObject。 1234567package com.alibaba.fastjson;public abstract class JSON &#123; // Java对象转换为JSON字符串 public static final String toJSONString(Object object); //JSON字符串转换为Java对象 public static final &lt;T&gt; T parseObject(String text, Class&lt;T&gt; clazz, Feature... features);&#125; 序列化： 1String jsonString = JSON.toJSONString(obj); 反序列化： 1VO vo = JSON.parseObject(&quot;...&quot;, VO.class); 泛型反序列化： 123import com.alibaba.fastjson.TypeReference;List&lt;VO&gt; list = JSON.parseObject(&quot;...&quot;, new TypeReference&lt;List&lt;VO&gt;&gt;() &#123;&#125;); 5. Fastjson 的性能fastjson是目前java语言中最快的json库，比自称最快的jackson速度还要快，第三方独立测试结果看这里：https://github.com/eishay/jvm-serializers/wiki。 自行做性能测试时，需关闭循环引用检测的功能。 12JSON.toJSONString(obj, SerializerFeature.DisableCircularReferenceDetect)VO vo = JSON.parseObject(&quot;...&quot;, VO.class, Feature.DisableCircularReferenceDetect) 另外，Fastjson 比 Gson 快大约6倍，测试结果可以看这里： 123456789101112131415161718192021222324Checking correctness…[done]Pre-warmup… java-built-in hessian kryo protostuff-runtime avro-generic msgpack json/jackson/databind json/jackson/databind-strings json/jackson/db-afterburner json/google-gson/databind json/svenson-databind json/flexjson/databind json/fastjson/databind smile/jackson/databind smile/jackson/db-afterburner smile/protostuff-runtime bson/jackson/databind xml/xstream+c xml/jackson/databind-aalto[done]pre. create ser deser shal +deep total size +dfljava-built-in 63 5523 27765 28084 28162 33686 889 514hessian 64 3776 6459 6505 6690 10466 501 313kryo 63 809 962 937 1001 1810 214 133protostuff-runtime 62 671 903 920 957 1627 241 151avro-generic 436 1234 1122 1416 1760 2994 221 133msgpack 61 789 1369 1385 1449 2238 233 146json/jackson/databind 60 1772 3089 3113 3246 5018 485 261json/jackson/databind-strings 64 2346 3739 3791 3921 6267 485 261json/jackson/db-afterburner 64 1482 2220 2233 2323 3805 485 261json/google-gson/databind 64 7076 4894 4962 5000 12076 486 259json/svenson-databind 64 5422 12387 12569 12468 17890 495 266json/flexjson/databind 62 20923 26853 26873 27272 48195 503 273json/fastjson/databind 63 1250 1208 1206 1247 2497 486 262smile/jackson/databind 60 1697 2117 2290 2298 3996 338 241smile/jackson/db-afterburner 60 1300 1614 1648 1703 3003 352 252smile/protostuff-runtime 61 1275 1612 1638 1685 2961 335 235bson/jackson/databind 63 5151 6729 6977 6918 12069 506 286xml/xstreamc 62 6358 13208 13319 13516 19874 487 244xml/jackson/databind-aalto 62 2955 5332 5465 5584 8539 683 286 6. Fastjson 使用示例我们创建一个班级的对象，和一个学生对象如下： 班级对象 123456789101112131415161718192021public class Grade &#123; private Long id; private String name; private List&lt;Student&gt; users = new ArrayList&lt;Student&gt;(); // 省略 setter、getter public void addStudent(Student student) &#123; users.add(student); &#125; @Override public String toString() &#123; return &quot;Grade&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &quot;, users=&quot; + users + &apos;&#125;&apos;; &#125;&#125; 学生对象 123456789101112131415public class Student &#123; private Long id; private String name; // 省略 setter、getter @Override public String toString() &#123; return &quot;Student&#123;&quot; + &quot;id=&quot; + id + &quot;, name=&apos;&quot; + name + &apos;\&apos;&apos; + &apos;&#125;&apos;; &#125;&#125; 运行的 Main 函数 123456789101112131415161718192021222324252627public class MainTest &#123; public static void main(String[] args) &#123; Grade group = new Grade(); group.setId(0L); group.setName(&quot;admin&quot;); Student student = new Student(); student.setId(2L); student.setName(&quot;guest&quot;); Student rootUser = new Student(); rootUser.setId(3L); rootUser.setName(&quot;root&quot;); group.addStudent(student); group.addStudent(rootUser); // 转换为 JSON String jsonString = JSON.toJSONString(group); System.out.println(&quot;JSON字符串：&quot; + jsonString); // 转换为 对象BEAN Grade grade = JSON.parseObject(jsonString, Grade.class); System.out.println(&quot;JavaBean对象：&quot; + grade); &#125;&#125; 最后的运行结果如下： 12345JSON字符串：&#123;&quot;id&quot;:0,&quot;name&quot;:&quot;admin&quot;,&quot;users&quot;:[&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;guest&quot;&#125;,&#123;&quot;id&quot;:3,&quot;name&quot;:&quot;root&quot;&#125;]&#125;JavaBean对象：Grade&#123;id=0, name=&apos;admin&apos;, users=[Student&#123;id=2, name=&apos;guest&apos;&#125;, Student&#123;id=3, name=&apos;root&apos;&#125;]&#125; 7. 将对象中的空值输出在fastjson中，缺省是不输出空值的。无论Map中的null和对象属性中的null，序列化的时候都会被忽略不输出，这样会减少产生文本的大小。但如果需要输出空值怎么做呢？ 如果你需要输出空值，需要使用 SerializerFeature.WriteMapNullValue 12Model obj = ...;JSON.toJSONString(obj, SerializerFeature.WriteMapNullValue); 几种空值特别处理方式： SerializerFeature 描述 WriteNullListAsEmpty 将Collection类型字段的字段空值输出为[] WriteNullStringAsEmpty 将字符串类型字段的空值输出为空字符串 “” WriteNullNumberAsZero 将数值类型字段的空值输出为0 WriteNullBooleanAsFalse 将Boolean类型字段的空值输出为false 具体的示例参考如下，可以同时选择多个： 1234567class Model &#123; public List&lt;Objec&gt; items;&#125;Model obj = ....;String text = JSON.toJSONString(obj, SerializerFeature.WriteMapNullValue, SerializerFeature.WriteNullListAsEmpty); 8. Fastjson 处理日期Fastjson 处理日期的API很简单，例如： 1JSON.toJSONStringWithDateFormat(date, &quot;yyyy-MM-dd HH:mm:ss.SSS&quot;) 使用ISO-8601日期格式 1JSON.toJSONString(obj, SerializerFeature.UseISO8601DateFormat); 全局修改日期格式 12JSON.DEFFAULT_DATE_FORMAT = &quot;yyyy-MM-dd&quot;;JSON.toJSONString(obj, SerializerFeature.WriteDateUseDateFormat); 反序列化能够自动识别如下日期格式： ISO-8601日期格式 yyyy-MM-dd yyyy-MM-dd HH:mm:ss yyyy-MM-dd HH:mm:ss.SSS 毫秒数字 毫秒数字字符串 .NET JSON日期格式 new Date(198293238) 虽然上面处理了单个的日期类型和全局的日期类型格式的配置，但是有时候我们需要的是对象中个别的日期类型差异化，并不一定是同一种格式的。那如何处理呢？接下来介绍 Fastjson 的定制序列化。 9. Fastjson 定制序列化9.1 简介fastjson支持多种方式定制序列化。 通过@JSONField定制序列化 通过@JSONType定制序列化 通过SerializeFilter定制序列化 通过ParseProcess定制反序列化 9.2 使用@JSONField配置 1、JSONField 注解介绍123456789101112131415161718package com.alibaba.fastjson.annotation;public @interface JSONField &#123; // 配置序列化和反序列化的顺序，1.1.42版本之后才支持 int ordinal() default 0; // 指定字段的名称 String name() default &quot;&quot;; // 指定字段的格式，对日期格式有用 String format() default &quot;&quot;; // 是否序列化 boolean serialize() default true; // 是否反序列化 boolean deserialize() default true;&#125; 2、JSONField配置方式可以把@JSONField配置在字段或者getter/setter方法上，例如： 配置在字段上 1234567public class VO &#123; @JSONField(name=&quot;ID&quot;) private int id; @JSONField(name=&quot;birthday&quot;,format=&quot;yyyy-MM-dd&quot;) public Date date;&#125; 配置在 Getter/Setter 上 123456789public class VO &#123; private int id; @JSONField(name=&quot;ID&quot;) public int getId() &#123; return id;&#125; @JSONField(name=&quot;ID&quot;) public void setId(int id) &#123;this.id = id;&#125;&#125; 注意：若属性是私有的，必须有set*方法。否则无法反序列化。 3、使用format配置日期格式化可以定制化配置各个日期字段的格式化 12345public class A &#123; // 配置date序列化和反序列使用yyyyMMdd日期格式 @JSONField(format=&quot;yyyyMMdd&quot;) public Date date;&#125; 4、使用serialize/deserialize指定字段不序列化123456789public class A &#123; @JSONField(serialize=false) public Date date; &#125; public class A &#123; @JSONField(deserialize=false) public Date date; &#125; 5、使用ordinal指定字段的顺序缺省Fastjson序列化一个java bean，是根据fieldName的字母序进行序列化的，你可以通过ordinal指定字段的顺序。这个特性需要1.1.42以上版本。 12345678910public static class VO &#123; @JSONField(ordinal = 3) private int f0; @JSONField(ordinal = 2) private int f1; @JSONField(ordinal = 1) private int f2;&#125; 6、使用serializeUsing制定属性的序列化类在fastjson 1.2.16版本之后，JSONField支持新的定制化配置serializeUsing，可以单独对某一个类的某个属性定制序列化，比如： 1234567891011121314public static class Model &#123; @JSONField(serializeUsing = ModelValueSerializer.class) public int value;&#125;public static class ModelValueSerializer implements ObjectSerializer &#123; @Override public void write(JSONSerializer serializer, Object object, Object fieldName, Type fieldType, int features) throws IOException &#123; Integer value = (Integer) object; String text = value + &quot;元&quot;; serializer.write(text); &#125;&#125; 测试代码 1234Model model = new Model();model.value = 100;String json = JSON.toJSONString(model);Assert.assertEquals(&quot;&#123;\&quot;value\&quot;:\&quot;100元\&quot;&#125;&quot;, json); 9.3 使用@JSONType配置和JSONField类似，但JSONType配置在类上，而不是field或者getter/setter方法上。 9.4通过SerializeFilter定制序列化1、简介SerializeFilter是通过编程扩展的方式定制序列化。fastjson支持6种SerializeFilter，用于不同场景的定制序列化。 PropertyPreFilter 根据PropertyName判断是否序列化 PropertyFilter 根据PropertyName和PropertyValue来判断是否序列化 NameFilter 修改Key，如果需要修改Key,process返回值则可 ValueFilter 修改Value BeforeFilter 序列化时在最前添加内容 AfterFilter 序列化时在最后添加内容 2、PropertyFilter 根据PropertyName和PropertyValue来判断是否序列化123public interface PropertyFilter extends SerializeFilter &#123; boolean apply(Object object, String propertyName, Object propertyValue);&#125; 可以通过扩展实现根据object或者属性名称或者属性值进行判断是否需要序列化。例如： 123456789101112PropertyFilter filter = new PropertyFilter() &#123; public boolean apply(Object source, String name, Object value) &#123; if (&quot;id&quot;.equals(name)) &#123; int id = ((Integer) value).intValue(); return id &gt;= 100; &#125; return false; &#125;&#125;;JSON.toJSONString(obj, filter); // 序列化的时候传入filter 3、PropertyPreFilter 根据PropertyName判断是否序列化和PropertyFilter不同只根据object和name进行判断，在调用getter之前，这样避免了getter调用可能存在的异常。 123public interface PropertyPreFilter extends SerializeFilter &#123; boolean apply(JSONSerializer serializer, Object object, String name); &#125; 4、NameFilter 序列化时修改Key如果需要修改Key,process返回值则可 123public interface NameFilter extends SerializeFilter &#123; String process(Object object, String propertyName, Object propertyValue);&#125; fastjson内置一个PascalNameFilter，用于输出将首字符大写的Pascal风格。 例如： 1234import com.alibaba.fastjson.serializer.PascalNameFilter;Object obj = ...;String jsonStr = JSON.toJSONString(obj, new PascalNameFilter()); 5、ValueFilter 序列化时修改Value123public interface ValueFilter extends SerializeFilter &#123; Object process(Object object, String propertyName, Object propertyValue);&#125; 6、BeforeFilter 序列化时在最前添加内容在序列化对象的所有属性之前执行某些操作，例如调用 writeKeyValue 添加内容 12345public abstract class BeforeFilter implements SerializeFilter &#123; protected final void writeKeyValue(String key, Object value) &#123; ... &#125; // 需要实现的抽象方法，在实现中调用writeKeyValue添加内容 public abstract void writeBefore(Object object);&#125; 7、AfterFilter 序列化时在最后添加内容在序列化对象的所有属性之后执行某些操作，例如调用 writeKeyValue 添加内容 12345 public abstract class AfterFilter implements SerializeFilter &#123; protected final void writeKeyValue(String key, Object value) &#123; ... &#125; // 需要实现的抽象方法，在实现中调用writeKeyValue添加内容 public abstract void writeAfter(Object object);&#125; 9.5 通过ParseProcess定制反序列化1、简介ParseProcess是编程扩展定制反序列化的接口。fastjson支持如下ParseProcess： ExtraProcessor 用于处理多余的字段 ExtraTypeProvider 用于处理多余字段时提供类型信息 2、使用ExtraProcessor 处理多余字段123456789101112131415161718public static class VO &#123; private int id; private Map&lt;String, Object&gt; attributes = new HashMap&lt;String, Object&gt;(); public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id;&#125; public Map&lt;String, Object&gt; getAttributes() &#123; return attributes;&#125;&#125;ExtraProcessor processor = new ExtraProcessor() &#123; public void processExtra(Object object, String key, Object value) &#123; VO vo = (VO) object; vo.getAttributes().put(key, value); &#125;&#125;;VO vo = JSON.parseObject(&quot;&#123;\&quot;id\&quot;:123,\&quot;name\&quot;:\&quot;abc\&quot;&#125;&quot;, VO.class, processor);Assert.assertEquals(123, vo.getId());Assert.assertEquals(&quot;abc&quot;, vo.getAttributes().get(&quot;name&quot;)); 3、使用ExtraTypeProvider 为多余的字段提供类型1234567891011121314151617181920212223242526public static class VO &#123; private int id; private Map&lt;String, Object&gt; attributes = new HashMap&lt;String, Object&gt;(); public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id;&#125; public Map&lt;String, Object&gt; getAttributes() &#123; return attributes;&#125;&#125;class MyExtraProcessor implements ExtraProcessor, ExtraTypeProvider &#123; public void processExtra(Object object, String key, Object value) &#123; VO vo = (VO) object; vo.getAttributes().put(key, value); &#125; public Type getExtraType(Object object, String key) &#123; if (&quot;value&quot;.equals(key)) &#123; return int.class; &#125; return null; &#125;&#125;;ExtraProcessor processor = new MyExtraProcessor();VO vo = JSON.parseObject(&quot;&#123;\&quot;id\&quot;:123,\&quot;value\&quot;:\&quot;123456\&quot;&#125;&quot;, VO.class, processor);Assert.assertEquals(123, vo.getId());Assert.assertEquals(123456, vo.getAttributes().get(&quot;value&quot;)); // value本应该是字符串类型的，通过getExtraType的处理变成Integer类型了。 10. 在 Spring MVC 中集成 Fastjson如果你使用 Spring MVC 来构建 Web 应用并对性能有较高的要求的话，可以使用 Fastjson 提供的FastJsonHttpMessageConverter 来替换 Spring MVC 默认的 HttpMessageConverter 以提高 @RestController @ResponseBody @RequestBody 注解的 JSON序列化速度。下面是配置方式，非常简单。 XML式如果是使用 XML 的方式配置 Spring MVC 的话，只需在 Spring MVC 的 XML 配置文件中加入下面配置即可 12345&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter&quot;/&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt; 通常默认配置已经可以满足大部分使用场景，如果你想对它进行自定义配置的话，你可以添加 FastJsonConfig Bean。 1234567891011&lt;mvc:annotation-driven&gt; &lt;mvc:message-converters&gt; &lt;bean class=&quot;com.alibaba.fastjson.support.spring.FastJsonHttpMessageConverter&quot;&gt; &lt;property name=&quot;fastJsonConfig&quot; ref=&quot;fastJsonConfig&quot;/&gt; &lt;/bean&gt; &lt;/mvc:message-converters&gt;&lt;/mvc:annotation-driven&gt;&lt;bean id=&quot;fastJsonConfig&quot; class=&quot;com.alibaba.fastjson.support.config.FastJsonConfig&quot;&gt; &lt;!-- 自定义配置... --&gt;&lt;/bean&gt; 编程式如果是使用编程的方式（通常是基于 Spring Boot 项目）配置 Spring MVC 的话只需继承 WebMvcConfigurerAdapter覆写configureMessageConverters方法即可，就像下面这样。 123456789101112@Configurationpublic class WebMvcConfigurer extends WebMvcConfigurerAdapter &#123; @Override public void configureMessageConverters(List&lt;HttpMessageConverter&lt;?&gt;&gt; converters) &#123; FastJsonHttpMessageConverter converter = new FastJsonHttpMessageConverter(); //自定义配置... //FastJsonConfig config = new FastJsonConfig(); //config.set ... //converter.setFastJsonConfig(config); converters.add(0, converter); &#125;&#125; 注意：1、如果你使用的 Fastjson 版本小于1.2.36的话(强烈建议使用最新版本)，在与Spring MVC 4.X 版本集成时需使用 FastJsonHttpMessageConverter4。 2、SpringBoot 2.0.1版本中加载WebMvcConfigurer的顺序发生了变动，故需使用converters.add(0, converter);指定FastJsonHttpMessageConverter在converters内的顺序，否则在SpringBoot 2.0.1及之后的版本中将优先使用Jackson处理。 11. 在 Spring Data Redis 中集成 Fastjson通常我们在 Spring 中使用 Redis 是通过 Spring Data Redis 提供的 RedisTemplate 来进行的，如果你准备使用 JSON 作为对象序列/反序列化的方式并对序列化速度有较高的要求的话，建议使用 Fastjson 提供的 GenericFastJsonRedisSerializer 或 FastJsonRedisSerializer 作为 RedisTemplate 的 RedisSerializer。下面是配置方式，非常简单。 XML式如果是使用 XML 的方式配置 Spring Data Redis 的话，只需将 RedisTemplate 中的 Serializer 替换为 GenericFastJsonRedisSerializer 即可。 123456&lt;bean id=&quot;redisTemplate&quot; class=&quot;org.springframework.data.redis.core.RedisTemplate&quot;&gt; &lt;property name=&quot;connectionFactory&quot; ref=&quot;jedisConnectionFactory&quot;/&gt; &lt;property name=&quot;defaultSerializer&quot;&gt; &lt;bean class=&quot;com.alibaba.fastjson.support.spring.GenericFastJsonRedisSerializer&quot;/&gt; &lt;/property&gt;&lt;/bean&gt; 下面是完整的 Spring 集成 Redis 配置供参考。 1234567891011121314151617181920212223242526272829303132333435&lt;!-- Redis 连接池配置(可选) --&gt;&lt;bean id=&quot;jedisPoolConfig&quot; class=&quot;redis.clients.jedis.JedisPoolConfig&quot;&gt; &lt;property name=&quot;maxTotal&quot; value=&quot;$&#123;redis.pool.maxActive&#125;&quot;/&gt; &lt;property name=&quot;maxIdle&quot; value=&quot;$&#123;redis.pool.maxIdle&#125;&quot;/&gt; &lt;property name=&quot;maxWaitMillis&quot; value=&quot;$&#123;redis.pool.maxWait&#125;&quot;/&gt; &lt;property name=&quot;testOnBorrow&quot; value=&quot;$&#123;redis.pool.testOnBorrow&#125;&quot;/&gt; &lt;!-- 更多连接池配置...--&gt;&lt;/bean&gt;&lt;!-- Redis 连接工厂配置 --&gt;&lt;bean id=&quot;jedisConnectionFactory&quot; class=&quot;org.springframework.data.redis.connection.jedis.JedisConnectionFactory&quot;&gt; &lt;!--设置连接池配置，不设置的话会使用默认的连接池配置，若想禁用连接池可设置 usePool = false --&gt; &lt;property name=&quot;poolConfig&quot; ref=&quot;jedisPoolConfig&quot; /&gt; &lt;property name=&quot;hostName&quot; value=&quot;$&#123;host&#125;&quot;/&gt; &lt;property name=&quot;port&quot; value=&quot;$&#123;port&#125;&quot;/&gt; &lt;property name=&quot;password&quot; value=&quot;$&#123;password&#125;&quot;/&gt; &lt;property name=&quot;database&quot; value=&quot;$&#123;database&#125;&quot;/&gt; &lt;!-- 更多连接工厂配置...--&gt;&lt;/bean&gt;&lt;!-- RedisTemplate 配置 --&gt;&lt;bean id=&quot;redisTemplate&quot; class=&quot;org.springframework.data.redis.core.RedisTemplate&quot;&gt; &lt;!-- 设置 Redis 连接工厂--&gt; &lt;property name=&quot;connectionFactory&quot; ref=&quot;jedisConnectionFactory&quot;/&gt; &lt;!-- 设置默认 Serializer ，包含 keySerializer &amp; valueSerializer --&gt; &lt;property name=&quot;defaultSerializer&quot;&gt; &lt;bean class=&quot;com.alibaba.fastjson.support.spring.GenericFastJsonRedisSerializer&quot;/&gt; &lt;/property&gt; &lt;!-- 单独设置 keySerializer --&gt; &lt;property name=&quot;keySerializer&quot;&gt; &lt;bean class=&quot;com.alibaba.fastjson.support.spring.GenericFastJsonRedisSerializer&quot;/&gt; &lt;/property&gt; &lt;!-- 单独设置 valueSerializer --&gt; &lt;property name=&quot;valueSerializer&quot;&gt; &lt;bean class=&quot;com.alibaba.fastjson.support.spring.GenericFastJsonRedisSerializer&quot;/&gt; &lt;/property&gt;&lt;/bean&gt; 编程式 如果是使用编程的方式（通常是基于 Spring Boot 项目）配置 RedisTemplate 的话只需在你的配置类(被@Configuration注解修饰的类)中显式创建 RedisTemplate Bean，设置 Serializer 即可。 123456789101112@Beanpublic RedisTemplate redisTemplate(RedisConnectionFactory redisConnectionFactory) &#123; RedisTemplate redisTemplate = new RedisTemplate(); redisTemplate.setConnectionFactory(redisConnectionFactory); GenericFastJsonRedisSerializer fastJsonRedisSerializer = new GenericFastJsonRedisSerializer(); redisTemplate.setDefaultSerializer(fastJsonRedisSerializer);//设置默认的Serialize，包含 keySerializer &amp; valueSerializer //redisTemplate.setKeySerializer(fastJsonRedisSerializer);//单独设置keySerializer //redisTemplate.setValueSerializer(fastJsonRedisSerializer);//单独设置valueSerializer return redisTemplate;&#125; 通常使用 GenericFastJsonRedisSerializer 即可满足大部分场景，如果你想定义特定类型专用的 RedisTemplate 可以使用 FastJsonRedisSerializer来代替 GenericFastJsonRedisSerializer，配置是类似的。 参考：https://github.com/alibaba/fastjson/wikihttps://www.cnblogs.com/jajian/p/10051901.html]]></content>
      <categories>
        <category>JSON</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis中#和$的区别]]></title>
    <url>%2F2019%2F05%2F23%2FMyBatis%2FMyBatis%E4%B8%AD%23%E5%92%8C%24%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[区别 #{ }是预编译处理，MyBatis在处理#{ }时，会将sql中的#{ }替换为?，然后调用PreparedStatement的set方法来赋值，传入字符串后，会在值两边加上单引号，如传入1,2,3就会变成&#39;1,2,3&#39;。 ${ }是字符串替换，MyBatis在处理${ }时，会将sql中的${ }替换为变量的值，传入的数据不会在两边加上单引号。 注意：使用${ }会导致sql注入，不利于系统的安全性！ SQL注入：就是通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。常见的有匿名登录（在登录框输入恶意的字符串）、借助异常获取数据库信息等。 应用场景 一般能用#的就别用$。 $方式一般用于传入数据库对象，例如传入字段名、表名等，例如order by ${column}。 注意：${}获取DAO参数数据时，参数必须使用@param注解进行修饰或者使用下标或者参数#{param1}形式。#{}获取DAO参数数据时，假如参数个数多于一个可有选择的使用@param。]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis中resultType和resultMap的区别]]></title>
    <url>%2F2019%2F05%2F23%2FMyBatis%2FMyBatis%E4%B8%ADresultType%E5%92%8CresultMap%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[使用MyBatis查询数据库记录时，返回类型常用的有两种：resultType和resultMap。那么两者之间有什么区别呢？ 如果只是返回一个值，比如说String或者int，那直接用resultType就行了，resultType=&quot;java.lang.String&quot;。123&lt;select id="getUserName" resultType="java.lang.String"&gt; select user_name from t_users&lt;/select&gt; 如果sql查询结果返回的列名和实体类中的字段名一致，可以使用resultType，MyBatis会自动把查询结果赋值给和字段名一致的字段。实体类对象：12345678@Datapublic class Users &#123; private Long id; private String userName; private Integer sex;&#125; Mapper：123&lt;select id="getUsersType" resultType="com.zkzong.mybatis.domain.Users"&gt; select user_name userName, sex from t_users&lt;/select&gt; 如果不一致，sql语句中可以使用别名的方式使其一致。 当sql的列名和实体类的列名不一致，这时就可以使用resultMap了。123456789&lt;resultMap id="userMap" type="com.zkzong.mybatis.domain.Users"&gt; &lt;id property="id" column="id"/&gt; &lt;result property="userName" column="user_name"/&gt; &lt;result property="sex" column="sex"/&gt;&lt;/resultMap&gt;&lt;select id="getUsersMap" resultMap="userMap"&gt; select id, user_name, sex from t_users&lt;/select&gt; property是实体类的字段名，column是sql查询的列名。 对于简单的映射，resultType和resultMap区别不大。但是resultMap功能更强大，可以通过设置typeHander来自定义实现功能。]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java Persistence with MyBatis 3]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2FJava%20Persistence%20with%20MyBatis%203%2F</url>
    <content type="text"><![CDATA[Hibernate将Java对象静态地映射到数据库的表上，而MyBatis是将查询的结果与Java对象映射起来，这使得MyBatis可以很好地与传统数据库协同工作。 Java Persistence with MyBatis 31234567891011&lt;insert id="insertStudent" parameterType="Student"&gt; INSERT INTO STUDENTS(STUD_ID,NAME,EMAIL,DOB) VALUES(#&#123;studId&#125;,#&#123;name&#125;,#&#123;email&#125;,#&#123;dob&#125;)&lt;/insert&gt;&lt;insert id="insertStudent" parameterType="Student"&gt; INSERT INTO STUDENTS(NAME,EMAIL,DOB) VALUES(#&#123;name&#125;,#&#123;email&#125;,#&#123;dob&#125;)&lt;/insert&gt;&lt;insert id="insertStudent" parameterType="Student" useGeneratedKeys="true" keyProperty="studId"&gt; INSERT INTO STUDENTS(NAME,EMAIL,DOB) VALUES(#&#123;name&#125;,#&#123;email&#125;,#&#123;dob&#125;)&lt;/insert&gt; studId是自增主键，以上三种方式都可以插入成功。 使用XML配置MyBatis environment dataSource：UNPOOLED、POOLED、JNDI TransactionManager：JDBC、MANAGED properties：属性配置元素可以将配置值具体化到一个属性文件中，并且使用属性文件的key名作为占位符。可以在&lt;properties&gt;元素中配置默认参数的值。如果&lt;properties&gt;中定义的元素和属性文件定义元素的key值相同，它们会被属性文件中定义的值覆盖。 typeAliases 在SQLMapper配置文件中，对于resultType和parameterType属性值，需要使用JavaBean的完全限定名。 也可以为完全限定名去一个别名（alias），然后其需要使用完全限定名的地方使用别名，而不用到处使用完全限定名。 12345&lt;typeAliases&gt; &lt;typeAlias alias="Student" type="com.mybatis3.domain.Student" /&gt; &lt;typeAlias alias="Tutor" type="com.mybatis3.domain.Tutor" /&gt; &lt;package name="com.mybatis3.domain" /&gt;&lt;/typeAliases&gt; 也可以不用为每一个JavaBean单独定义别名，可以为提供需要取别名的JavaBean所在的包(package)，MyBatis会自动扫描包内定义的JavaBeans，然后分别为JavaBean注册一个小写字母开头的非完全限定的类名形式的别名。 123&lt;typeAliases&gt; &lt;package name="com.mybatis3.domain" /&gt;&lt;/typeAliases&gt; 使用注解@Alias。 12 @Alias("StudentAlias")public class Student&#123;&#125; 类型处理器TypeHandler 使用XML配置SQL映射器 通过字符串（字符串形式为：映射器配置文件所在的包名namespace + 在文件内定义的语句id，如 com.zkzong.mybatis.mapper.StudentMapper 和语句 id findStudentById 组成）调用映射的SQL语句。 MyBatis通过使用映射器Mapper接口提供了更好的调用映射语句。一旦通过映射器配置文件配置了映射语句，可以创建一个完全对应的一个映射器接口，接口名跟配置文件名相同，接口所在包名也跟配置文件所在包名完全一样。映射器接口中的方法签名也跟映射器配置文件中完全对应：方法名为配置文件中id值；方法参数类型为parameterType对应值；方法返回值类型为returnType对应值。 自动生成主键可以使用useGeneratedKeys和keyProperty属性让数据库生成auto_increment列的值，并将生成的值设置到其中一个输入对象属性内。 对于List、Collection、Iterable类型，MyBatis将返回java.util.ArrayList 对于Map类型，MyBatis将返回java.util.HashMap 对于Set类型，MyBatis将返回java.util.HashSet 对于SortedSet类型，MyBatis将返回java.util.TreeSet 结果集映射ResultMap 简单ResultMap 拓展ResultMap 一对一映射 使用嵌套结果ResultMap实现一对一关系映射：&lt;association&gt; 使用嵌套查询实现一对一关系映射： 一对多映射 使用内嵌结果ResultMap实现一对多映射 使用嵌套select语句实现一对多映射 动态SQL&lt;if&gt;&lt;choose&gt;&lt;where&gt;&lt;foreach&gt;&lt;trim&gt;&lt;set&gt; 传入多个输入参数 MyBatis中的映射语句有一个parameterType属性来指定输入参数的类型。如果想给映射语句传入多个参数的话，可以将所有的输入参数放到HashMap中，将HashMap传递给映射语句。 Mybatis还支持将多个输入参数传递给映射语句，并以#{param}的语法形式应用它们。 使用RowBounds对结果集进行分页使用ResultSetHandler自定义结果集ResultSet处理使用注解配置SQL映射器@Insert@Update@Select 动态SQL@SelectProvider@InsertProvider@UpdateProvider@DeleteProvider 【注意mybatis和springmybatis版本】]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MyBatis常见问题]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2FMyBatis%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[123456@Select(&quot;select * from user where name = #&#123;name&#125;&quot;)User findByName(String name);// 两个参数必须加@Param注解@Select(&quot;select * from user where name = #&#123;name&#125; and age = #&#123;age&#125;&quot;)User findByNameAndAge(@Param(&quot;name&quot;) String name, @Param(&quot;age&quot;) Integer age); 如果只有一个参数，可以不加@Param注解，但是如果参数大于1个，必须加@Param注解，否则报错。 @Select动态sql https://segmentfault.com/q/1010000006875476]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[PageHelper]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2FPageHelper%2F</url>
    <content type="text"><![CDATA[12345678// 查找所有记录PageHelper.startPage(1, 0);// 统计总数PageHelper.startPage(1, -1);// 查找第一页，每页5条PageHelper.startPage(1, 5); spring需要配置xmlspring boot不需要]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[iBATIS]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2FiBATIS%2F</url>
    <content type="text"><![CDATA[（针对iBATIS 2.x版本） 1. 简介iBATIS是一个ORM框架，它和其它框架（如Hibernate）的最大不同是：iBATIS强调SQL的使用，而其它框架使用自定义的查询语言如HQL。 2. 环境配置 创建EMPLOYEE表，SQL语句如下：1234567CREATE TABLE EMPLOYEE ( id INT NOT NULL auto_increment, first_name VARCHAR(20) default NULL, last_name VARCHAR(20) default NULL, salary INT default NULL, PRIMARY KEY (id)); 创建EMPLOYEE表。 创建SqlMapConfig.xml此文件配置数据库连接等信息。12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE sqlMapConfigPUBLIC "-//ibatis.apache.org//DTD SQL Map Config 2.0//EN""http://ibatis.apache.org/dtd/sql-map-config-2.dtd"&gt;&lt;sqlMapConfig&gt; &lt;settings useStatementNamespaces="true" /&gt; &lt;transactionManager type="JDBC"&gt; &lt;dataSource type="SIMPLE"&gt; &lt;property name="JDBC.Driver" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="JDBC.ConnectionURL" value="jdbc:mysql://localhost:3306/test" /&gt; &lt;property name="JDBC.Username" value="root" /&gt; &lt;property name="JDBC.Password" value="root" /&gt; &lt;/dataSource&gt; &lt;/transactionManager&gt; &lt;sqlMap resource="Employee.xml" /&gt;&lt;/sqlMapConfig&gt; 以下是一些可选的配置信息：1234567&lt;property name="JDBC.AutoCommit" value="true"/&gt;&lt;property name="Pool.MaximumActiveConnections" value="10"/&gt;&lt;property name="Pool.MaximumIdleConnections" value="5"/&gt;&lt;property name="Pool.MaximumCheckoutTime" value="150000"/&gt;&lt;property name="Pool.MaximumTimeToWait" value="500"/&gt;&lt;property name="Pool.PingQuery" value="select 1 from Employee"/&gt;&lt;property name="Pool.PingEnabled" value="false"/&gt; 3. CREATE操作 Employee POJO类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.zkzong.ibatis;public class Employee &#123; private int id; private String first_name; private String last_name; private int salary; /* Define constructors for the Employee class. */ public Employee() &#123; &#125; public Employee(String fname, String lname, int salary) &#123; this.first_name = fname; this.last_name = lname; this.salary = salary; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getFirstName() &#123; return first_name; &#125; public void setFirstName(String fname) &#123; this.first_name = fname; &#125; public String getLastName() &#123; return last_name; &#125; public void setlastName(String lname) &#123; this.last_name = lname; &#125; public int getSalary() &#123; return salary; &#125; public void setSalary(int salary) &#123; this.salary = salary; &#125;&#125; /* End of Employee */ 创建Employee.xml 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE sqlMapPUBLIC "-//ibatis.apache.org//DTD SQL Map 2.0//EN""http://ibatis.apache.org/dtd/sql-map-2.dtd"&gt;&lt;sqlMap namespace="Employee"&gt; &lt;!-- Perform Insert Operation --&gt; &lt;insert id="insert" parameterClass="com.zkzong.ibatis.Employee"&gt; insert into EMPLOYEE(first_name, last_name, salary) values (#first_name#, #last_name#, #salary#) &lt;selectKey resultClass="int" keyProperty="id"&gt; select last_insert_id() as id &lt;/selectKey&gt; &lt;/insert&gt; &lt;/sqlMap&gt; parameterClass可以是string、int、float、double或其他类对象。本例中我们在调用insert方法传入Employee作为参数。如果数据库表使用了IDENTITY、AUTO_INCREMENT、SERIAL或定义了SEQUENCE/GENERATOR，可以在中使用来使用或返回数据库生成的值。 IbatisInsert.java123456789101112131415161718192021222324package com.zkzong.ibatis;import java.io.IOException;import java.io.Reader;import java.sql.SQLException;import com.ibatis.common.resources.Resources;import com.ibatis.sqlmap.client.SqlMapClient;import com.ibatis.sqlmap.client.SqlMapClientBuilder;public class IbatisInsert &#123; public static void main(String[] args) throws IOException, SQLException &#123; Reader rd = Resources.getResourceAsReader("SqlMapConfig.xml"); SqlMapClient smc = SqlMapClientBuilder.buildSqlMapClient(rd); /* This would insert one record in Employee table. */ System.out.println("Going to insert record....."); Employee em = new Employee("Zara", "Ali", 5000); smc.insert("Employee.insert", em); System.out.println("Record Inserted Successfully "); &#125;&#125; 4. READ操作 Employee.xml 123456789101112&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE sqlMapPUBLIC "-//ibatis.apache.org//DTD SQL Map 2.0//EN""http://ibatis.apache.org/dtd/sql-map-2.dtd"&gt;&lt;sqlMap namespace="Employee"&gt; &lt;!-- Perform Read Operation --&gt; &lt;select id="getAll" resultClass="com.zkzong.ibatis.Employee"&gt; SELECT * FROM EMPLOYEE &lt;/select&gt;&lt;/sqlMap&gt; IbatisRead.java 1234567891011121314151617181920212223242526272829303132package com.zkzong.ibatis;import java.io.IOException;import java.io.Reader;import java.sql.SQLException;import java.util.List;import com.ibatis.common.resources.Resources;import com.ibatis.sqlmap.client.SqlMapClient;import com.ibatis.sqlmap.client.SqlMapClientBuilder;public class IbatisRead &#123; public static void main(String[] args) throws IOException, SQLException &#123; Reader rd = Resources.getResourceAsReader("SqlMapConfig.xml"); SqlMapClient smc = SqlMapClientBuilder.buildSqlMapClient(rd); /* This would read all records from the Employee table. */ System.out.println("Going to read records....."); List&lt;Employee&gt; ems = (List&lt;Employee&gt;) smc.queryForList("Employee.getAll", null); Employee em = null; for (Employee e : ems) &#123; System.out.print(" " + e.getId()); System.out.print(" " + e.getFirstName()); System.out.print(" " + e.getLastName()); System.out.print(" " + e.getSalary()); em = e; System.out.println(""); &#125; System.out.println("Records Read Successfully "); &#125;&#125; 5. UPDATE操作 Employee.xml 1234567891011121314&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE sqlMapPUBLIC "-//ibatis.apache.org//DTD SQL Map 2.0//EN""http://ibatis.apache.org/dtd/sql-map-2.dtd"&gt;&lt;sqlMap namespace="Employee"&gt; &lt;!-- Perform Update Operation --&gt; &lt;update id="update" parameterClass="com.zkzong.ibatis.Employee"&gt; UPDATE EMPLOYEE SET first_name = #first_name# WHERE id = #id# &lt;/update&gt;&lt;/sqlMap&gt; IbatisUpdate.java 123456789101112131415161718192021222324252627282930313233343536373839package com.zkzong.ibatis;import java.io.IOException;import java.io.Reader;import java.sql.SQLException;import java.util.List;import com.ibatis.common.resources.Resources;import com.ibatis.sqlmap.client.SqlMapClient;import com.ibatis.sqlmap.client.SqlMapClientBuilder;public class IbatisUpdate &#123; public static void main(String[] args) throws IOException, SQLException &#123; Reader rd = Resources.getResourceAsReader("SqlMapConfig.xml"); SqlMapClient smc = SqlMapClientBuilder.buildSqlMapClient(rd); /* This would update one record in Employee table. */ System.out.println("Going to update record....."); Employee rec = new Employee(); rec.setId(1); rec.setFirstName("Roma"); smc.update("Employee.update", rec); System.out.println("Record updated Successfully "); System.out.println("Going to read records....."); List&lt;Employee&gt; ems = (List&lt;Employee&gt;) smc.queryForList("Employee.getAll", null); Employee em = null; for (Employee e : ems) &#123; System.out.print(" " + e.getId()); System.out.print(" " + e.getFirstName()); System.out.print(" " + e.getLastName()); System.out.print(" " + e.getSalary()); em = e; System.out.println(""); &#125; System.out.println("Records Read Successfully "); &#125;&#125; 6. DELETE操作 Employee.xml 12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE sqlMapPUBLIC "-//ibatis.apache.org//DTD SQL Map 2.0//EN""http://ibatis.apache.org/dtd/sql-map-2.dtd"&gt;&lt;sqlMap namespace="Employee"&gt; &lt;!-- Perform Delete Operation --&gt; &lt;delete id="delete" parameterClass="int"&gt; DELETE FROM EMPLOYEE WHERE id = #id# &lt;/delete&gt;&lt;/sqlMap&gt; IbatisDelete.java 123456789101112131415161718192021222324252627282930313233343536373839package com.zkzong.ibatis;import java.io.IOException;import java.io.Reader;import java.sql.SQLException;import java.util.List;import com.ibatis.common.resources.Resources;import com.ibatis.sqlmap.client.SqlMapClient;import com.ibatis.sqlmap.client.SqlMapClientBuilder;public class IbatisDelete &#123; public static void main(String[] args) throws IOException, SQLException &#123; Reader rd = Resources.getResourceAsReader("SqlMapConfig.xml"); SqlMapClient smc = SqlMapClientBuilder.buildSqlMapClient(rd); /* This would delete one record in Employee table. */ System.out.println("Going to delete record....."); int id = 1; smc.delete("Employee.delete", id); System.out.println("Record deleted Successfully "); System.out.println("Going to read records....."); List&lt;Employee&gt; ems = (List&lt;Employee&gt;) smc.queryForList( "Employee.getAll", null); Employee em = null; for (Employee e : ems) &#123; System.out.print(" " + e.getId()); System.out.print(" " + e.getFirstName()); System.out.print(" " + e.getLastName()); System.out.print(" " + e.getSalary()); em = e; System.out.println(""); &#125; System.out.println("Records Read Successfully "); &#125;&#125; 7. RESULT MAPSresultMap是iBATIS中最重要的元素。使用iBATIS ResultMap最多可以减少90%的JDBC编码，甚至可以实现JDBC不支持的操作。 Employee.xml 12345678910111213141516171819&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE sqlMapPUBLIC "-//ibatis.apache.org//DTD SQL Map 2.0//EN""http://ibatis.apache.org/dtd/sql-map-2.dtd"&gt;&lt;sqlMap namespace="Employee"&gt; &lt;!-- Using ResultMap --&gt; &lt;resultMap id="result" class="com.zkzong.ibatis.Employee"&gt; &lt;result property="id" column="id"/&gt; &lt;result property="first_name" column="first_name"/&gt; &lt;result property="last_name" column="last_name"/&gt; &lt;result property="salary" column="salary"/&gt; &lt;/resultMap&gt; &lt;select id="useResultMap" resultMap="result"&gt; SELECT * FROM EMPLOYEE WHERE id=#id# &lt;/select&gt; &lt;/sqlMap&gt; IbatisResultMap.java 123456789101112131415161718192021222324252627package com.zkzong.ibatis;import java.io.IOException;import java.io.Reader;import java.sql.SQLException;import com.ibatis.common.resources.Resources;import com.ibatis.sqlmap.client.SqlMapClient;import com.ibatis.sqlmap.client.SqlMapClientBuilder;public class IbatisResultMap &#123; public static void main(String[] args) throws IOException, SQLException &#123; Reader rd = Resources.getResourceAsReader("SqlMapConfig.xml"); SqlMapClient smc = SqlMapClientBuilder.buildSqlMapClient(rd); int id = 2; System.out.println("Going to read record....."); Employee e = (Employee) smc.queryForObject("Employee.useResultMap", id); System.out.println("ID: " + e.getId()); System.out.println("First Name: " + e.getFirstName()); System.out.println("Last Name: " + e.getLastName()); System.out.println("Salary: " + e.getSalary()); System.out.println("Record read Successfully "); &#125;&#125; 8. 存储过程9. 动态SQL动态SQL是iBATIS非常重要的特性。有时需要改变WHERE语句的查询条件，在这种情况下iBATIS提供了一系列的动态SQL标签用来映射条件以加强SQL的可重用性和灵活性。所有的逻辑都是放在.XML文件的标签里。 Employee.xml 123456789101112131415161718&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE sqlMapPUBLIC "-//ibatis.apache.org//DTD SQL Map 2.0//EN""http://ibatis.apache.org/dtd/sql-map-2.dtd"&gt;&lt;sqlMap namespace="Employee"&gt; &lt;select id="findByID" resultClass="com.zkzong.ibatis.Employee"&gt; SELECT * FROM EMPLOYEE &lt;dynamic prepend="WHERE "&gt; &lt;isNull property="id"&gt; id IS NULL &lt;/isNull&gt; &lt;isNotNull property="id"&gt; id = #id# &lt;/isNotNull&gt; &lt;/dynamic&gt; &lt;/select&gt;&lt;/sqlMap&gt; IbatisReadDy.java 123456789101112131415161718192021222324252627282930313233343536package com.zkzong.ibatis;import java.io.IOException;import java.io.Reader;import java.sql.SQLException;import java.util.List;import com.ibatis.common.resources.Resources;import com.ibatis.sqlmap.client.SqlMapClient;import com.ibatis.sqlmap.client.SqlMapClientBuilder;public class IbatisReadDy &#123; public static void main(String[] args) throws IOException, SQLException &#123; Reader rd = Resources.getResourceAsReader("SqlMapConfig.xml"); SqlMapClient smc = SqlMapClientBuilder.buildSqlMapClient(rd); /* This would read all records from the Employee table. */ System.out.println("Going to read records....."); Employee rec = new Employee(); rec.setId(2); List&lt;Employee&gt; ems = (List&lt;Employee&gt;) smc.queryForList( "Employee.findByID", rec); Employee em = null; for (Employee e : ems) &#123; System.out.print(" " + e.getId()); System.out.print(" " + e.getFirstName()); System.out.print(" " + e.getLastName()); System.out.print(" " + e.getSalary()); em = e; System.out.println(""); &#125; System.out.println("Records Read Successfully "); &#125;&#125; iBATIS OGNL表达式iBATIS提供强大的OGNL表达式以减少其他元素。 if choose, when, otherwise where foreachif语句动态SQL中最常用的是在where子句中包含条件：1234567&lt;select id="findActiveBlogWithTitleLike" parameterType="Blog" resultType="Blog"&gt; SELECT * FROM BLOG WHERE state = 'ACTIVE. &lt;if test="title != null"&gt; AND title like #&#123;title&#125; &lt;/if&gt;&lt;/select&gt; 这个语句提供可选的文本查询功能。如果传入的title为空，返回所有的active Blog。如果传入的title不为空，根据like条件查询。可以包含多个if语句：12345678910&lt;select id="findActiveBlogWithTitleLike" parameterType="Blog" resultType="Blog"&gt; SELECT * FROM BLOG WHERE state = 'ACTIVE. &lt;if test="title != null"&gt; AND title like #&#123;title&#125; &lt;/if&gt; &lt;if test="author != null"&gt; AND author like #&#123;author&#125; &lt;/if&gt;&lt;/select&gt; choose, when, otherwise语句iBATIS中的choose和Java中的switch语句类似，选择其中一个执行。12345678910111213141516&lt;select id="findActiveBlogWithTitleLike"parameterType="Blog" resultType="Blog"&gt; SELECT * FROM BLOG WHERE state = 'ACTIVE' &lt;choose&gt; &lt;when test="title != null"&gt; AND title like #&#123;title&#125; &lt;/when&gt; &lt;when test="author != null and author.name != null"&gt; AND author like #&#123;author&#125; &lt;/when&gt; &lt;otherwise&gt; AND featured = 1 &lt;/otherwise&gt; &lt;/choose&gt;&lt;/select&gt; where语句如果条件中没有符合的，可能会出现这样的SQL:12SELECT * FROM BLOGWHERE 这个SQL会执行失败，如果改为以下的代码就可以执行：123456789101112131415&lt;select id="findActiveBlogLike"parameterType="Blog" resultType="Blog"&gt; SELECT * FROM BLOG &lt;where&gt; &lt;if test="state != null"&gt; state = #&#123;state&#125; &lt;/if&gt; &lt;if test="title != null"&gt; AND title like #&#123;title&#125; &lt;/if&gt; &lt;if test="author != null&gt; AND author like #&#123;author&#125; &lt;/if&gt; &lt;/where&gt;&lt;/select&gt; 只有有标签返回内容是才会插入where语句。而且如果返回的内容以AND或OR开始，它会自动去除。foreach语句foreach元素允许指定collection和声明item、index以便在元素体内使用。允许指定open和close字符串，在iterations之间添加separator。创建in条件如下：123456789&lt;select id="selectPostIn" resultType="domain.blog.Post"&gt; SELECT * FROM POST P WHERE ID in &lt;foreach item="item" index="index" collection="list"open="(" separator="," close=")"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt;]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mybatis generator用法]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2Fmybatis%20generator%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[mybatis-generator有三种用法：命令行、eclipse插件、maven插件。个人推荐使用命令行或maven插件。 命令行的用法请参考：http://blog.csdn.net/wyc_cs/article/details/9023117maven插件的用法请参考：http://www.cnblogs.com/yjmyzz/p/mybatis-generator-tutorial.html 下面重点介绍一下注意事项：实体类生成构造方法：&lt;javaModelGenerator&gt;中添加&lt;property name=&quot;constructorBased&quot; value=&quot;true&quot; /&gt; 实体类生成toString方法：&lt;plugin type=&quot;org.mybatis.generator.plugins.ToStringPlugin&quot; /&gt;格式 实体类实现序列化接口：&lt;plugin type=&quot;org.mybatis.generator.plugins.SerializablePlugin&quot; /&gt;serialVersionUID值 自增主键：自增主键在插入数据时一般不需要给这个主键赋值，可以使用以下配置：&lt;table&gt;中添加&lt;generatedKey column=&quot;id&quot; sqlStatement=&quot;JDBC&quot; identity=&quot;true&quot; /&gt; mapper.xml追加而不是覆盖：生成的.xml文件在重新生成时是以追加的方式添加到xml文件中的，而不是覆盖，具体原因可以查看这篇文章https://my.oschina.net/u/140938/blog/220006，因此每次都要删除xml文件再重新生成。 其他参考文章：plugins：http://www.mybatis.org/generator/reference/plugins.html Mybatis Generator配置详解：http://www.jianshu.com/p/e09d2370b796]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[mybatis笔记]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2Fmybatis%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[实际上，Mybatis 只做了两件事情： 根据 JDBC 规范 建立与数据库的连接。 通过反射打通Java对象和数据库参数和返回值之间相互转化的关系。]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[spring boot mapper加载方式]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2Fspring%20boot%20mapper%E5%8A%A0%E8%BD%BD%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[引用xml配置 使用class类配置 使用starter配置 3.1 @Mapper 3.2 @MapperScan(basePackages = “com.gomefinance.cif.mapper”)mybatis.mapperLocations=classpath:mapper/*.xml http://blog.csdn.net/mickjoust/article/details/51646658]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从源码角度理解MyBatis字段映射]]></title>
    <url>%2F2019%2F05%2F14%2FMyBatis%2F%E4%BB%8E%E6%BA%90%E7%A0%81%E8%A7%92%E5%BA%A6%E7%90%86%E8%A7%A3MyBatis%E5%AD%97%E6%AE%B5%E6%98%A0%E5%B0%84%2F</url>
    <content type="text"><![CDATA[MyBatis在转换查询结果到需要的Java业务对象时做了三件事： 解决了数据库列名到Java列名的映射。 解决了数据库类型到Java类型的转换工作。 在转换过程中具备一定的容错能力。 其实核心就是： 数据库中的列名怎么和对象中的字段对应起来。 数据库中的列的类型怎么转换到合适的Java类型，不引起转换失败。 我们先来看第一点，数据库中的列名怎么和对象中的字段对应起来。首先是PO(Persistant Object) CityPO里面有五个字段：1234567public class CityPO &#123; Integer id; Long cityId; String cityName; String cityEnName; String cityPyName;&#125; 本次要查询的数据库中的列名如下所示:12345678910111213mysql&gt; mysql&gt; desc SU_City;+--------------+-------------+------+-----+-------------------+-----------------------------+| Field | Type | Null | Key | Default | Extra |+--------------+-------------+------+-----+-------------------+-----------------------------+| id | int(11) | NO | PRI | NULL | auto_increment || city_id | int(11) | NO | UNI | NULL | || city_name | varchar(20) | NO | | | || city_en_name | varchar(20) | NO | | | || city_py_name | varchar(50) | NO | | | || create_time | datetime | NO | | CURRENT_TIMESTAMP | || updatetime | datetime | NO | MUL | CURRENT_TIMESTAMP | on update CURRENT_TIMESTAMP |+--------------+-------------+------+-----+-------------------+-----------------------------+7 rows in set (0.01 sec) 我们是按照驼峰式命名，把数据库中的列名对应到了对象的字段名。如下是MyBatis的接口类和映射文件。接口类：123public interface CityMapper &#123; CityPO selectCity(int id);&#125; 映射文件：123456789&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="mapper.CityMapper"&gt; &lt;select id="selectCity" resultType="po.CityPO"&gt; select id,city_id,city_name,city_en_name from SU_City where id = #&#123;id&#125; &lt;/select&gt;&lt;/mapper&gt; 在上面的映射文件中，namespace指定了这个接口类的全限定类名，紧随其后的select代表是select语句，id是接口类中函数的名字，resultType代表了从这条语句中返回的期望类型的类的完全限定名或别名，在此例子中是我们的业务对象CityPO的类路径。 主要有三种方案： 驼峰式命名开关，或者不开——数据库列和字段名全一致。 Select时指定AS。 resultMap 最稳健。 1. 驼峰命名开关因为CityPO的列名是完全根据数据库列名驼峰式命名后得到的，因此MyBatis提供了一个配置项。开启开配置项后，在匹配时，能够根据数据库列名找到对应对应的驼峰式命名后的字段。1234&lt;settings&gt; &lt;!-- 开启驼峰，开启后，只要数据库字段和对象属性名字母相同，无论中间加多少下划线都可以识别 --&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true" /&gt;&lt;/settings&gt; 从源码角度解读一下，MyBatis处理ResultSet的映射默认都在DefaultResultSetHandler中完成。 处理行数据的源码主要在下面的函数里进行，由于我们在映射文件中没有定义额外的ResultMap，因此会直接进入else分支的代码。 123456789public void handleRowValues(ResultSetWrapper rsw, ResultMap resultMap, ResultHandler&lt;?&gt; resultHandler, RowBounds rowBounds, ResultMapping parentMapping) throws SQLException &#123; if (resultMap.hasNestedResultMaps()) &#123; ensureNoRowBounds(); checkResultHandler(); handleRowValuesForNestedResultMap(rsw, resultMap, resultHandler, rowBounds, parentMapping); &#125; else &#123; handleRowValuesForSimpleResultMap(rsw, resultMap, resultHandler, rowBounds, parentMapping); &#125;&#125; 进入handleRowValuesForSimpleResultMap中，主要处理函数如下，在这里完成了对象的生成及赋值。1Object rowValue = getRowValue(rsw, discriminatedResultMap); 在这里先创建了对象的实例，然后获取了对象的元信息，为反射赋值做准备。123456789101112131415private Object getRowValue(ResultSetWrapper rsw, ResultMap resultMap) throws SQLException &#123; final ResultLoaderMap lazyLoader = new ResultLoaderMap(); Object rowValue = createResultObject(rsw, resultMap, lazyLoader, null); if (rowValue != null &amp;&amp; !hasTypeHandlerForResultObject(rsw, resultMap.getType())) &#123; final MetaObject metaObject = configuration.newMetaObject(rowValue); boolean foundValues = this.useConstructorMappings; if (shouldApplyAutomaticMappings(resultMap, false)) &#123; foundValues = applyAutomaticMappings(rsw, resultMap, metaObject, null) || foundValues; &#125; foundValues = applyPropertyMappings(rsw, resultMap, metaObject, lazyLoader, null) || foundValues; foundValues = lazyLoader.size() &gt; 0 || foundValues; rowValue = (foundValues || configuration.isReturnInstanceForEmptyRow()) ? rowValue : null; &#125; return rowValue;&#125; 在applyAutomaticMappings完成了整个过程，我们进去探一探。 就是下面这个函数创建好了映射关系，这个函数的下半部分是完成赋值的，映射的部分下次会详细分析。 1List&lt;UnMappedColumnAutoMapping&gt; autoMapping = createAutomaticMappings(rsw, resultMap, metaObject, columnPrefix); 在这个方法里，上半部分是生成了数据库的列名，在这个函数中找到了对应的字段名。 1final String property = metaObject.findProperty(propertyName, configuration.isMapUnderscoreToCamelCase()); 我们进去看一看，它传进了生成好的数据库列名，传进了前面提到的是否根据驼峰式命名映射开关的值。 事实证明，真的很简单，往下看，就是把下划线都去了。 123456public String findProperty(String name, boolean useCamelCaseMapping) &#123; if (useCamelCaseMapping) &#123; name = name.replace("_", ""); &#125; return findProperty(name);&#125; 隐隐觉得是不是大小写不敏感啊，继续往下看，这里返回找到的字段名。 12345678private StringBuilder buildProperty(String name, StringBuilder builder) &#123; ...... String propertyName = reflector.findPropertyName(name); if (propertyName != null) &#123; builder.append(propertyName); &#125; return builder;&#125; 好了，真相大白，就是大小写不敏感的。 123public String findPropertyName(String name) &#123; return caseInsensitivePropertyMap.get(name.toUpperCase(Locale.ENGLISH));&#125; 2. Select …. AS当我们的数据库列名和对象字段之间不是驼峰式命名的关系，可以在Select时使用AS，使得列名和对象名匹配上。 映射文件中是本次会执行的sql，我们会查出id，city_id，city_name，city_en_name。 按照开启的驼峰式命名开关，我们会对应到对象的id，cityId，cityName，cityEnName字段。 123&lt;select id="selectCity" resultType="po.CityPO"&gt; select id,city_id,city_name,city_en_name from SU_City where id = #&#123;id&#125;&lt;/select&gt; 不过在这次，我们对PO做了小小的改动，把cityEnName改成了cityEnglishName。 123456public class CityPO &#123; Integer id; Long cityId; String cityName; String cityEnglishName; // 由cityEnName改成了cityEnglishName&#125; 由于找不到匹配的列，cityEnlishName肯定没法被反射赋值，因此值为Null。 1CityPO&#123;id=2, cityId=2, cityName=&apos;北京&apos;, cityEnglishName=&apos;null&apos;&#125; 解决办法：在Select字段的时候使用AS，下面是改动后的映射文件。 12345678&lt;select id="selectCity" resultType="po.CityPO"&gt; select id, city_id, city_name, city_en_name AS cityEnglishName from SU_City where id = #&#123;id&#125;&lt;/select&gt; 改动后执行得到的结果如下。 1CityPO&#123;id=2, cityId=2, cityName=&apos;北京&apos;, cityEnglishName=&apos;beijing&apos;&#125; 那么我们来看看它是如何生效的，主要的代码在哪里。上面我们第一个介绍的函数handleRowValues中传入了参数rsw，它是对ResultSet的一个包装，在这个包装里，完成了具体使用哪个名字作为数据库的列名。 12final ResultSetWrapper rsw = new ResultSetWrapper(rs, configuration);handleRowValues(rsw, resultMap, resultHandler, new RowBounds(), null); 在这个构造函数当中，我们会获取数据库的列名，AS为什么可以生效，具体就在下面这段代码。 1234567891011super();this.typeHandlerRegistry = configuration.getTypeHandlerRegistry();this.resultSet = rs;final ResultSetMetaData metaData = rs.getMetaData();final int columnCount = metaData.getColumnCount();for (int i = 1; i &lt;= columnCount; i++) &#123; // 在这里 columnNames.add(configuration.isUseColumnLabel() ? metaData.getColumnLabel(i) : metaData.getColumnName(i)); jdbcTypes.add(JdbcType.forCode(metaData.getColumnType(i))); classNames.add(metaData.getColumnClassName(i));&#125; 在添加列名时，会从配置中获取是否使用类标签，isUseColumnLabel，默认为true。根据Javadoc，这个ColumnLabel就是AS后的那个名字，如果没有AS的话，就是获取的原生的字段名。 123456789101112/** * Gets the designated column's suggested title for use in printouts and * displays. The suggested title is usually specified by the SQL &lt;code&gt;AS&lt;/code&gt; * clause. If a SQL &lt;code&gt;AS&lt;/code&gt; is not specified, the value returned from * &lt;code&gt;getColumnLabel&lt;/code&gt; will be the same as the value returned by the * &lt;code&gt;getColumnName&lt;/code&gt; method. * * @param column the first column is 1, the second is 2, ... * @return the suggested column title * @exception SQLException if a database access error occurs */String getColumnLabel(int column) throws SQLException; 后面的过程就和上面方案一一模一样了，不再赘述。 3. ResultMap resultMap 元素是 MyBatis 中最重要最强大的元素。它可以让你从 90% 的 JDBC ResultSets 数据提取代码中解放出来，并在一些情形下允许你做一些 JDBC 不支持的事情。 实际上，在对复杂语句进行联合映射的时候，它很可能可以代替数千行的同等功能的代码。 ResultMap 的设计思想是，简单的语句不需要明确的结果映射，而复杂一点的语句只需要描述它们的关系就行了。 ResultMap是MyBatis中可以完成复杂语句映射的东西，但在我们的日常开发中，我们往往是一个XML对应JavaBeans 或 POJOs(Plain Old Java Objects，普通 Java 对象)，并没有特别复杂的应用，下面也是基于日常的使用，看看简单的ResultMap在源码层面是如何展现的。 123456789101112131415&lt;resultMap id="cityMap" type="po.CityPO"&gt; &lt;result column="id" property="id"/&gt; &lt;result column="city_id" property="cityId"/&gt; &lt;result column="city_name" property="cityName"/&gt; &lt;result column="city_en_name" property="cityEnglishName"/&gt;&lt;/resultMap&gt;&lt;select id="selectCity" resultMap="cityMap"&gt; select id, city_id, city_name, city_en_name from SU_City where id = #&#123;id&#125;&lt;/select&gt; 在resultMap的子元素result对应了result和对象字段之间的映射，并通过id标示，你在Select语句中指定需要使用的resultMap即可。 源码层面的话，依旧在DefaultResultSetHandler的handleResultSets中处理返回集合。 1List&lt;ResultMap&gt; resultMaps = mappedStatement.getResultMaps(); 在这次的ResultMap中，相比之前方案，其属性更加的丰富起来。将之前写的Result的信息保存在resultMappings，idResultMappings等中，以备后续使用。 后续的函数走向和方案一二一致，但在创建自动映射的时候出现了不同。 12private List&lt;UnMappedColumnAutoMapping&gt; createAutomaticMappings(ResultSetWrapper rsw, ResultMap resultMap, MetaObject metaObject, String columnPrefix) throws SQLException &#123;&#125; 在这个函数中，会获取没有映射过的列名。 1final List&lt;String&gt; unmappedColumnNames = rsw.getUnmappedColumnNames(resultMap, columnPrefix); 之后会根据resultMap查看是否有未映射的字段。 1loadMappedAndUnmappedColumnNames(resultMap, columnPrefix); 12345678910111213141516171819private void loadMappedAndUnmappedColumnNames(ResultMap resultMap, String columnPrefix) throws SQLException &#123; List&lt;String&gt; mappedColumnNames = new ArrayList&lt;&gt;(); List&lt;String&gt; unmappedColumnNames = new ArrayList&lt;&gt;(); final String upperColumnPrefix = columnPrefix == null ? null : columnPrefix.toUpperCase(Locale.ENGLISH); // 这里没有配置前缀，根据之前的图，定义了ResultMap后，会记录这些已经配置映射的字段。 final Set&lt;String&gt; mappedColumns = prependPrefixes(resultMap.getMappedColumns(), upperColumnPrefix); for (String columnName : columnNames) &#123; // 遍历列名，如果在已映射的配置中，那么就加入已经映射的列名数据 final String upperColumnName = columnName.toUpperCase(Locale.ENGLISH); if (mappedColumns.contains(upperColumnName)) &#123; mappedColumnNames.add(upperColumnName); &#125; else &#123; unmappedColumnNames.add(columnName); &#125; &#125; // 生成未映射和已映射的Map mappedColumnNamesMap.put(getMapKey(resultMap, columnPrefix), mappedColumnNames); unMappedColumnNamesMap.put(getMapKey(resultMap, columnPrefix), unmappedColumnNames);&#125; 如果有没配置在ResultMap中，且Select出来的，那么之后也会按照之前方案一那样，继续往下走，从对象中寻找映射关系。 由于没有未映射的字段，使用自动映射的结果是false。 1foundValues = applyAutomaticMappings(rsw, resultMap, metaObject, columnPrefix) || foundValues; 之后继续往下走，使用applyPropertyMappings来创建对象。其用到了propertyMappings，里面包含了字段名，列名，字段的类型和对应的处理器。 遍历整个Mappings。 1Object value = getPropertyMappingValue(rsw.getResultSet(), metaObject, propertyMapping, lazyLoader, columnPrefix); 函数里主要的就是获取这个字段对应的类型处理器，防止类型转换失败，这一部分下次会专门看一下。 123final TypeHandler&lt;?&gt; typeHandler = propertyMapping.getTypeHandler();final String column = prependPrefix(propertyMapping.getColumn(), columnPrefix);return typeHandler.getResult(rs, column); TypeHandler就是一个接口，主要完成的工作就是从Result根据列名，获取相应类型的值，为下一步反射赋值做准备。至于它是怎么决定为什么用这个类型的TypeHandler下次再看。 然后就是给对应字段赋值。 1metaObject.setValue(property, value); 最后就完成了整个类的赋值。 总结大致上，MyBatis完成映射主要是两种方式： 只根据列名，利用自动映射，根据反射类的信息，得到列名和字段之间的关系，使用对应的TypeHandler完成字段的赋值。 使用ResultMap预先定义好映射关系，最后也是根据TypeHandler和反射完成字段的赋值。 就简单的用法来说，两者都可以。在一次会话中，Configuration中的ResultMap关系建立好，在每一次查询的时候就不用再去重新建立了，直接用就行。而自动映射的话，执行过一次后，也会在会话中建立自动映射的缓存，所以没什么差别。但如果复杂的映射的话，就非ResultMap莫属啦。具体可以参考MyBatis文档关于映射的章节，因为目前用不到比较复杂的映射，不做深究了。]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[POI读取excel数字列自动带.0的解决办法]]></title>
    <url>%2F2019%2F04%2F27%2FPOI%2FPOI%E8%AF%BB%E5%8F%96excel%E6%95%B0%E5%AD%97%E5%88%97%E8%87%AA%E5%8A%A8%E5%B8%A6.0%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[最近做项目需要用POI读取excel的数据并处理，excel中有一列的整数，读取后值自动带.0。 解决办法有两种： 设置CellType 使用DataFormatter格式化 使用for循环遍历123456789for (int rowNum = 1; rowNum &lt;= lastRowNum; rowNum++) &#123; XSSFCell number = xssfRow.getCell(6); // 第1种方式 number.setCellType(CellType.STRING); // 第二种方式 DataFormatter dataFormatter = new DataFormatter(); String value = dataFormatter.formatCellValue(number);&#125; 使用foreach循环遍历1234567for (Cell cell : row) &#123; // 第1种方式 cell.setCellType(CellType.STRING); // 第二种方式 DataFormatter dataFormatter = new DataFormatter(); String value = dataFormatter.formatCellValue(cell);&#125;]]></content>
      <categories>
        <category>POI</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java MongoDB保存图片]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FJava%20MongoDB%E4%BF%9D%E5%AD%98%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用GridFS API把图片文件保存到MongoDB。GridFS API也能保存其他二进制文件，如视频和音乐文件。 1. 保存图片下面代码使用photo命名空间，新的filename保存图片到MongoDB。123456String newFileName = &quot;mkyong-java-image&quot;;File imageFile = new File(&quot;mongodb.png&quot;);GridFS gfsPhoto = new GridFS(db, &quot;photo&quot;);GridFSInputFile gfsFile = gfsPhoto.createFile(imageFile);gfsFile.setFilename(newFileName);gfsFile.save(); 2. 获取图片1234String newFileName = &quot;mkyong-java-image&quot;;GridFS gfsPhoto = new GridFS(db, &quot;photo&quot;);GridFSDBFile imageForOutput = gfsPhoto.findOne(newFileName);System.out.println(imageForOutput); 输出，图片以如下的JSON格式被保存：12345678910111213141516&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;4dc9511a14a7d017fee35746&quot; &#125; , &quot;chunkSize&quot; : 262144 , &quot;length&quot; : 22672 , &quot;md5&quot; : &quot;1462a6cfa27669af1d8d21c2d7dd1f8b&quot; , &quot;filename&quot; : &quot;mkyong-java-image&quot; , &quot;contentType&quot; : null , &quot;uploadDate&quot; : &#123; &quot;$date&quot; : &quot;2011-05-10T14:52:10Z&quot; &#125; , &quot;aliases&quot; : null&#125; 3. 打印所有图片使用DBCursor遍历所有图片。 12345GridFS gfsPhoto = new GridFS(db, &quot;photo&quot;);DBCursor cursor = gfsPhoto.getFileList();while (cursor.hasNext()) &#123; System.out.println(cursor.next());&#125; 4. 保存为另一张图片从MongoDB中获取图片并把它保存为另一张图片。 1234String newFileName = &quot;mkyong-java-image&quot;;GridFS gfsPhoto = new GridFS(db, &quot;photo&quot;);GridFSDBFile imageForOutput = gfsPhoto.findOne(newFileName);imageForOutput.writeTo(&quot;mongodbNew.png&quot;); //output to new file 5. 删除图片123String newFileName = &quot;mkyong-java-image&quot;;GridFS gfsPhoto = new GridFS(db, &quot;photo&quot;);gfsPhoto.remove(gfsPhoto.findOne(newFileName)); 完整实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import com.mongodb.DB;import com.mongodb.DBCollection;import com.mongodb.DBCursor;import com.mongodb.MongoClient;import com.mongodb.MongoException;import com.mongodb.gridfs.GridFS;import com.mongodb.gridfs.GridFSDBFile;import com.mongodb.gridfs.GridFSInputFile;import java.io.File;import java.io.IOException;import java.net.UnknownHostException;public class SaveImage &#123; public static void main(String[] args) &#123; try &#123; MongoClient mongoClient = new MongoClient(&quot;localhost&quot;, 27017); DB db = mongoClient.getDB(&quot;imagedb&quot;); DBCollection collection = db.getCollection(&quot;dummyColl&quot;); String newFileName = &quot;mkyong-java-image&quot;; File imageFile = new File(&quot;mongodb.png&quot;); // create a &quot;photo&quot; namespace GridFS gfsPhoto = new GridFS(db, &quot;photo&quot;); // get image file from local drive GridFSInputFile gfsFile = gfsPhoto.createFile(imageFile); // set a new filename for identify purpose gfsFile.setFilename(newFileName); // save the image file into mongoDB gfsFile.save(); // print the result DBCursor cursor = gfsPhoto.getFileList(); while (cursor.hasNext()) &#123; System.out.println(cursor.next()); &#125; // get image file by it&apos;s filename GridFSDBFile imageForOutput = gfsPhoto.findOne(newFileName); // save it into a new image file imageForOutput.writeTo(&quot;mongodbNew.png&quot;); // remove the image file from mongoDB gfsPhoto.remove(gfsPhoto.findOne(newFileName)); System.out.println(&quot;Done&quot;); &#125; catch (UnknownHostException e) &#123; e.printStackTrace(); &#125; catch (MongoException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java MongoDB删除]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FJava%20MongoDB%E5%88%A0%E9%99%A4%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用collection.delete()删除文档。 测试数据插入10个文档：123for (int i = 1; i &lt;= 10; i++) &#123; collection.insertOne(new Document().append(&quot;number&quot;, i));&#125; 1. collection.delete()下面是几个删除文档的例子。 例1获取第一个文档并删除。本例中number = 1的文档被删除。12Document doc = collection.find().first(); //get first documentcollection.deleteOne(doc); 例2把查询放到Document中。本例中number = 2的文档被删除。123Document document = new Document();document.put(&quot;number&quot;, 2);collection.deleteOne(document); 两种常见错误：1. 这样的查询只删除number = 3的文档1234Document document = new Document();document.put(&quot;number&quot;, 2); document.put(&quot;number&quot;, 3); //override above value 2collection.deleteOne(document); 2. 但是如下面的查询，删除并不起作用123456Document document = new Document();List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();list.add(7);list.add(8);document.put(&quot;number&quot;, list);collection.remove(document); 对于and查询，需要使用$in或$and操作符，参考例5。 例3直接使用Document删除。本例中number = 3的文档被删除。1collection.deleteOne(new Document().append(&quot;number&quot;, 3)); 例4在Document中使用$gt操作符。本例中number = 10的文档被删除。123Document query = new Document();query.put(&quot;number&quot;, new Document(&quot;$gt&quot;, 9));collection.deleteOne(query); 例5使用$in构建查询并删除多个文档。本例中number = 4和number = 5的文档被删除。123456Document query2 = new Document();List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();list.add(4);list.add(5);query2.put(&quot;number&quot;, new Document(&quot;$in&quot;, list));collection.deleteMany(query2); 例6使用循环的方式删除所有文档。（不推荐这种方式，推荐例7的方式）12345FindIterable&lt;Document&gt; deleteDocuments = collection.find();MongoCursor&lt;Document&gt; deleteIterator = deleteDocuments.iterator();while (deleteIterator.hasNext()) &#123; collection.deleteOne(deleteIterator.next());&#125; 例7传入一个空的Document对象，删除所有文档。1collection.deleteMany(new Document()); 例8删除文档和集合。1collection.drop(); 例9delete()方法会返回DeleteResult对象，它包含了一些关于删除操作的有用信息。可以使用getDeletedCount()获取删除文档数。12DeleteResult deleteResult = collection.deleteMany(new Document());System.out.println(&quot;删除文档数：&quot; + deleteResult.getDeletedCount()); 2. 完整实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import com.mongodb.MongoClient;import com.mongodb.MongoException;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoCursor;import com.mongodb.client.MongoDatabase;import com.mongodb.client.result.DeleteResult;import org.bson.Document;import java.util.ArrayList;import java.util.List;public class RemoveDocument &#123; public static void main(String[] args) &#123; try &#123; MongoClient mongoClient = new MongoClient(&quot;localhost&quot;, 27017); MongoDatabase database = mongoClient.getDatabase(&quot;test&quot;); // get a single collection MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;dummyColl&quot;); //insert number 1 to 10 for testing for (int i = 1; i &lt;= 10; i++) &#123; collection.insertOne(new Document().append(&quot;number&quot;, i)); &#125; //remove number = 1 Document doc = collection.find().first(); //get first document collection.deleteOne(doc); //remove number = 2 Document document = new Document(); document.put(&quot;number&quot;, 2); collection.deleteOne(document); //remove number = 3 collection.deleteOne(new Document().append(&quot;number&quot;, 3)); //remove number &gt; 9 , means delete number = 10 Document query = new Document(); query.put(&quot;number&quot;, new Document(&quot;$gt&quot;, 9)); collection.deleteOne(query); //remove number = 4 and 5 Document query2 = new Document(); List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); list.add(4); list.add(5); query2.put(&quot;number&quot;, new Document(&quot;$in&quot;, list)); collection.deleteOne(query2); FindIterable&lt;Document&gt; deleteDocuments = collection.find(); MongoCursor&lt;Document&gt; deleteIterator = deleteDocuments.iterator(); while (deleteIterator.hasNext()) &#123; collection.deleteOne(deleteIterator.next()); &#125; collection.deleteMany(new Document()); collection.drop(); DeleteResult deleteResult = collection.deleteMany(new Document()); System.out.println(&quot;删除文档数：&quot; + deleteResult.getDeletedCount()); //print out the document FindIterable&lt;Document&gt; documents = collection.find(); MongoCursor&lt;Document&gt; iterator = documents.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; collection.drop(); System.out.println(&quot;Done&quot;); &#125; catch (MongoException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java MongoDB基本操作]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FJava%20MongoDB%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用Java操作MongoDB，如创建连接数据库、集合和文档，保存、更新、删除和查询文档。 1. 引入依赖12345&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.5.0&lt;/version&gt;&lt;/dependency&gt; 2. Mongo连接使用MongoClient连接到数据库。 1MongoClient mongo = new MongoClient(&quot;localhost&quot;, 27017); 如果是安全模式，需要认证。 123MongoClient mongoClient = new MongoClient();DB db = mongoClient.getDB(&quot;database name&quot;);boolean auth = db.authenticate(&quot;username&quot;, &quot;password&quot;.toCharArray()); 3. MongoDB数据库获取数据库： 1MongoDatabase database = mongoClient.getDatabase(&quot;t&quot;); 显示所有数据库： 1234MongoIterable&lt;String&gt; dbs = mongoClient.listDatabaseNames();for (String db : dbs) &#123; System.out.println(db);&#125; 4. 集合获取集合：12MongoDatabase mongoDatabase = mongoClient.getDatabase(&quot;test&quot;);MongoCollection&lt;Document&gt; collection = mongoDatabase.getCollection(&quot;coll&quot;); 显示所有集合1234MongoIterable&lt;String&gt; collections = mongoDatabase.listCollectionNames();for (String c : collections) &#123; System.out.println(c);&#125; 5. 保存123456DBCollection table = db.getCollection(&quot;user&quot;);BasicDBObject document = new BasicDBObject();document.put(&quot;name&quot;, &quot;mkyong&quot;);document.put(&quot;age&quot;, 30);document.put(&quot;createdDate&quot;, new Date());table.insert(document); 6. 更新123456789101112DBCollection table = db.getCollection(&quot;user&quot;);BasicDBObject query = new BasicDBObject();query.put(&quot;name&quot;, &quot;mkyong&quot;);BasicDBObject newDocument = new BasicDBObject();newDocument.put(&quot;name&quot;, &quot;mkyong-updated&quot;);BasicDBObject updateObj = new BasicDBObject();updateObj.put(&quot;$set&quot;, newDocument);table.update(query, updateObj); 7. 查询12345678910DBCollection table = db.getCollection(&quot;user&quot;);BasicDBObject searchQuery = new BasicDBObject();searchQuery.put(&quot;name&quot;, &quot;mkyong&quot;);DBCursor cursor = table.find(searchQuery);while (cursor.hasNext()) &#123; System.out.println(cursor.next());&#125; 8. 删除123456DBCollection table = db.getCollection(&quot;user&quot;);BasicDBObject searchQuery = new BasicDBObject();searchQuery.put(&quot;name&quot;, &quot;mkyong&quot;);table.remove(searchQuery); 完整例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import com.mongodb.BasicDBObject;import com.mongodb.DB;import com.mongodb.DBCollection;import com.mongodb.DBCursor;import com.mongodb.MongoClient;import com.mongodb.MongoException;import com.mongodb.client.MongoDatabase;import java.util.Date;public class HelloWorld &#123; public static void main(String[] args) &#123; try &#123; /**** Connect to MongoDB ****/ // Since 2.10.0, uses MongoClient MongoClient mongo = new MongoClient(&quot;localhost&quot;, 27017); /**** Get database ****/ // if database doesn&apos;t exists, MongoDB will create it for you DB db = mongo.getDB(&quot;testdb&quot;); MongoDatabase mongoDatabase = mongo.getDatabase(&quot;testdb&quot;); /**** Get collection / table from &apos;testdb&apos; ****/ // if collection doesn&apos;t exists, MongoDB will create it for you DBCollection table = db.getCollection(&quot;user&quot;); /**** Insert ****/ // create a document to store key and value BasicDBObject document = new BasicDBObject(); document.put(&quot;name&quot;, &quot;mkyong&quot;); document.put(&quot;age&quot;, 30); document.put(&quot;createdDate&quot;, new Date()); table.insert(document); /**** Find and display ****/ BasicDBObject searchQuery = new BasicDBObject(); searchQuery.put(&quot;name&quot;, &quot;mkyong&quot;); DBCursor cursor = table.find(searchQuery); while (cursor.hasNext()) &#123; System.out.println(cursor.next()); &#125; /**** Update ****/ // search document where name=&quot;mkyong&quot; and update it with new values BasicDBObject query = new BasicDBObject(); query.put(&quot;name&quot;, &quot;mkyong&quot;); BasicDBObject newDocument = new BasicDBObject(); newDocument.put(&quot;name&quot;, &quot;mkyong-updated&quot;); BasicDBObject updateObj = new BasicDBObject(); updateObj.put(&quot;$set&quot;, newDocument); table.update(query, updateObj); /**** Find and display ****/ BasicDBObject searchQuery2 = new BasicDBObject().append(&quot;name&quot;, &quot;mkyong-updated&quot;); DBCursor cursor2 = table.find(searchQuery2); while (cursor2.hasNext()) &#123; System.out.println(cursor2.next()); &#125; /**** Done ****/ System.out.println(&quot;Done&quot;); &#125; catch (MongoException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java MongoDB插入]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FJava%20MongoDB%E6%8F%92%E5%85%A5%2F</url>
    <content type="text"><![CDATA[本文介绍3中插入文档的方法。 1. 使用Document插入文档1234567891011Document document = new Document();document.put(&quot;database&quot;, &quot;test&quot;);document.put(&quot;table&quot;, &quot;hosting&quot;);Document documentDetail = new Document();documentDetail.put(&quot;records&quot;, 99);documentDetail.put(&quot;index&quot;, &quot;vps_index1&quot;);documentDetail.put(&quot;active&quot;, &quot;true&quot;);document.put(&quot;detail&quot;, documentDetail);collection.insertOne(document); 2. 使用Map插入文档123456789101112Map&lt;String, Object&gt; documentMap = new HashMap&lt;String, Object&gt;();documentMap.put(&quot;database&quot;, &quot;test&quot;);documentMap.put(&quot;table&quot;, &quot;hosting&quot;);Map&lt;String, Object&gt; documentMapDetail = new HashMap&lt;String, Object&gt;();documentMapDetail.put(&quot;records&quot;, 99);documentMapDetail.put(&quot;index&quot;, &quot;vps_index1&quot;);documentMapDetail.put(&quot;active&quot;, &quot;true&quot;);documentMap.put(&quot;detail&quot;, documentMapDetail);collection.insertOne(new Document(documentMap)); 3. 使用JSON插入文档1234String json = &quot;&#123;\&quot;database\&quot;:\&quot;test\&quot;, \&quot;table\&quot;:\&quot;hosting\&quot;, \&quot;detail\&quot;:&#123;\&quot;records\&quot;:99, \&quot;index\&quot;:\&quot;vps_index1\&quot;, \&quot;active\&quot;:\&quot;true\&quot;&#125;&#125;&quot;;Document d = Document.parse(json);collection.insertOne(d); 完整实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import com.mongodb.MongoClient;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoCursor;import com.mongodb.client.MongoDatabase;import org.bson.Document;import java.util.HashMap;import java.util.Map;public class InsertDocument &#123; public static void main(String[] args) &#123; MongoClient mongoClient = new MongoClient(&quot;localhost&quot;, 27017); MongoDatabase database = mongoClient.getDatabase(&quot;test&quot;); MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;coll&quot;); Document document = new Document(); document.put(&quot;database&quot;, &quot;test&quot;); document.put(&quot;table&quot;, &quot;hosting&quot;); Document documentDetail = new Document(); documentDetail.put(&quot;records&quot;, 99); documentDetail.put(&quot;index&quot;, &quot;vps_index1&quot;); documentDetail.put(&quot;active&quot;, &quot;true&quot;); document.put(&quot;detail&quot;, documentDetail); collection.insertOne(document); Map&lt;String, Object&gt; documentMap = new HashMap&lt;String, Object&gt;(); documentMap.put(&quot;database&quot;, &quot;test&quot;); documentMap.put(&quot;table&quot;, &quot;hosting&quot;); Map&lt;String, Object&gt; documentMapDetail = new HashMap&lt;String, Object&gt;(); documentMapDetail.put(&quot;records&quot;, 99); documentMapDetail.put(&quot;index&quot;, &quot;vps_index1&quot;); documentMapDetail.put(&quot;active&quot;, &quot;true&quot;); documentMap.put(&quot;detail&quot;, documentMapDetail); collection.insertOne(new Document(documentMap)); String json = &quot;&#123;\&quot;database\&quot;:\&quot;test\&quot;, \&quot;table\&quot;:\&quot;hosting\&quot;, \&quot;detail\&quot;:&#123;\&quot;records\&quot;:99, \&quot;index\&quot;:\&quot;vps_index1\&quot;, \&quot;active\&quot;:\&quot;true\&quot;&#125;&#125;&quot;; Document d = Document.parse(json); collection.insertOne(d); FindIterable&lt;Document&gt; documents = collection.find(); MongoCursor&lt;Document&gt; mongoCursor = documents.iterator(); while (mongoCursor.hasNext()) &#123; System.out.println(mongoCursor.next()); &#125; &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java MongoDB查询]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FJava%20MongoDB%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[本文介绍如何从集合中查询文档的通用方法。 测试数据插入5条测试文档12345&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 1 , &quot;name&quot; : &quot;mkyong-1&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 2 , &quot;name&quot; : &quot;mkyong-2&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 3 , &quot;name&quot; : &quot;mkyong-3&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 4 , &quot;name&quot; : &quot;mkyong-4&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 5 , &quot;name&quot; : &quot;mkyong-5&quot;&#125; 1. find()1.1 获取第一条文档12Document document = collection.find().first();System.out.println(document); 输出：1&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 1 , &quot;name&quot; : &quot;mkyong-1&quot;&#125; 1.2 获取所有文档12345FindIterable&lt;Document&gt; documents = collection.find();MongoCursor&lt;Document&gt; mongoCursor = documents.iterator();while (mongoCursor.hasNext()) &#123; System.out.println(mongoCursor.next());&#125; 输出：12345&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 1 , &quot;name&quot; : &quot;mkyong-1&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 2 , &quot;name&quot; : &quot;mkyong-2&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 3 , &quot;name&quot; : &quot;mkyong-3&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 4 , &quot;name&quot; : &quot;mkyong-4&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 5 , &quot;name&quot; : &quot;mkyong-5&quot;&#125; 1.3 获取文档的单一字段12345678Document fields = new Document();fields.put(&quot;name&quot;, 1);FindIterable&lt;Document&gt; projection = collection.find().projection(fields);MongoCursor&lt;Document&gt; iterator = projection.iterator();while (iterator.hasNext()) &#123; System.out.println(iterator.next());&#125; 输出：12345&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;name&quot; : &quot;mkyong-1&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;name&quot; : &quot;mkyong-2&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;name&quot; : &quot;mkyong-3&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;name&quot; : &quot;mkyong-4&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;name&quot; : &quot;mkyong-5&quot;&#125; 2. 使用find()对比查询2.1 获取所有number = 5的文档1234567Document whereQuery = new Document();whereQuery.put(&quot;number&quot;, 5);FindIterable&lt;Document&gt; whereDocuments = collection.find(whereQuery);MongoCursor&lt;Document&gt; whereIterator = whereDocuments.iterator();while (whereIterator.hasNext()) &#123; System.out.println(whereIterator.next());&#125; 输出：1&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 5 , &quot;name&quot; : &quot;mkyong-5&quot;&#125; 2.2 $in - 获取number在2、4、5中的文档1234567891011Document inQuery = new Document();List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;();list.add(2);list.add(4);list.add(5);inQuery.put(&quot;number&quot;, new Document(&quot;$in&quot;, list));FindIterable&lt;Document&gt; listDocuments = collection.find(inQuery);MongoCursor&lt;Document&gt; listIterator = listDocuments.iterator();while (listIterator.hasNext()) &#123; System.out.println(listIterator.next());&#125; 输出：123&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 2 , &quot;name&quot; : &quot;mkyong-2&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 4 , &quot;name&quot; : &quot;mkyong-4&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 5 , &quot;name&quot; : &quot;mkyong-5&quot;&#125; 2.3 $gt $lt - 获取5 &gt; number &gt; 2的文档1234567Document gtQuery = new Document();gtQuery.put(&quot;number&quot;, new Document(&quot;$gt&quot;, 2).append(&quot;$lt&quot;, 5));FindIterable&lt;Document&gt; gtDocuments = collection.find(gtQuery);MongoCursor&lt;Document&gt; gtIterator = gtDocuments.iterator();while (gtIterator.hasNext()) &#123; System.out.println(gtIterator.next());&#125; 输出：12&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 3 , &quot;name&quot; : &quot;mkyong-3&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 4 , &quot;name&quot; : &quot;mkyong-4&quot;&#125; 2.4 $ne - 获取number != 4的文档1234567Document neQuery = new Document();neQuery.put(&quot;number&quot;, new Document(&quot;$ne&quot;, 4));FindIterable&lt;Document&gt; neDocuments = collection.find(neQuery);MongoCursor&lt;Document&gt; neIterator = neDocuments.iterator();while (neIterator.hasNext()) &#123; System.out.println(neIterator.next());&#125; 输出：1234&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 1 , &quot;name&quot; : &quot;mkyong-1&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 2 , &quot;name&quot; : &quot;mkyong-2&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 3 , &quot;name&quot; : &quot;mkyong-3&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 5 , &quot;name&quot; : &quot;mkyong-5&quot;&#125; 3. 使用find()进行逻辑查询3.1 $and - 获取number = 2 and name = &#39;mkyong-2&#39;的文档1234567891011121314Document andQuery = new Document();List&lt;Document&gt; obj = new ArrayList&lt;Document&gt;();obj.add(new Document(&quot;number&quot;, 2));obj.add(new Document(&quot;name&quot;, &quot;mkyong-2&quot;));andQuery.put(&quot;$and&quot;, obj);System.out.println(andQuery.toString());FindIterable&lt;Document&gt; andDocuments = collection.find(andQuery);MongoCursor&lt;Document&gt; andIterator = andDocuments.iterator();while (andIterator.hasNext()) &#123; System.out.println(andIterator.next());&#125; 输出：123&#123; &quot;$and&quot; : [ &#123; &quot;number&quot; : 2&#125; , &#123; &quot;name&quot; : &quot;mkyong-2&quot;&#125;]&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;number&quot; : 2 , &quot;name&quot; : &quot;mkyong-2&quot;&#125; 4. 使用find()通过正则表达式查询4.1 $regex123456789101112Document regexQuery = new Document();regexQuery.put(&quot;name&quot;, new Document(&quot;$regex&quot;, &quot;Mky.*-[1-3]&quot;) .append(&quot;$options&quot;, &quot;i&quot;));System.out.println(regexQuery.toString());FindIterable&lt;Document&gt; regexDocuments = collection.find(regexQuery);MongoCursor&lt;Document&gt; regexIterator = regexDocuments.iterator();while (regexIterator.hasNext()) &#123; System.out.println(regexIterator.next());&#125; 输出：12345&#123; &quot;name&quot; : &#123; &quot;$regex&quot; : &quot;Mky.*-[1-3]&quot; , &quot;$options&quot; : &quot;i&quot;&#125;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;515ad59e3004c89329c7b259&quot;&#125; , &quot;number&quot; : 1 , &quot;name&quot; : &quot;mkyong-1&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;515ad59e3004c89329c7b25a&quot;&#125; , &quot;number&quot; : 2 , &quot;name&quot; : &quot;mkyong-2&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;515ad59e3004c89329c7b25b&quot;&#125; , &quot;number&quot; : 3 , &quot;name&quot; : &quot;mkyong-3&quot;&#125; 5. 完整实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151import com.mongodb.MongoClient;import com.mongodb.MongoException;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoCursor;import com.mongodb.client.MongoDatabase;import org.bson.Document;import java.util.ArrayList;import java.util.Calendar;import java.util.List;public class FindDocument &#123; public static void insertDummyDocuments(MongoCollection&lt;Document&gt; collection) &#123; List&lt;Document&gt; list = new ArrayList&lt;Document&gt;(); Calendar cal = Calendar.getInstance(); for (int i = 1; i &lt;= 5; i++) &#123; Document data = new Document(); data.append(&quot;number&quot;, i); data.append(&quot;name&quot;, &quot;mkyong-&quot; + i); // data.append(&quot;date&quot;, cal.getTime()); // +1 day cal.add(Calendar.DATE, 1); list.add(data); &#125; collection.insertMany(list); &#125; public static void main(String[] args) &#123; try &#123; MongoClient mongoClient = new MongoClient(&quot;localhost&quot;, 27017); MongoDatabase database = mongoClient.getDatabase(&quot;test&quot;); // get a single collection MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;dummyColl&quot;); insertDummyDocuments(collection); System.out.println(&quot;1. Find first matched document&quot;); Document document = collection.find().first(); System.out.println(document); System.out.println(&quot;\n1. Find all matched documents&quot;); FindIterable&lt;Document&gt; documents = collection.find(); MongoCursor&lt;Document&gt; mongoCursor = documents.iterator(); while (mongoCursor.hasNext()) &#123; System.out.println(mongoCursor.next()); &#125; System.out.println(&quot;\n1. Get &apos;name&apos; field only&quot;);// Document allQuery = new Document(); Document fields = new Document(); fields.put(&quot;name&quot;, 1); FindIterable&lt;Document&gt; projection = collection.find().projection(fields); MongoCursor&lt;Document&gt; iterator = projection.iterator(); while (iterator.hasNext()) &#123; System.out.println(iterator.next()); &#125; System.out.println(&quot;\n2. Find where number = 5&quot;); Document whereQuery = new Document(); whereQuery.put(&quot;number&quot;, 5); FindIterable&lt;Document&gt; whereDocuments = collection.find(whereQuery); MongoCursor&lt;Document&gt; whereIterator = whereDocuments.iterator(); while (whereIterator.hasNext()) &#123; System.out.println(whereIterator.next()); &#125; System.out.println(&quot;\n2. Find where number in 2,4 and 5&quot;); Document inQuery = new Document(); List&lt;Integer&gt; list = new ArrayList&lt;Integer&gt;(); list.add(2); list.add(4); list.add(5); inQuery.put(&quot;number&quot;, new Document(&quot;$in&quot;, list)); FindIterable&lt;Document&gt; listDocuments = collection.find(inQuery); MongoCursor&lt;Document&gt; listIterator = listDocuments.iterator(); while (listIterator.hasNext()) &#123; System.out.println(listIterator.next()); &#125; System.out.println(&quot;\n2. Find where 5 &gt; number &gt; 2&quot;); Document gtQuery = new Document(); gtQuery.put(&quot;number&quot;, new Document(&quot;$gt&quot;, 2).append(&quot;$lt&quot;, 5)); FindIterable&lt;Document&gt; gtDocuments = collection.find(gtQuery); MongoCursor&lt;Document&gt; gtIterator = gtDocuments.iterator(); while (gtIterator.hasNext()) &#123; System.out.println(gtIterator.next()); &#125; System.out.println(&quot;\n2. Find where number != 4&quot;); Document neQuery = new Document(); neQuery.put(&quot;number&quot;, new Document(&quot;$ne&quot;, 4)); FindIterable&lt;Document&gt; neDocuments = collection.find(neQuery); MongoCursor&lt;Document&gt; neIterator = neDocuments.iterator(); while (neIterator.hasNext()) &#123; System.out.println(neIterator.next()); &#125; System.out.println(&quot;\n3. Find when number = 2 and name = &apos;mkyong-2&apos; example&quot;); Document andQuery = new Document(); List&lt;Document&gt; obj = new ArrayList&lt;Document&gt;(); obj.add(new Document(&quot;number&quot;, 2)); obj.add(new Document(&quot;name&quot;, &quot;mkyong-2&quot;)); andQuery.put(&quot;$and&quot;, obj); System.out.println(andQuery.toString()); FindIterable&lt;Document&gt; andDocuments = collection.find(andQuery); MongoCursor&lt;Document&gt; andIterator = andDocuments.iterator(); while (andIterator.hasNext()) &#123; System.out.println(andIterator.next()); &#125; System.out.println(&quot;\n4. Find where name = &apos;Mky.*-[1-3]&apos;, case sensitive example&quot;); Document regexQuery = new Document(); regexQuery.put(&quot;name&quot;, new Document(&quot;$regex&quot;, &quot;Mky.*-[1-3]&quot;) .append(&quot;$options&quot;, &quot;i&quot;)); System.out.println(regexQuery.toString()); FindIterable&lt;Document&gt; regexDocuments = collection.find(regexQuery); MongoCursor&lt;Document&gt; regexIterator = regexDocuments.iterator(); while (regexIterator.hasNext()) &#123; System.out.println(regexIterator.next()); &#125; collection.drop(); System.out.println(&quot;Done&quot;); &#125; catch (MongoException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java MongoDB更新]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FJava%20MongoDB%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[本文介绍如何使用collection.update()更新文档。 测试数据假设已经插入下面数据：123456789101112131415&#123; &quot;hosting&quot; : &quot;hostA&quot;, &quot;type&quot; : &quot;vps&quot;, &quot;clients&quot; : 1000&#125;,&#123; &quot;hosting&quot; : &quot;hostB&quot;, &quot;type&quot; : &quot;dedicated server&quot;, &quot;clients&quot; : 100&#125;,&#123; &quot;hosting&quot; : &quot;hostC&quot;, &quot;type&quot; : &quot;vps&quot;, &quot;clients&quot; : 900&#125; 123&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostA&quot; , &quot;type&quot; : &quot;vps&quot; , &quot;clients&quot; : 1000&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostB&quot; , &quot;type&quot; : &quot;dedicated server&quot; , &quot;clients&quot; : 100&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostC&quot; , &quot;type&quot; : &quot;vps&quot; , &quot;clients&quot; : 900&#125; 1. 使用$set更新文档查询hosting = &#39;hostB&#39;的文档并把clients的值从100更新为10。 使用$set只更新特定的字段。 123456Document updateDocument = new Document();updateDocument.append(&quot;$set&quot;, new Document().append(&quot;clients&quot;, 110));Document searchQuery2 = new Document().append(&quot;hosting&quot;, &quot;hostB&quot;);collection.updateOne(searchQuery2, updateDocument); 输出：123&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostA&quot; , &quot;type&quot; : &quot;vps&quot; , &quot;clients&quot; : 1000&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostB&quot; , &quot;type&quot; : &quot;dedicated server&quot; , &quot;clients&quot; : 110&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostC&quot; , &quot;type&quot; : &quot;vps&quot; , &quot;clients&quot; : 900&#125; 2. 使用$inc更新文档该例子介绍使用$inc修改符增加特定值。 查询hosting = &#39;hostB&#39;的文档，并把’clients’的值从100增加到199。1234Document newDocument2 = new Document().append(&quot;$inc&quot;, new Document().append(&quot;clients&quot;, 99));collection.updateOne(new Document().append(&quot;hosting&quot;, &quot;hostB&quot;), newDocument2); 输出：123&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostA&quot; , &quot;type&quot; : &quot;vps&quot; , &quot;clients&quot; : 1000&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostB&quot; , &quot;type&quot; : &quot;dedicated server&quot; , &quot;clients&quot; : 199&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostC&quot; , &quot;type&quot; : &quot;vps&quot; , &quot;clients&quot; : 900&#125; 3. 使用updateMany更新多个文档该例子介绍使用updateMany更新符合条件的多个文档。 查询type = &#39;vps&#39;的文档，并把所有符合条件的文档的clients更新为888。 1234567Document updateQuery = new Document();updateQuery.append(&quot;$set&quot;, new Document().append(&quot;clients&quot;, &quot;888&quot;));Document searchQuery3 = new Document();searchQuery3.append(&quot;type&quot;, &quot;vps&quot;);collection.updateMany(searchQuery3, updateQuery); 输出：123&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostA&quot; , &quot;clients&quot; : &quot;888&quot; , &quot;type&quot; : &quot;vps&quot;&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostB&quot; , &quot;type&quot; : &quot;dedicated server&quot; , &quot;clients&quot; : 100&#125;&#123; &quot;_id&quot; : &#123; &quot;$oid&quot; : &quot;id&quot;&#125; , &quot;hosting&quot; : &quot;hostC&quot; , &quot;clients&quot; : &quot;888&quot; , &quot;type&quot; : &quot;vps&quot;&#125; 4. 完整实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import com.mongodb.MongoClient;import com.mongodb.MongoException;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoCursor;import com.mongodb.client.MongoDatabase;import org.bson.Document;public class UpdateDocument &#123; public static void printAllDocuments(MongoCollection&lt;Document&gt; collection) &#123; FindIterable&lt;Document&gt; documents = collection.find(); MongoCursor&lt;Document&gt; mongoCursor = documents.iterator(); while (mongoCursor.hasNext()) &#123; System.out.println(mongoCursor.next()); &#125; &#125; public static void removeAllDocuments(MongoCollection&lt;Document&gt; collection) &#123; collection.deleteMany(new Document()); &#125; public static void insertDummyDocuments(MongoCollection&lt;Document&gt; collection) &#123; Document document = new Document(); document.put(&quot;hosting&quot;, &quot;hostA&quot;); document.put(&quot;type&quot;, &quot;vps&quot;); document.put(&quot;clients&quot;, 1000); Document document2 = new Document(); document2.put(&quot;hosting&quot;, &quot;hostB&quot;); document2.put(&quot;type&quot;, &quot;dedicated server&quot;); document2.put(&quot;clients&quot;, 100); Document document3 = new Document(); document3.put(&quot;hosting&quot;, &quot;hostC&quot;); document3.put(&quot;type&quot;, &quot;vps&quot;); document3.put(&quot;clients&quot;, 900); collection.insertOne(document); collection.insertOne(document2); collection.insertOne(document3); &#125; public static void main(String[] args) &#123; try &#123; MongoClient mongoClient = new MongoClient(&quot;localhost&quot;, 27017); MongoDatabase database = mongoClient.getDatabase(&quot;test&quot;); // get a single collection MongoCollection&lt;Document&gt; collection = database.getCollection(&quot;dummyColl&quot;); System.out.println(&quot;\nTesting 1...with $set&quot;); insertDummyDocuments(collection); Document updateDocument = new Document(); updateDocument.append(&quot;$set&quot;, new Document().append(&quot;clients&quot;, 110)); Document searchQuery2 = new Document().append(&quot;hosting&quot;, &quot;hostB&quot;); collection.updateOne(searchQuery2, updateDocument); printAllDocuments(collection); removeAllDocuments(collection); System.out.println(&quot;\nTesting 2... with $inc&quot;); insertDummyDocuments(collection); // find hosting = hostB and increase it&apos;s &quot;clients&quot; value by 99 Document newDocument2 = new Document().append(&quot;$inc&quot;, new Document().append(&quot;clients&quot;, 99)); collection.updateOne(new Document().append(&quot;hosting&quot;, &quot;hostB&quot;), newDocument2); printAllDocuments(collection); removeAllDocuments(collection); System.out.println(&quot;\nTesting 3... with $multi&quot;); insertDummyDocuments(collection); // find type = vps , update all matched documents , clients value to 888 Document updateQuery = new Document(); updateQuery.append(&quot;$set&quot;, new Document().append(&quot;clients&quot;, &quot;888&quot;)); Document searchQuery3 = new Document(); searchQuery3.append(&quot;type&quot;, &quot;vps&quot;); collection.updateMany(searchQuery3, updateQuery); // collection.update(searchQuery3, updateQuery, false, true); printAllDocuments(collection); removeAllDocuments(collection); System.out.println(&quot;Done&quot;); &#125; catch (MongoException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MongoDB副本集搭建]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FMongoDB%E5%89%AF%E6%9C%AC%E9%9B%86%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[使用命令行每个副本集成员以下面命令启动：1sudo mongod --dbpath /data/db --replSet rs0 在其中一台使用rs.initiate()命令即可成为主服务。如果要修改host的名称，可执行如下命令：12config=&#123;&quot;_id&quot;:&quot;rs0&quot;,&quot;members&quot;:[&#123;&quot;_id&quot;:0,&quot;host&quot;:&quot;192.168.88.129:27017&quot;&#125;]&#125;rs.reconfig(config,&#123;&quot;force&quot;:true&#125;) 当主服务启动之后，使用mongo登录，然后使用rs.add()命令把另外两台加进来。12rs.add(&quot;192.168.88.130:27017&quot;)rs.add(&quot;192.168.88.132:27017&quot;) 使用config文件https://docs.mongodb.com/manual/reference/configuration-options/#replication-options 使用config文件启动，主要是在/etc/mongod.conf文件中配置。12replication: replSetName: rs0 使用mongod --config /etc/mongod.conf启动副本集成员。在其中一台使用rs.initiate()命令，则该机器成为主服务。然后添加其他副本集成员：12rs.add(&quot;192.168.88.130:27017&quot;)rs.add(&quot;192.168.88.132:27017&quot;)]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MongoDB权威指南]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FMongoDB%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[第1章 简介MongoDB是面向文档的数据库。 优点： 丰富的数据类型 容易扩展 丰富的功能 索引 索引 存储JavaScript 聚合 固定集合 文件存储 不牺牲速度 简便的管理 第2章 入门基本概念： 文档是MongoDB中数据的基本单位，非常类似于关系数据库管理系统中的行（但是比行要复杂得多）。 类似地，集合可以被看作是没有模式的表。 MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限。 MongoDB自带简洁但功能强大的JavaScript shell，这个工具对于管理MongoDB实例和操作数据作用非常大。 每一个文档都有一个特殊的键“_id”，它在文档所处的集合中是唯一的。 2.1 文档多个键及其关联的值有序地放置在一起便是文档。 文档中的键/值对是有序的。 文档中的值不仅可以是在双引号里面的字符串，还可以是其他几种数据类型。文档的键是字符串。除了少数例外情况，键可以使用任意UTF-8字符。 键不能含有\0（空字符）。这个字符用来表示键的结尾。 .和$有特别的意义，只有在特定环境下才能使用。 以下划线“_”开头的键是保留的。 MongoDB不但区分类型，也区分大小写。 MongoDB的文档不能有重复的键。 2.2 集合集合就是一组文档。 2.2.1 无模式集合是无模式的。这意味着一个集合里面的文档可以是格式各样的。 2.2.2 命名集合名可以是满足下列条件的任意UTF-8字符串： 集合名不能是空字符串””。 集合名不能含有\0字符（空字符，这个字符表示集合名的结尾。 集合名不能以”system.”开头，这是为系统集合保留的前缀。 用户创建的集合名字不能含有保留字符$。 2.3 数据库MongoDB中多个文档组成集合，同样多个集合可以组成数据库。一个MongoDB实例可以承载多个数据库，他们之间可视为完全独立的。每个数据库都有独立的权限控制。 数据库通过名字来标识。数据库名可以是满足以下条件的任意UTF-8字符串。 不能是空字符串（””）。 不得含有’’（空格）、.、$、/、\和\0（空字符）。 应全部小写。 最多64字节。要记住一点，数据库名最终会变成文件系统里的文件。 2.4 启动MongoDBLinux：./mongodWindows：mongod.exe 2.5 MongoDB shellLinux：./mongoWindows：mongo shell中的基本操作创建：insert读取：find、findOne更新：update删除：remove 使用shell的窍门help]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB删除]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FSpring%20Data%20MongoDB%E5%88%A0%E9%99%A4%2F</url>
    <content type="text"><![CDATA[在Spring Data MongoDB中，可以使用remove()和findAndRemove()删除文档。 remove() - 删除一个或多个文档。 findAndRemove() - 删除单个文档，并返回删除的文档。 常见错误：不要使用findAndRemove()执行批量删除，因为只有符合条件的第一个文档被删除了。 1. 删除文档实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import java.util.ArrayList;import java.util.List;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import com.mkyong.config.SpringMongoConfig;import com.mkyong.model.User;/** * Delete example * * @author mkyong * */public class DeleteApp &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringMongoConfig.class); MongoOperations mongoOperation = (MongoOperations) ctx.getBean(&quot;mongoTemplate&quot;); // insert 6 users for testing List&lt;User&gt; users = new ArrayList&lt;User&gt;(); User user1 = new User(&quot;1001&quot;, &quot;ant&quot;, 10); User user2 = new User(&quot;1002&quot;, &quot;bird&quot;, 20); User user3 = new User(&quot;1003&quot;, &quot;cat&quot;, 30); User user4 = new User(&quot;1004&quot;, &quot;dog&quot;, 40); User user5 = new User(&quot;1005&quot;, &quot;elephant&quot;, 50); User user6 = new User(&quot;1006&quot;, &quot;frog&quot;, 60); users.add(user1); users.add(user2); users.add(user3); users.add(user4); users.add(user5); users.add(user6); mongoOperation.insert(users, User.class); Query query1 = new Query(); query1.addCriteria(Criteria.where(&quot;name&quot;).exists(true) .orOperator( Criteria.where(&quot;name&quot;).is(&quot;frog&quot;), Criteria.where(&quot;name&quot;).is(&quot;dog&quot;) )); mongoOperation.remove(query1, User.class); Query query2 = new Query(); query2.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;bird&quot;)); User userTest2 = mongoOperation.findOne(query2, User.class); mongoOperation.remove(userTest2); // The first document that matches the query is returned and also // removed from the collection in the database. Query query3 = new Query(); query3.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;ant&quot;)); User userTest3 = mongoOperation.findAndRemove(query3, User.class); System.out.println(&quot;Deleted document : &quot; + userTest3); // either cat or elephant is deleted only, // common mistake, don&apos;t use for batch delete. /* Query query4 = new Query(); query4.addCriteria(Criteria.where(&quot;name&quot;) .exists(true) .orOperator( Criteria.where(&quot;name&quot;).is(&quot;cat&quot;), Criteria.where(&quot;name&quot;).is(&quot;elephant&quot;) ) ); mongoOperation.findAndRemove(query4, User.class); System.out.println(&quot;Deleted document : &quot; + userTest4); */ System.out.println(&quot;\nAll users : &quot;); List&lt;User&gt; allUsers = mongoOperation.findAll(User.class); for (User user : allUsers) &#123; System.out.println(user); &#125; mongoOperation.dropCollection(User.class); &#125;&#125; 输出：12345Deleted document : User [id=5162e0153004c3cb0a907370, ic=1001, name=ant, age=10]All users :User [id=5162e0153004c3cb0a907372, ic=1003, name=cat, age=30]User [id=5162e0153004c3cb0a907374, ic=1005, name=elephant, age=50]]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB基本操作]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FSpring%20Data%20MongoDB%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[本文主要借号如何使用Spring Data MongoDB进行CRUD操作，有两种方式：注解方式和XML方式。 1. 引入依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-mongodb&lt;/artifactId&gt; &lt;version&gt;1.10.7.RELEASE&lt;/version&gt;&lt;/dependency&gt; 2. 使用注解和XML配置2.1 注解继承AbstractMongoConfiguration是最快的方式，它配置了所有你需要的配置，如mongoTemplate。 12345678910111213141516171819import com.mongodb.Mongo;import com.mongodb.MongoClient;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.mongodb.config.AbstractMongoConfiguration;@Configurationpublic class SpringMongoConfig extends AbstractMongoConfiguration &#123; @Override protected String getDatabaseName() &#123; return &quot;test&quot;; &#125; @Bean @Override public Mongo mongo() throws Exception &#123; return new MongoClient(&quot;127.0.0.1&quot;); &#125;&#125; 但是，下面这种配置更灵活。 12345678910111213141516171819202122import com.mongodb.MongoClient;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.mongodb.MongoDbFactory;import org.springframework.data.mongodb.core.MongoTemplate;import org.springframework.data.mongodb.core.SimpleMongoDbFactory;import java.net.UnknownHostException;@Configurationpublic class SpringMongoConfig1 &#123; @Bean public MongoDbFactory mongoDbFactory() throws UnknownHostException &#123; return new SimpleMongoDbFactory(new MongoClient(), &quot;test&quot;); &#125; @Bean public MongoTemplate mongoTemplate() throws UnknownHostException &#123; MongoTemplate mongoTemplate = new MongoTemplate(mongoDbFactory()); return mongoTemplate; &#125;&#125; 使用AnnotationConfigApplicationContext加载配置：12ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringMongoConfig.class);MongoOperations mongoOperation = (MongoOperations)ctx.getBean(&quot;mongoTemplate&quot;); 2.2 XML12345678910111213141516&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mongo=&quot;http://www.springframework.org/schema/data/mongo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/mongo http://www.springframework.org/schema/data/mongo/spring-mongo.xsd&quot;&gt; &lt;mongo:mongo host=&quot;127.0.0.1&quot; port=&quot;27017&quot; /&gt; &lt;mongo:db-factory dbname=&quot;test&quot; /&gt; &lt;bean id=&quot;mongoTemplate&quot; class=&quot;org.springframework.data.mongodb.core.MongoTemplate&quot;&gt; &lt;constructor-arg name=&quot;mongoDbFactory&quot; ref=&quot;mongoDbFactory&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; 使用GenericXmlApplicationContext加载配置：12ApplicationContext ctx = new GenericXmlApplicationContext(&quot;SpringConfig.xml&quot;);MongoOperations mongoOperation = (MongoOperations)ctx.getBean(&quot;mongoTemplate&quot;); 3. User实体类12345678910111213141516import org.springframework.data.annotation.Id;import org.springframework.data.mongodb.core.mapping.Document;@Document(collection = &quot;users&quot;)public class User &#123; @Id private String id; String username; String password; //getter, setter, toString, Constructors&#125; 4. 实例 - CRUD操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354package com.zkzong.mongodb.springdata.core;import com.zkzong.mongodb.springdata.config.SpringMongoConfig;import com.zkzong.mongodb.springdata.model.User;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import org.springframework.data.mongodb.core.query.Update;import java.util.List;public class App &#123; public static void main(String[] args) &#123; // For XML //ApplicationContext ctx = new GenericXmlApplicationContext("SpringConfig.xml"); // For Annotation ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringMongoConfig.class); MongoOperations mongoOperation = (MongoOperations) ctx.getBean("mongoTemplate"); User user = new User("zong", "zong"); // save mongoOperation.save(user); // now user object got the created id. System.out.println("1. user : " + user); // query to search user Query searchUserQuery = new Query(Criteria.where("username").is("zong")); // find the saved user again. User savedUser = mongoOperation.findOne(searchUserQuery, User.class); System.out.println("2. find - savedUser : " + savedUser); // update password mongoOperation.updateFirst(searchUserQuery, Update.update("password", "new password"), User.class); // find the updated user object User updatedUser = mongoOperation.findOne(searchUserQuery, User.class); System.out.println("3. updatedUser : " + updatedUser); // delete mongoOperation.remove(searchUserQuery, User.class); // List, it should be empty now. List&lt;User&gt; listUser = mongoOperation.findAll(User.class); System.out.println("4. Number of user = " + listUser.size()); &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB插入]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FSpring%20Data%20MongoDB%E6%8F%92%E5%85%A5%2F</url>
    <content type="text"><![CDATA[在Spring Data MongoDB中，使用save()和insert()方法可以插入一个或多个文档到数据库。 123456789101112131415161718User user = new User(&quot;...&quot;);//save user object into &quot;user&quot; collection / table//class name will be used as collection namemongoOperation.save(user);//save user object into &quot;tableA&quot; collectionmongoOperation.save(user,&quot;tableA&quot;);//insert user object into &quot;user&quot; collection//class name will be used as collection namemongoOperation.insert(user);//insert user object into &quot;tableA&quot; collectionmongoOperation.insert(user, &quot;tableA&quot;);//insert a list of user objectsmongoOperation.insert(listofUser); 默认情况下，保存文档时如果没有指定集合名，使用类名作为集合名。 1. 插入save和insert的区别： save - 应该叫做saveOrUpdate()，当_id存在执行insert()，当_id不存在执行update()。 insert - 只是insert，如果_id存在会报错。 1234567891011//get an existed data, and update itUser userD1 = mongoOperation.findOne( new Query(Criteria.where(&quot;age&quot;).is(64)), User.class);userD1.setName(&quot;new name&quot;);userD1.setAge(100);//if you insert it, &apos;E11000 duplicate key error index&apos; error is generated.//mongoOperation.insert(userD1);//instead you should use save.mongoOperation.save(userD1); 2. 插入文档完整实例使用spring容器创建mongoTemplate。12345678910111213141516171819202122import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.mongodb.core.MongoTemplate;import com.mongodb.MongoClient;/** * Spring MongoDB configuration file * */@Configurationpublic class SpringMongoConfig&#123; public @Bean MongoTemplate mongoTemplate() throws Exception &#123; MongoTemplate mongoTemplate = new MongoTemplate(new MongoClient(&quot;127.0.0.1&quot;),&quot;yourdb&quot;); return mongoTemplate; &#125;&#125; 使用@Document指定集合名称。本例中，保存user对象时，保存的集合名称是users。 12345678910111213141516171819202122232425import java.util.Date;import org.springframework.data.annotation.Id;import org.springframework.data.mongodb.core.index.Indexed;import org.springframework.data.mongodb.core.mapping.Document;import org.springframework.format.annotation.DateTimeFormat;import org.springframework.format.annotation.DateTimeFormat.ISO;@Document(collection = &quot;users&quot;)public class User &#123; @Id private String id; @Indexed private String ic; private String name; private int age; @DateTimeFormat(iso = ISO.DATE_TIME) private Date createdDate; //getter and setter methods&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879import java.util.ArrayList;import java.util.Date;import java.util.List;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import com.mkyong.config.SpringMongoConfig;import com.mkyong.user.User;public class App &#123; public static void main(String[] args) &#123; // For Annotation ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringMongoConfig.class); MongoOperations mongoOperation = (MongoOperations) ctx.getBean(&quot;mongoTemplate&quot;); // Case1 - insert a user, put &quot;tableA&quot; as collection name System.out.println(&quot;Case 1...&quot;); User userA = new User(&quot;1000&quot;, &quot;apple&quot;, 54, new Date()); mongoOperation.save(userA, &quot;tableA&quot;); // find Query findUserQuery = new Query(); findUserQuery.addCriteria(Criteria.where(&quot;ic&quot;).is(&quot;1000&quot;)); User userA1 = mongoOperation.findOne(findUserQuery, User.class, &quot;tableA&quot;); System.out.println(userA1); // Case2 - insert a user, put entity as collection name System.out.println(&quot;Case 2...&quot;); User userB = new User(&quot;2000&quot;, &quot;orange&quot;, 64, new Date()); mongoOperation.save(userB); // find User userB1 = mongoOperation.findOne( new Query(Criteria.where(&quot;age&quot;).is(64)), User.class); System.out.println(userB1); // Case3 - insert a list of users System.out.println(&quot;Case 3...&quot;); User userC = new User(&quot;3000&quot;, &quot;metallica&quot;, 34, new Date()); User userD = new User(&quot;4000&quot;, &quot;metallica&quot;, 34, new Date()); User userE = new User(&quot;5000&quot;, &quot;metallica&quot;, 34, new Date()); List&lt;User&gt; userList = new ArrayList&lt;User&gt;(); userList.add(userC); userList.add(userD); userList.add(userE); mongoOperation.insert(userList, User.class); // find List&lt;User&gt; users = mongoOperation.find( new Query(Criteria.where(&quot;name&quot;).is(&quot;metallica&quot;)), User.class); for (User temp : users) &#123; System.out.println(temp); &#125; //save vs insert System.out.println(&quot;Case 4...&quot;); User userD1 = mongoOperation.findOne( new Query(Criteria.where(&quot;age&quot;).is(64)), User.class); userD1.setName(&quot;new name&quot;); userD1.setAge(100); //E11000 duplicate key error index, _id existed //mongoOperation.insert(userD1); mongoOperation.save(userD1); User userE1 = mongoOperation.findOne( new Query(Criteria.where(&quot;age&quot;).is(100)), User.class); System.out.println(userE1); &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java MongoDB认证]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FJava%20MongoDB%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[默认情况下，MongoDB不需要用户名和密码即可运行。本文介绍如何使用MongoDB驱动在安全模式下连接数据库。 1. 安全模式下启动MongoDB使用--auth参数启动MongoDB，然后需要用户名和密码才能对数据库进行操作。 1mongod --auth 添加相应的数据库和用户： 1234&gt; use admin&gt; db.addUser(&quot;admin&quot;,&quot;password&quot;)&gt; use testdb&gt; db.addUser(&quot;mkyong&quot;,&quot;password&quot;) 具体可以参考之前文章MongoDB认证 2. Java MongoDB认证123456789101112131415161718192021222324252627282930313233import com.mongodb.MongoClient;import com.mongodb.MongoCredential;import com.mongodb.ServerAddress;import com.mongodb.client.MongoDatabase;import java.util.ArrayList;import java.util.List;public class Authentication &#123; public static void main(String[] args) &#123; try &#123; //连接到MongoDB服务 如果是远程连接可以替换“localhost”为服务器所在IP地址 //ServerAddress()两个参数分别为 服务器地址 和 端口 ServerAddress serverAddress = new ServerAddress(&quot;localhost&quot;,27017); List&lt;ServerAddress&gt; addrs = new ArrayList&lt;ServerAddress&gt;(); addrs.add(serverAddress); //MongoCredential.createScramSha1Credential()三个参数分别为 用户名 数据库名称 密码 MongoCredential credential = MongoCredential.createScramSha1Credential(&quot;username&quot;, &quot;databaseName&quot;, &quot;password&quot;.toCharArray()); List&lt;MongoCredential&gt; credentials = new ArrayList&lt;MongoCredential&gt;(); credentials.add(credential); //通过连接认证获取MongoDB连接 MongoClient mongoClient = new MongoClient(addrs,credentials); //连接到数据库 MongoDatabase mongoDatabase = mongoClient.getDatabase(&quot;databaseName&quot;); System.out.println(&quot;Connect to database successfully&quot;); &#125; catch (Exception e) &#123; System.err.println( e.getClass().getName() + &quot;: &quot; + e.getMessage() ); &#125; &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB更新]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FSpring%20Data%20MongoDB%E6%9B%B4%E6%96%B0%2F</url>
    <content type="text"><![CDATA[在Spring Data MongoDB中，可以使用如下方法更新文档： save - 如果_id存在则更新，否则插入。更新整个文档。 updateFirst - 更新查询出的第一个文档。 updateMulti - 更新查询出的所有文档。 upsert - 如果没有查询出文档，则会创建一个新文档。 findAndModify - 和updateMulti相同，但是它有一个额外的选项可以返回更新前或更新后的文档。 1. saveOrUpdate - 例1假设下面的json数据插入到MongoDB。1234567&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1001&quot;, &quot;name&quot; : &quot;appleA&quot;, &quot;age&quot; : 20, &quot;createdDate&quot; : ISODate(&quot;2013-04-06T23:17:35.530Z&quot;)&#125; 查询文档，并使用save()方法修改和更新。123456789101112131415Query query = new Query();query.addCriteria(Criteria.where("name").is("appleA"));User userTest1 = mongoOperation.findOne(query, User.class);System.out.println("userTest1 - " + userTest1);//modify and update with save()userTest1.setAge(99);mongoOperation.save(userTest1);//get the updated object againUser userTest1_1 = mongoOperation.findOne(query, User.class);System.out.println("userTest1_1 - " + userTest1_1); 输出：12userTest1 - User [id=id, ic=1001, name=appleA, age=20, createdDate=Sat Apr 06 23:17:35 MYT 2013]userTest1_1 - User [id=id, ic=1001, name=appleA, age=99, createdDate=Sat Apr 06 23:17:35 MYT 2013] 1. saveOrUpdate - 例2这是一个失败的例子，请仔细阅读，这是一个相当普遍的错误。 假设下面的json数据插入到MongoDB。1234567&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1002&quot;, &quot;name&quot; : &quot;appleB&quot;, &quot;age&quot; : 20, &quot;createdDate&quot; : ISODate(&quot;2013-04-06T15:22:34.530Z&quot;)&#125; 使用Query获取只包含name字段的文档，返回的User对象的age、ic和createDate字段都为null，如果要修改age字段并更新，它会覆盖所有值而不是更新age字段。12345678910111213141516171819Query query = new Query();query.addCriteria(Criteria.where("name").is("appleB"));query.fields().include("name");User userTest2 = mongoOperation.findOne(query, User.class);System.out.println("userTest2 - " + userTest2);userTest2.setAge(99);mongoOperation.save(userTest2);// ooppss, you just override everything, it caused ic=null and// createdDate=nullQuery query1 = new Query();query1.addCriteria(Criteria.where("name").is("appleB"));User userTest2_1 = mongoOperation.findOne(query1, User.class);System.out.println("userTest2_1 - " + userTest2_1); 输出：12userTest2 - User [id=51603dba3004d7fffc202391, ic=null, name=appleB, age=0, createdDate=null]userTest2_1 - User [id=51603dba3004d7fffc202391, ic=null, name=appleB, age=99, createdDate=null] save()方法之后，age字段被正确更新了，但是ic和createdDate字段都是null，整个user对象都被更新了。为了只更新某个字段而不是整个对象，可以使用updateFirst()或者updateMulti()，而不是save()。 3. updateFirst更新符合查询条件的第一个文档。本例中，只有age字段被更新。1234567&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1003&quot;, &quot;name&quot; : &quot;appleC&quot;, &quot;age&quot; : 20, &quot;createdDate&quot; : ISODate(&quot;2013-04-06T23:22:34.530Z&quot;)&#125; 12345678910111213141516171819//returns only &apos;name&apos; fieldQuery query = new Query();query.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleC&quot;));query.fields().include(&quot;name&quot;);User userTest3 = mongoOperation.findOne(query, User.class);System.out.println(&quot;userTest3 - &quot; + userTest3);Update update = new Update();update.set(&quot;age&quot;, 100);mongoOperation.updateFirst(query, update, User.class);//returns everythingQuery query1 = new Query();query1.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleC&quot;));User userTest3_1 = mongoOperation.findOne(query1, User.class);System.out.println(&quot;userTest3_1 - &quot; + userTest3_1); 输出：12userTest3 - User [id=id, ic=null, name=appleC, age=0, createdDate=null]userTest3_1 - User [id=id, ic=1003, name=appleC, age=100, createdDate=Sat Apr 06 23:22:34 MYT 2013] 4. updateMulti更新符合查询条件的所有文档。1234567891011121314&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1004&quot;, &quot;name&quot; : &quot;appleD&quot;, &quot;age&quot; : 20, &quot;createdDate&quot; : ISODate(&quot;2013-04-06T15:22:34.530Z&quot;)&#125;&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1005&quot;, &quot;name&quot; : &quot;appleE&quot;, &quot;age&quot; : 20, &quot;createdDate&quot; : ISODate(&quot;2013-04-06T15:22:34.530Z&quot;)&#125; 123456789101112131415161718192021222324252627//show the use of $or operatorQuery query = new Query();query.addCriteria(Criteria .where(&quot;name&quot;).exists(true) .orOperator(Criteria.where(&quot;name&quot;).is(&quot;appleD&quot;), Criteria.where(&quot;name&quot;).is(&quot;appleE&quot;)));Update update = new Update();//update age to 11update.set(&quot;age&quot;, 11);//remove the createdDate fieldupdate.unset(&quot;createdDate&quot;);// if use updateFirst, it will update 1004 only.// mongoOperation.updateFirst(query4, update4, User.class);// update all matched, both 1004 and 1005mongoOperation.updateMulti(query, update, User.class);System.out.println(query.toString());List&lt;User&gt; usersTest4 = mongoOperation.find(query4, User.class);for (User userTest4 : usersTest4) &#123; System.out.println(&quot;userTest4 - &quot; + userTest4);&#125; 输出：12345Query: &#123; &quot;name&quot; : &#123; &quot;$exists&quot; : true&#125; , &quot;$or&quot; : [ &#123; &quot;name&quot; : &quot;appleD&quot;&#125; , &#123; &quot;name&quot; : &quot;appleE&quot;&#125;]&#125;, Fields: null, Sort: nulluserTest4 - User [id=id, ic=1004, name=appleD, age=11, createdDate=null]userTest4 - User [id=id, ic=1005, name=appleE, age=11, createdDate=null] 5. upsert如果查询到符合的文档就更新，如果没有就根据查询条件创建一个新的文档，就像findAndModifyElseCreate()。123&#123; //no data&#125; 1234567891011//search a document that doesn't existQuery query = new Query();query.addCriteria(Criteria.where("name").is("appleZ"));Update update = new Update();update.set("age", 21);mongoOperation.upsert(query, update, User.class);User userTest5 = mongoOperation.findOne(query, User.class);System.out.println("userTest5 - " + userTest5); 输出：1userTest5 - User [id=id, ic=null, name=appleZ, age=21, createdDate=null] 6. findAndModify使用一个操作查询、修改并获得更新后的对象。1234567&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1006&quot;, &quot;name&quot; : &quot;appleF&quot;, &quot;age&quot; : 20, &quot;createdDate&quot; : ISODate(&quot;2013-04-07T13:11:34.530Z&quot;)&#125; 12345678910111213Query query6 = new Query();query6.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleF&quot;));Update update6 = new Update();update6.set(&quot;age&quot;, 101);update6.set(&quot;ic&quot;, 1111);//FindAndModifyOptions().returnNew(true) = newly updated document//FindAndModifyOptions().returnNew(false) = old document (not update yet)User userTest6 = mongoOperation.findAndModify( query6, update6, new FindAndModifyOptions().returnNew(true), User.class);System.out.println(&quot;userTest6 - &quot; + userTest6); 输出：1userTest6 - User [id=id, ic=1111, name=appleF, age=101, createdDate=Sun Apr 07 13:11:34 MYT 2013] 7. 完整实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154import java.util.ArrayList;import java.util.Date;import java.util.List;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.data.mongodb.core.FindAndModifyOptions;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import org.springframework.data.mongodb.core.query.Update;import com.mkyong.config.SpringMongoConfig;import com.mkyong.model.User;public class UpdateApp &#123; public static void main(String[] args) &#123; // For Annotation ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringMongoConfig.class); MongoOperations mongoOperation = (MongoOperations) ctx.getBean(&quot;mongoTemplate&quot;); // insert 6 users for testing List&lt;User&gt; users = new ArrayList&lt;User&gt;(); User user1 = new User(&quot;1001&quot;, &quot;appleA&quot;, 20, new Date()); User user2 = new User(&quot;1002&quot;, &quot;appleB&quot;, 20, new Date()); User user3 = new User(&quot;1003&quot;, &quot;appleC&quot;, 20, new Date()); User user4 = new User(&quot;1004&quot;, &quot;appleD&quot;, 20, new Date()); User user5 = new User(&quot;1005&quot;, &quot;appleE&quot;, 20, new Date()); User user6 = new User(&quot;1006&quot;, &quot;appleF&quot;, 20, new Date()); users.add(user1); users.add(user2); users.add(user3); users.add(user4); users.add(user5); users.add(user6); mongoOperation.insert(users, User.class); // Case 1 ... find and update System.out.println(&quot;Case 1&quot;); Query query1 = new Query(); query1.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleA&quot;)); User userTest1 = mongoOperation.findOne(query1, User.class); System.out.println(&quot;userTest1 - &quot; + userTest1); userTest1.setAge(99); mongoOperation.save(userTest1); User userTest1_1 = mongoOperation.findOne(query1, User.class); System.out.println(&quot;userTest1_1 - &quot; + userTest1_1); // Case 2 ... select single field only System.out.println(&quot;\nCase 2&quot;); Query query2 = new Query(); query2.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleB&quot;)); query2.fields().include(&quot;name&quot;); User userTest2 = mongoOperation.findOne(query2, User.class); System.out.println(&quot;userTest2 - &quot; + userTest2); userTest2.setAge(99); mongoOperation.save(userTest2); // ooppss, you just override everything, it caused ic=null and // createdDate=null Query query2_1 = new Query(); query2_1.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleB&quot;)); User userTest2_1 = mongoOperation.findOne(query2_1, User.class); System.out.println(&quot;userTest2_1 - &quot; + userTest2_1); System.out.println(&quot;\nCase 3&quot;); Query query3 = new Query(); query3.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleC&quot;)); query3.fields().include(&quot;name&quot;); User userTest3 = mongoOperation.findOne(query3, User.class); System.out.println(&quot;userTest3 - &quot; + userTest3); Update update3 = new Update(); update3.set(&quot;age&quot;, 100); mongoOperation.updateFirst(query3, update3, User.class); Query query3_1 = new Query(); query3_1.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleC&quot;)); User userTest3_1 = mongoOperation.findOne(query3_1, User.class); System.out.println(&quot;userTest3_1 - &quot; + userTest3_1); System.out.println(&quot;\nCase 4&quot;); Query query4 = new Query(); query4.addCriteria(Criteria .where(&quot;name&quot;) .exists(true) .orOperator(Criteria.where(&quot;name&quot;).is(&quot;appleD&quot;), Criteria.where(&quot;name&quot;).is(&quot;appleE&quot;))); Update update4 = new Update(); update4.set(&quot;age&quot;, 11); update4.unset(&quot;createdDate&quot;); // update 1004 only. // mongoOperation.updateFirst(query4, update4, User.class); // update all matched mongoOperation.updateMulti(query4, update4, User.class); System.out.println(query4.toString()); List&lt;User&gt; usersTest4 = mongoOperation.find(query4, User.class); for (User userTest4 : usersTest4) &#123; System.out.println(&quot;userTest4 - &quot; + userTest4); &#125; System.out.println(&quot;\nCase 5&quot;); Query query5 = new Query(); query5.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleZ&quot;)); Update update5 = new Update(); update5.set(&quot;age&quot;, 21); mongoOperation.upsert(query5, update5, User.class); User userTest5 = mongoOperation.findOne(query5, User.class); System.out.println(&quot;userTest5 - &quot; + userTest5); System.out.println(&quot;\nCase 6&quot;); Query query6 = new Query(); query6.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;appleF&quot;)); Update update6 = new Update(); update6.set(&quot;age&quot;, 101); update6.set(&quot;ic&quot;, 1111); User userTest6 = mongoOperation.findAndModify(query6, update6, new FindAndModifyOptions().returnNew(true), User.class); System.out.println(&quot;userTest6 - &quot; + userTest6); mongoOperation.dropCollection(User.class); &#125;&#125; 输出：12345678910111213141516171819202122Case 1userTest1 - User [id=id, ic=1001, name=appleA, age=20, createdDate=Sun Apr 07 13:22:48 MYT 2013]userTest1_1 - User [id=id, ic=1001, name=appleA, age=99, createdDate=Sun Apr 07 13:22:48 MYT 2013]Case 2userTest2 - User [id=id, ic=null, name=appleB, age=0, createdDate=null]userTest2_1 - User [id=id, ic=null, name=appleB, age=99, createdDate=null]Case 3userTest3 - User [id=id, ic=null, name=appleC, age=0, createdDate=null]userTest3_1 - User [id=id, ic=1003, name=appleC, age=100, createdDate=Sun Apr 07 13:22:48 MYT 2013]Case 4Query: &#123; &quot;name&quot; : &#123; &quot;$exists&quot; : true&#125; , &quot;$or&quot; : [ &#123; &quot;name&quot; : &quot;appleD&quot;&#125; , &#123; &quot;name&quot; : &quot;appleE&quot;&#125;]&#125;, Fields: null, Sort: nulluserTest4 - User [id=id, ic=1004, name=appleD, age=11, createdDate=null]userTest4 - User [id=id, ic=1005, name=appleE, age=11, createdDate=null]Case 5userTest5 - User [id=id, ic=null, name=appleZ, age=21, createdDate=null]Case 6userTest6 - User [id=id, ic=1006, name=appleF, age=20, createdDate=Sun Apr 07 13:22:48 MYT 2013]]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB查询]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FSpring%20Data%20MongoDB%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[本文介绍一下使用Query、Criteria和其他常见操作查询文档的例子。 测试数据1234567&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1001&quot;, &quot;name&quot; : &quot;ant&quot;, &quot;age&quot; : 10 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1002&quot;, &quot;name&quot; : &quot;bird&quot;, &quot;age&quot; : 20 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1003&quot;, &quot;name&quot; : &quot;cat&quot;, &quot;age&quot; : 30 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1004&quot;, &quot;name&quot; : &quot;dog&quot;, &quot;age&quot; : 40 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1005&quot;, &quot;name&quot; : &quot;elephant&quot;, &quot;age&quot; : 50 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;id&quot;), &quot;ic&quot; : &quot;1006&quot;, &quot;name&quot; : &quot;frog&quot;, &quot;age&quot; : 60 &#125; 1. BasicQuery如果数据MongoDB控制台的find()命令，只需要把原生的查询放到BasicQuery就可以了。12345BasicQuery query1 = new BasicQuery(&quot;&#123; age : &#123; $lt : 40 &#125;, name : &apos;cat&apos; &#125;&quot;);User userTest1 = mongoOperation.findOne(query1, User.class);System.out.println(&quot;query1 - &quot; + query1.toString());System.out.println(&quot;userTest1 - &quot; + userTest1); 输出：12query1 - Query: &#123; &quot;age&quot; : &#123; &quot;$lt&quot; : 40&#125; , &quot;name&quot; : &quot;cat&quot;&#125;, Fields: null, Sort: &#123; &#125;userTest1 - User [id=id, ic=1003, name=cat, age=30] 2. findOnefindOne返回符合查询条件的一个wendang文档，如果有多个查询条件，可以使用Criteria.and()方法。123456Query query2 = new Query();query2.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;dog&quot;).and(&quot;age&quot;).is(40));User userTest2 = mongoOperation.findOne(query2, User.class);System.out.println(&quot;query2 - &quot; + query2.toString());System.out.println(&quot;userTest2 - &quot; + userTest2); 输出：12query2 - Query: &#123; &quot;name&quot; : &quot;dog&quot; , &quot;age&quot; : 40&#125;, Fields: null, Sort: nulluserTest2 - User [id=id, ic=1004, name=dog, age=40] 3. find和$in查询并返回符合查询的文档列表。本例也展示了$in的用法。1234567891011121314List&lt;Integer&gt; listOfAge = new ArrayList&lt;Integer&gt;();listOfAge.add(10);listOfAge.add(30);listOfAge.add(40);Query query3 = new Query();query3.addCriteria(Criteria.where(&quot;age&quot;).in(listOfAge));List&lt;User&gt; userTest3 = mongoOperation.find(query3, User.class);System.out.println(&quot;query3 - &quot; + query3.toString());for (User user : userTest3) &#123; System.out.println(&quot;userTest3 - &quot; + user);&#125; 输出：1234query3 - Query: &#123; &quot;age&quot; : &#123; &quot;$in&quot; : [ 10 , 30 , 40]&#125;&#125;, Fields: null, Sort: nulluserTest3 - User [id=id, ic=1001, name=ant, age=10]userTest3 - User [id=id, ic=1003, name=cat, age=30]userTest3 - User [id=id, ic=1004, name=dog, age=40] find，$gt，$lt，$and查询并返回符合查询的文档列表。本例也展示了$gt、$lt和$and的用法。123456789Query query4 = new Query();query4.addCriteria(Criteria.where(&quot;age&quot;).lt(40).and(&quot;age&quot;).gt(10));List&lt;User&gt; userTest4 = mongoOperation.find(query4, User.class);System.out.println(&quot;query4 - &quot; + query4.toString());for (User user : userTest4) &#123; System.out.println(&quot;userTest4 - &quot; + user);&#125; 上面例子报错了：12Due to limitations of the com.mongodb.BasicDBObject, you can&apos;t add a second &apos;age&apos; expressionspecified as &apos;age : &#123; &quot;$gt&quot; : 10&#125;&apos;. Criteria already contains &apos;age : &#123; &quot;$lt&quot; : 40&#125;&apos;. 提示说不能在同一个字段上使用Criteria.and()，可以使用Criteria.andOperator()修复改问题。123456789101112131415Query query4 = new Query();query4.addCriteria( Criteria.where(&quot;age&quot;).exists(true) .andOperator( Criteria.where(&quot;age&quot;).gt(10), Criteria.where(&quot;age&quot;).lt(40) ));List&lt;User&gt; userTest4 = mongoOperation.find(query4, User.class);System.out.println(&quot;query4 - &quot; + query4.toString());for (User user : userTest4) &#123; System.out.println(&quot;userTest4 - &quot; + user);&#125; 输出：123query4 - Query: &#123; &quot;age&quot; : &#123; &quot;$lt&quot; : 40&#125; , &quot;$and&quot; : [ &#123; &quot;age&quot; : &#123; &quot;$gt&quot; : 10&#125;&#125;]&#125;, Fields: null, Sort: nulluserTest4 - User [id=51627a0a3004cc5c0af72964, ic=1002, name=bird, age=20]userTest4 - User [id=51627a0a3004cc5c0af72965, ic=1003, name=cat, age=30] 5. find并排序12345678910Query query5 = new Query();query5.addCriteria(Criteria.where(&quot;age&quot;).gte(30));query5.with(new Sort(Sort.Direction.DESC, &quot;age&quot;));List&lt;User&gt; userTest5 = mongoOperation.find(query5, User.class);System.out.println(&quot;query5 - &quot; + query5.toString());for (User user : userTest5) &#123; System.out.println(&quot;userTest5 - &quot; + user);&#125; 输出：12345query5 - Query: &#123; &quot;age&quot; : &#123; &quot;$gte&quot; : 30&#125;&#125;, Fields: null, Sort: &#123; &quot;age&quot; : -1&#125;userTest5 - User [id=id, ic=1006, name=frog, age=60]userTest5 - User [id=id, ic=1005, name=elephant, age=50]userTest5 - User [id=id, ic=1004, name=dog, age=40]userTest5 - User [id=id, ic=1003, name=cat, age=30] 6. find和$regex使用正则表达式查找。 123456789Query query6 = new Query();query6.addCriteria(Criteria.where(&quot;name&quot;).regex(&quot;D.*G&quot;, &quot;i&quot;));List&lt;User&gt; userTest6 = mongoOperation.find(query6, User.class);System.out.println(&quot;query6 - &quot; + query6.toString());for (User user : userTest6) &#123; System.out.println(&quot;userTest6 - &quot; + user);&#125; 输出：12query6 - Query: &#123; &quot;name&quot; : &#123; &quot;$regex&quot; : &quot;D.*G&quot; , &quot;$options&quot; : &quot;i&quot;&#125;&#125;, Fields: null, Sort: nulluserTest6 - User [id=id, ic=1004, name=dog, age=40] 7. 完整实例1234567891011121314151617181920212223import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.mongodb.core.MongoTemplate;import com.mongodb.MongoClient;/** * Spring MongoDB configuration file * */@Configurationpublic class SpringMongoConfig&#123; public @Bean MongoTemplate mongoTemplate() throws Exception &#123; MongoTemplate mongoTemplate = new MongoTemplate(new MongoClient(&quot;127.0.0.1&quot;),&quot;yourdb&quot;); return mongoTemplate; &#125;&#125; 1234567891011121314151617181920212223import java.util.Date;import org.springframework.data.annotation.Id;import org.springframework.data.mongodb.core.index.Indexed;import org.springframework.data.mongodb.core.mapping.Document;import org.springframework.format.annotation.DateTimeFormat;import org.springframework.format.annotation.DateTimeFormat.ISO;@Document(collection = &quot;users&quot;)public class User &#123; @Id private String id; @Indexed private String ic; private String name; private int age; //getter, setter and constructor methods&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129import java.util.ArrayList;import java.util.List;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.data.domain.Sort;import org.springframework.data.mongodb.core.MongoOperations;import org.springframework.data.mongodb.core.query.BasicQuery;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import com.mkyong.config.SpringMongoConfig;import com.mkyong.model.User;/** * Query example * * @author mkyong * */public class QueryApp &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringMongoConfig.class); MongoOperations mongoOperation = (MongoOperations) ctx.getBean(&quot;mongoTemplate&quot;); // insert 6 users for testing List&lt;User&gt; users = new ArrayList&lt;User&gt;(); User user1 = new User(&quot;1001&quot;, &quot;ant&quot;, 10); User user2 = new User(&quot;1002&quot;, &quot;bird&quot;, 20); User user3 = new User(&quot;1003&quot;, &quot;cat&quot;, 30); User user4 = new User(&quot;1004&quot;, &quot;dog&quot;, 40); User user5 = new User(&quot;1005&quot;, &quot;elephant&quot;,50); User user6 = new User(&quot;1006&quot;, &quot;frog&quot;, 60); users.add(user1); users.add(user2); users.add(user3); users.add(user4); users.add(user5); users.add(user6); mongoOperation.insert(users, User.class); System.out.println(&quot;Case 1 - find with BasicQuery example&quot;); BasicQuery query1 = new BasicQuery(&quot;&#123; age : &#123; $lt : 40 &#125;, name : &apos;cat&apos; &#125;&quot;); User userTest1 = mongoOperation.findOne(query1, User.class); System.out.println(&quot;query1 - &quot; + query1.toString()); System.out.println(&quot;userTest1 - &quot; + userTest1); System.out.println(&quot;\nCase 2 - find example&quot;); Query query2 = new Query(); query2.addCriteria(Criteria.where(&quot;name&quot;).is(&quot;dog&quot;).and(&quot;age&quot;).is(40)); User userTest2 = mongoOperation.findOne(query2, User.class); System.out.println(&quot;query2 - &quot; + query2.toString()); System.out.println(&quot;userTest2 - &quot; + userTest2); System.out.println(&quot;\nCase 3 - find list $inc example&quot;); List&lt;Integer&gt; listOfAge = new ArrayList&lt;Integer&gt;(); listOfAge.add(10); listOfAge.add(30); listOfAge.add(40); Query query3 = new Query(); query3.addCriteria(Criteria.where(&quot;age&quot;).in(listOfAge)); List&lt;User&gt; userTest3 = mongoOperation.find(query3, User.class); System.out.println(&quot;query3 - &quot; + query3.toString()); for (User user : userTest3) &#123; System.out.println(&quot;userTest3 - &quot; + user); &#125; System.out.println(&quot;\nCase 4 - find list $and $lt, $gt example&quot;); Query query4 = new Query(); // it hits error // query4.addCriteria(Criteria.where(&quot;age&quot;).lt(40).and(&quot;age&quot;).gt(10)); query4.addCriteria( Criteria.where(&quot;age&quot;).exists(true).andOperator( Criteria.where(&quot;age&quot;).gt(10), Criteria.where(&quot;age&quot;).lt(40) ) ); List&lt;User&gt; userTest4 = mongoOperation.find(query4, User.class); System.out.println(&quot;query4 - &quot; + query4.toString()); for (User user : userTest4) &#123; System.out.println(&quot;userTest4 - &quot; + user); &#125; System.out.println(&quot;\nCase 5 - find list and sorting example&quot;); Query query5 = new Query(); query5.addCriteria(Criteria.where(&quot;age&quot;).gte(30)); query5.with(new Sort(Sort.Direction.DESC, &quot;age&quot;)); List&lt;User&gt; userTest5 = mongoOperation.find(query5, User.class); System.out.println(&quot;query5 - &quot; + query5.toString()); for (User user : userTest5) &#123; System.out.println(&quot;userTest5 - &quot; + user); &#125; System.out.println(&quot;\nCase 6 - find by regex example&quot;); Query query6 = new Query(); query6.addCriteria(Criteria.where(&quot;name&quot;).regex(&quot;D.*G&quot;, &quot;i&quot;)); List&lt;User&gt; userTest6 = mongoOperation.find(query6, User.class); System.out.println(&quot;query6 - &quot; + query6.toString()); for (User user : userTest6) &#123; System.out.println(&quot;userTest6 - &quot; + user); &#125; mongoOperation.dropCollection(User.class); &#125;&#125; 输出：1234567891011121314151617181920212223242526272829Case 1 - find with BasicQuery examplequery1 - Query: &#123; &quot;age&quot; : &#123; &quot;$lt&quot; : 40&#125; , &quot;name&quot; : &quot;cat&quot;&#125;, Fields: null, Sort: &#123; &#125;userTest1 - User [id=id, ic=1003, name=cat, age=30]Case 2 - find examplequery2 - Query: &#123; &quot;name&quot; : &quot;dog&quot; , &quot;age&quot; : 40&#125;, Fields: null, Sort: nulluserTest2 - User [id=id, ic=1004, name=dog, age=40]Case 3 - find list $inc examplequery3 - Query: &#123; &quot;age&quot; : &#123; &quot;$in&quot; : [ 10 , 30 , 40]&#125;&#125;, Fields: null, Sort: nulluserTest3 - User [id=id, ic=1001, name=ant, age=10]userTest3 - User [id=id, ic=1003, name=cat, age=30]userTest3 - User [id=id, ic=1004, name=dog, age=40]Case 4 - find list $and $lt, $gt examplequery4 - Query: &#123; &quot;age&quot; : &#123; &quot;$lt&quot; : 40&#125; , &quot;$and&quot; : [ &#123; &quot;age&quot; : &#123; &quot;$gt&quot; : 10&#125;&#125;]&#125;, Fields: null, Sort: nulluserTest4 - User [id=id, ic=1002, name=bird, age=20]userTest4 - User [id=id, ic=1003, name=cat, age=30]Case 5 - find list and sorting examplequery5 - Query: &#123; &quot;age&quot; : &#123; &quot;$gte&quot; : 30&#125;&#125;, Fields: null, Sort: &#123; &quot;age&quot; : -1&#125;userTest5 - User [id=id, ic=1006, name=frog, age=60]userTest5 - User [id=id, ic=1005, name=elephant, age=50]userTest5 - User [id=id, ic=1004, name=dog, age=40]userTest5 - User [id=id, ic=1003, name=cat, age=30]Case 6 - find by regex examplequery6 - Query: &#123; &quot;name&quot; : &#123; &quot;$regex&quot; : &quot;D.*G&quot; , &quot;$options&quot; : &quot;i&quot;&#125;&#125;, Fields: null, Sort: nulluserTest6 - User [id=id, ic=1004, name=dog, age=40]]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Data MongoDB：使用GridFS保存二进制文件]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2FSpring%20Data%20MongoDB%EF%BC%9A%E4%BD%BF%E7%94%A8GridFS%E4%BF%9D%E5%AD%98%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[在MongoDB中，可以使用GridFS保存二进制文件。本文介绍如何使用GridFsTemplate保存和读取图片文件。 1. GridFS - 保存（使用Spring注解方式）1234567891011121314151617181920212223242526272829303132import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.mongodb.config.AbstractMongoConfiguration;import org.springframework.data.mongodb.gridfs.GridFsTemplate;import com.mongodb.Mongo;import com.mongodb.MongoClient;/** * Spring MongoDB configuration file * */@Configurationpublic class SpringMongoConfig extends AbstractMongoConfiguration&#123; @Bean public GridFsTemplate gridFsTemplate() throws Exception &#123; return new GridFsTemplate(mongoDbFactory(), mappingMongoConverter()); &#125; @Override protected String getDatabaseName() &#123; return &quot;yourdb&quot;; &#125; @Override @Bean public Mongo mongo() throws Exception &#123; return new MongoClient(&quot;127.0.0.1&quot;); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;import org.springframework.context.ApplicationContext;import org.springframework.context.annotation.AnnotationConfigApplicationContext;import org.springframework.data.mongodb.gridfs.GridFsOperations;import com.mkyong.config.SpringMongoConfig;import com.mongodb.BasicDBObject;import com.mongodb.DBObject;/** * GridFs example * * @author zong * */public class GridFsAppStore &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringMongoConfig.class); GridFsOperations gridOperations = (GridFsOperations) ctx.getBean(&quot;gridFsTemplate&quot;); DBObject metaData = new BasicDBObject(); metaData.put(&quot;extra1&quot;, &quot;anything 1&quot;); metaData.put(&quot;extra2&quot;, &quot;anything 2&quot;); InputStream inputStream = null; try &#123; inputStream = new FileInputStream(&quot;/Users/mkyong/Downloads/testing.png&quot;); gridOperations.store(inputStream, &quot;testing.png&quot;, &quot;image/png&quot;, metaData); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; finally &#123; if (inputStream != null) &#123; try &#123; inputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; System.out.println(&quot;Done&quot;); &#125;&#125; 使用MongoDB控制台查看保存的文件：12345678910111213141516171819202122&gt; show dbsyourdb 0.203125GB&gt; use yourdbswitched to db yourdb&gt; show collectionsfs.chunksfs.filessystem.indexes&gt; db.fs.files.find()&#123; &quot;_id&quot; : ObjectId(&quot;51641c5630045c9b3db35afc&quot;), &quot;chunkSize&quot; : NumberLong(262144),&quot;length&quot; : NumberLong(4238), &quot;md5&quot; : &quot;9969527cd95a5a573f15e953f0036800&quot;, &quot;filename&quot; : &quot;testing.png&quot;,&quot;contentType&quot; : &quot;image/png&quot;, &quot;uploadDate&quot; : ISODate(&quot;2013-04-09T13:49:10.104Z&quot;),&quot;aliases&quot; : null, &quot;metadata&quot; : &#123; &quot;extra1&quot; : &quot;anything 1&quot;, &quot;extra2&quot; : &quot;anything 2&quot; &#125; &#125;&gt;&gt; db.fs.chunks.find()&#123; &quot;_id&quot; : ObjectId(&quot;51641c5630045c9b3db35afd&quot;),&quot;files_id&quot; : ObjectId(&quot;51641c5630045c9b3db35afc&quot;), &quot;n&quot; : 0,&quot;data&quot; : BinData(0,&quot;/9j/4AAQSkZJRgABAgAAZ......EQH/9k=&quot;) &#125; 图片信息保存在fs.files，图片的二进制流保存在fs.chunks，通过_id和files_id关联。 2. GridFS - 读取（使用xml方式）12345678910111213141516171819&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:mongo=&quot;http://www.springframework.org/schema/data/mongo&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/data/mongo http://www.springframework.org/schema/data/mongo/spring-mongo.xsd http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;&gt; &lt;mongo:db-factory id=&quot;mongoDbFactory&quot; dbname=&quot;yourdb&quot; /&gt; &lt;mongo:mapping-converter id=&quot;converter&quot; /&gt; &lt;bean name=&quot;gridFsTemplate&quot; class=&quot;org.springframework.data.mongodb.gridfs.GridFsTemplate&quot;&gt; &lt;constructor-arg ref=&quot;mongoDbFactory&quot; /&gt; &lt;constructor-arg ref=&quot;converter&quot; /&gt; &lt;/bean&gt;&lt;/beans&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import java.io.IOException;import java.util.List;import org.springframework.context.ApplicationContext;import org.springframework.context.support.GenericXmlApplicationContext;import org.springframework.data.mongodb.core.query.Criteria;import org.springframework.data.mongodb.core.query.Query;import org.springframework.data.mongodb.gridfs.GridFsOperations;import com.mongodb.BasicDBObject;import com.mongodb.DBObject;import com.mongodb.gridfs.GridFSDBFile;/** * GridFs example * * @author zong * */public class GridFsAppRead &#123; public static void main(String[] args) &#123; ApplicationContext ctx = new GenericXmlApplicationContext(&quot;SpringConfig.xml&quot;); GridFsOperations gridOperations = (GridFsOperations) ctx.getBean(&quot;gridFsTemplate&quot;); List&lt;GridFSDBFile&gt; result = gridOperations.find( new Query().addCriteria(Criteria.where(&quot;filename&quot;).is(&quot;testing.png&quot;))); for (GridFSDBFile file : result) &#123; try &#123; System.out.println(file.getFilename()); System.out.println(file.getContentType()); //save as another image file.writeTo(&quot;/Users/mkyong/Downloads/new-testing.png&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(&quot;Done&quot;); &#125;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MongoDB认证]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2Fmongodb%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[简介对MongoDB进行访问控制就是在访问之前先对用户校验，只有当用户有相关权限是才能根据角色执行相关操作。 MongoDB支持各种认证机制，具体请查看Authentication Mechanisms。 下面使用单独的mongod实例和默认的认证机制说明访问控制。 复制集和集群当访问控制可用时，复制集和集群需要内部认证。详细介绍请查看Internal Authentication。 管理员当访问控制可用时，需要保证在admin数据库中有一个用户有userAdmin或userAdminAnyDatabase角色。这个用户可以管理用户和角色，如创建用户，授权或撤销角色，创建或修改自定义角色。 步骤1. 以没有访问控制的方式启动MongoDB如，下面命令以没有访问控制的方式启动一个单独的mongod实例： mongod --port 27017 --dbpath /data/db1 2. 连接到实例mongo --port 27017 可以指定相应的参数连接到mongo，如–host。 3. 创建管理员用户在admin数据库中添加一个userAdminAnyDatabase角色的用户。例如，下面命令在admin数据库中创建了myUserAdmin用户。 注意：你创建用户的数据库（如admin）是用户认证数据库。虽然用户认证了该数据库，但还可以拥有其他数据库的角色。12345678use admindb.createUser( &#123; user: &quot;myUserAdmin&quot;, pwd: &quot;abc123&quot;, roles: [ &#123; role: &quot;userAdminAnyDatabase&quot;, db: &quot;admin&quot; &#125; ] &#125;) 4. 使用访问控制重启MongoDB使用参数--auth重启mongod。如果使用了配置文件，那就是security.authorization。 mongod --auth --port 27017 --dbpath /data/db1 5. 使用管理员账号连接并认证通过mongo shell，有两种连接方式： 1、 通过用户信息认证连接 2、 不认证连接，然后使用db.auth()方法认证 连接时认证开启mongo shell，使用-u &lt;username&gt;，-p &lt;password&gt;和--authenticationDatabase &lt;database&gt;参数 mongo --port 27017 -u &quot;myUserAdmin&quot; -p &quot;abc123&quot; --authenticationDatabase &quot;admin&quot; 连接后认证mongo --port 27017 切换到认证数据库（如admin），然后使用db.auth(&lt;username&gt;, &lt;pwd&gt;)方法认证： 12use admindb.auth(&quot;myUserAdmin&quot;, &quot;abc123&quot; ) 6. 创建其他用户一旦被认证为管理员，就可以使用db.createUser()创建其他用户，可以分配内置角色或自定义角色给用户。 用户myUserAdmin只有管理用户和角色的权限。当用户myUserAdmin试图执行其他操作，如从test数据库的foo集合中读取数据，MongoDB就会报错。 123456789use testdb.createUser( &#123; user: &quot;myTester&quot;, pwd: &quot;xyz123&quot;, roles: [ &#123; role: &quot;readWrite&quot;, db: &quot;test&quot; &#125;, &#123; role: &quot;read&quot;, db: &quot;reporting&quot; &#125; ] &#125;) 7. 使用myTester连接并认证连接时认证mongo --port 27017 -u &quot;myTester&quot; -p &quot;xyz123&quot; --authenticationDatabase &quot;test&quot; 连接后认证123mongo --port 27017use testdb.auth(&quot;myTester&quot;, &quot;xyz123&quot; ) 插入数据用户myTester对test数据库有读写权限，对reporting数据库有读权限。如，可以对test数据库执行插入操作： db.foo.insert( { x: 1, y: 1 } )]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MongoDB基本操作]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2Fmongodb%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[本文主要介绍mongodb的一些基本操作，如创建、更新、查找、删除记录和创建索引。 1. 安装MongoDB安装可以参考前两篇文章，分别介绍了在Windows和Ubuntu上安装的步骤。 使用mongod启动MongoDB 123456789$./mongodTue Sep 11 21:55:36 [initandlisten] MongoDB starting :pid=72280 port=27017 dbpath=/data/db/ 64-bit host=Yongs-MacBook-Air.localTue Sep 11 21:55:36 [initandlisten] db version v2.0.7, pdfile version 4.5Tue Sep 11 21:55:36 [initandlisten] options: &#123;&#125;Tue Sep 11 21:55:36 [initandlisten] journal dir=/data/db/journalTue Sep 11 21:55:36 [initandlisten] recover : no journal files present, no recovery neededTue Sep 11 21:55:36 [websvr] admin web console waiting for connections on port 28017Tue Sep 11 21:55:36 [initandlisten] waiting for connections on port 27017 2. 连接到MongoDB123$ ./mongoMongoDB shell version: 2.0.7connecting to: test 3. 创建数据库或集合（表）MongoDB中，在第一次插入数据时数据库和集合会被自动创建。使用use database-name切换到数据库（即使该数据库没有被创建）。 下面例子中，插入数据后，数据库mkyong和集合users都会被创建。 123456789$ ./mongoMongoDB shell version: 2.0.7connecting to: test&gt; use mkyongswitched to db mkyong&gt; db.users.insert(&#123;username:&quot;mkyong&quot;,password:&quot;123456&quot;&#125;)&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;504f45cd17f6c778042c3c07&quot;), &quot;username&quot; : &quot;mkyong&quot;, &quot;password&quot; : &quot;123456&quot; &#125; 你应该知道的三个数据库命令： show dbs - 显示所有数据库 use db_name - 切换到db_name数据库 show collections - 显示当前数据库的所有集合 在MongoDB中，集合对应SQL中的表。 4. 插入数据使用db.tablename.insert({data})或db.tablename.save({data})命令插入数据。 1234&gt; db.users.save(&#123;username:&quot;google&quot;,password:&quot;google123&quot;&#125;)&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;504f45cd17f6c778042c3c07&quot;), &quot;username&quot; : &quot;mkyong&quot;, &quot;password&quot; : &quot;123456&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;504f48ea17f6c778042c3c0a&quot;), &quot;username&quot; : &quot;google&quot;, &quot;password&quot; : &quot;google123&quot; &#125; 5. 更新数据使用db.tablename.update({criteria},{$set: {new value}})命令更新数据。下面例子更新了用户名为mkyong的密码。 1234&gt; db.users.update(&#123;username:&quot;mkyong&quot;&#125;,&#123;$set:&#123;password:&quot;hello123&quot;&#125;&#125;)&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;504f48ea17f6c778042c3c0a&quot;), &quot;username&quot; : &quot;google&quot;, &quot;password&quot; : &quot;google123&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;504f45cd17f6c778042c3c07&quot;), &quot;password&quot; : &quot;hello123&quot;, &quot;username&quot; : &quot;mkyong&quot; &#125; 6. 查询数据使用db.tablename.find({criteria})命令查询数据。 6.1 查询集合users的所有数据123&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;504f48ea17f6c778042c3c0a&quot;), &quot;username&quot; : &quot;google&quot;, &quot;password&quot; : &quot;google123&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;504f45cd17f6c778042c3c07&quot;), &quot;password&quot; : &quot;hello123&quot;, &quot;username&quot; : &quot;mkyong&quot; &#125; 6.2 查询username为google的数据12&gt; db.users.find(&#123;username:&quot;google&quot;&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;504f48ea17f6c778042c3c0a&quot;), &quot;username&quot; : &quot;google&quot;, &quot;password&quot; : &quot;google123&quot; &#125; 6.3 查询username长度小于等于2的数据1db.users.find(&#123;$where:&quot;this.username.length&lt;=2&quot;&#125;) 6.4 查询包含username字段的数据1db.users.find(&#123;username:&#123;$exists : true&#125;&#125;) 7. 删除数据使用db.tablename.remove({criteria})命令删除数据。123&gt; db.users.remove(&#123;username:&quot;google&quot;&#125;)&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;504f45cd17f6c778042c3c07&quot;), &quot;password&quot; : &quot;hello123&quot;, &quot;username&quot; : &quot;mkyong&quot; &#125; 注意：删除集合中的所有数据，使用db.tablename.remove()删除集合，使用db.tablename.drop() 8. 索引索引能加快查询数据的速度。 8.1 查看集合users所有索引，默认_id是主键并自动创建。123456789101112&gt; db.users.getIndexes()[ &#123; &quot;v&quot; : 1, &quot;key&quot; : &#123; &quot;_id&quot; : 1 &#125;, &quot;ns&quot; : &quot;mkyong.users&quot;, &quot;name&quot; : &quot;_id_&quot; &#125;]&gt; 8.2 使用db.tablename.ensureIndex(column)命令创建索引下面例子在username列创建索引。1234567891011121314151617181920&gt; db.users.ensureIndex(&#123;username:1&#125;)&gt; db.users.getIndexes()[ &#123; &quot;v&quot; : 1, &quot;key&quot; : &#123; &quot;_id&quot; : 1 &#125;, &quot;ns&quot; : &quot;mkyong.users&quot;, &quot;name&quot; : &quot;_id_&quot; &#125;, &#123; &quot;v&quot; : 1, &quot;key&quot; : &#123; &quot;username&quot; : 1 &#125;, &quot;ns&quot; : &quot;mkyong.users&quot;, &quot;name&quot; : &quot;username_1&quot; &#125;] 8.3 使用db.tablename.dropIndex(column)命令删除索引1234567891011121314&gt; db.users.dropIndex(&#123;username:1&#125;)&#123; &quot;nIndexesWas&quot; : 2, &quot;ok&quot; : 1 &#125;&gt; db.users.getIndexes()[ &#123; &quot;v&quot; : 1, &quot;key&quot; : &#123; &quot;_id&quot; : 1 &#125;, &quot;ns&quot; : &quot;mkyong.users&quot;, &quot;name&quot; : &quot;_id_&quot; &#125;]&gt; 8.4 使用db.tablename.ensureIndex({column},{unique:true})命令创建唯一索引123456789101112131415161718192021&gt; db.users.ensureIndex(&#123;username:1&#125;,&#123;unique:true&#125;);&gt; db.users.getIndexes()[ &#123; &quot;v&quot; : 1, &quot;key&quot; : &#123; &quot;_id&quot; : 1 &#125;, &quot;ns&quot; : &quot;mkyong.users&quot;, &quot;name&quot; : &quot;_id_&quot; &#125;, &#123; &quot;v&quot; : 1, &quot;key&quot; : &#123; &quot;username&quot; : 1 &#125;, &quot;unique&quot; : true, &quot;ns&quot; : &quot;mkyong.users&quot;, &quot;name&quot; : &quot;username_1&quot; &#125;] 9. 帮助最后，使用help()命令提供帮助手册。 9.1 help - 所有可用命令12345678&gt; help db.help() help on db methods db.mycoll.help() help on collection methods rs.help() help on replica set methods help admin administrative help help connect connecting to a db help help keys key shortcuts //... 9.2 db.help() - 显示db帮助12345678&gt; db.help()DB methods: db.addUser(username, password[, readOnly=false]) db.auth(username, password) db.cloneDatabase(fromhost) db.commandHelp(name) returns the help for the command db.copyDatabase(fromdb, todb, fromhost) //... 9.3 db.collection.help() - 显示collection帮助123456789&gt; db.users.help()DBCollection help db.users.find().help() - show DBCursor help db.users.count() db.users.dataSize() db.users.distinct( key ) - eg. db.users.distinct( &apos;x&apos; ) db.users.drop() drop the collection db.users.dropIndex(name) //... 9.4 db.collection.function.help() - 显示function帮助123456789&gt; db.users.find().help()find() modifiers .sort( &#123;...&#125; ) .limit( n ) .skip( n ) .count() - total # of objects matching query, ignores skip,limit .size() - total # of objects cursor would return, honors skip,limit .explain([verbose]) //...]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[删除集合中的_class列]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2F%E5%88%A0%E9%99%A4%E9%9B%86%E5%90%88%E4%B8%AD%E7%9A%84_class%E5%88%97%2F</url>
    <content type="text"><![CDATA[默认情况下，Spring Data的MappingMongoConverter为MongoDb中的每个对象添加了一个额外的_class列。例如：1234567public class User &#123; String username; String password; //...getters and setters&#125; 保存：123MongoOperations mongoOperation = (MongoOperations)ctx.getBean(&quot;mongoTemplate&quot;);User user = new User(&quot;mkyong&quot;, &quot;password123&quot;);mongoOperation.save(user, &quot;users&quot;); 结果：123456&gt; db.users.find()&#123; &quot;_class&quot; : &quot;com.mkyong.user.User&quot;, &quot;_id&quot; : ObjectId(&quot;5050aef830041f24ff2bd16e&quot;), &quot;password&quot; : &quot;new password&quot;, &quot;username&quot; : &quot;mkyong&quot;&#125; Spring Data创建了额外的_class列。为了删除额外的_class列，重写MappingMongoConverter，传入new DefaultMongoTypeMapper(null)。 下面介绍了两种删除_class的方法：注解和xml。 1. 注解12345678910111213141516171819202122@Configurationpublic class SpringMongoConfig &#123; @Bean public MongoDbFactory mongoDbFactory() throws Exception &#123; return new SimpleMongoDbFactory(new Mongo(), &quot;database&quot;); &#125; @Bean public MongoTemplate mongoTemplate() throws Exception &#123; //remove _class MappingMongoConverter converter = new MappingMongoConverter(mongoDbFactory(), new MongoMappingContext()); converter.setTypeMapper(new DefaultMongoTypeMapper(null)); MongoTemplate mongoTemplate = new MongoTemplate(mongoDbFactory(), converter); return mongoTemplate; &#125;&#125; 2. XML12345678910111213141516171819202122&lt;mongo:mongo host=&quot;localhost&quot; port=&quot;27017&quot; /&gt;&lt;mongo:db-factory dbname=&quot;database&quot; /&gt; &lt;bean id=&quot;mappingContext&quot; class=&quot;org.springframework.data.mongodb.core.mapping.MongoMappingContext&quot; /&gt; &lt;bean id=&quot;defaultMongoTypeMapper&quot; class=&quot;org.springframework.data.mongodb.core.convert.DefaultMongoTypeMapper&quot;&gt; &lt;constructor-arg name=&quot;typeKey&quot;&gt;&lt;null/&gt;&lt;/constructor-arg&gt; &lt;/bean&gt; &lt;bean id=&quot;mappingMongoConverter&quot; class=&quot;org.springframework.data.mongodb.core.convert.MappingMongoConverter&quot;&gt; &lt;constructor-arg name=&quot;mongoDbFactory&quot; ref=&quot;mongoDbFactory&quot; /&gt; &lt;constructor-arg name=&quot;mappingContext&quot; ref=&quot;mappingContext&quot; /&gt; &lt;property name=&quot;typeMapper&quot; ref=&quot;defaultMongoTypeMapper&quot; /&gt; &lt;/bean&gt; &lt;bean id=&quot;mongoTemplate&quot; class=&quot;org.springframework.data.mongodb.core.MongoTemplate&quot;&gt; &lt;constructor-arg name=&quot;mongoDbFactory&quot; ref=&quot;mongoDbFactory&quot; /&gt; &lt;constructor-arg name=&quot;mongoConverter&quot; ref=&quot;mappingMongoConverter&quot; /&gt; &lt;/bean&gt; 3. Spring Boot1234567891011@Beanpublic MongoTemplate mongoTemplate(MongoDbFactory mongoDbFactory, MongoMappingContext context) &#123; MappingMongoConverter converter = new MappingMongoConverter(new DefaultDbRefResolver(mongoDbFactory), context); converter.setTypeMapper(new DefaultMongoTypeMapper(null)); MongoTemplate mongoTemplate = new MongoTemplate(mongoDbFactory, converter); return mongoTemplate;&#125; 4. 测试_class已经被删除了。 12345&gt; db.users.find()&#123; &quot;_id&quot; : ObjectId(&quot;random code&quot;), &quot;password&quot; : &quot;new password&quot;, &quot;username&quot; : &quot;mkyong&quot;&#125;]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MongoDB导入导出]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2Fmongodb%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%2F</url>
    <content type="text"><![CDATA[本文介绍使用mongoexport和mongoimport命令备份和恢复数据。 1. 使用mongoexport命令备份数据库相关命令参数：1234567891011$ mongoexport --helpExport MongoDB data to CSV, TSV or JSON files.options: -h [ --host ] arg mongo host to connect to ( &lt;set name&gt;/s1,s2 for -u [ --username ] arg username -p [ --password ] arg password -d [ --db ] arg database to use -c [ --collection ] arg collection to use (some commands) -q [ --query ] arg query filter, as a JSON string -o [ --out ] arg output file; if not specified, stdout is used 1.1 导出所有文档所有字段到文件domain-bk.json文件123$ mongoexport -d webmitta -c domain -o domain-bk.jsonconnected to: 127.0.0.1exported 10951 records 1.2 导出所有哦文档的domain和worth字段123$ mongoexport -d webmitta -c domain -f &quot;domain,worth&quot; -o domain-bk.jsonconnected to: 127.0.0.1exported 10951 records 1.3 通过条件导出文档，下面例子导出worth &gt; 100000的文档123$mongoexport -d webmitta -c domain -f &quot;domain,worth&quot; -q &apos;&#123;worth:&#123;$gt:100000&#125;&#125;&apos; -o domain-bk.jsonconnected to: 127.0.0.1exported 10903 records 1.4 使用用户名和密码导出远程服务器的文档123$ mongoexport -h id.mongolab.com:47307 -d heroku_app -c domain -u username123 -p password123 -o domain-bk.jsonconnected to: id.mongolab.com:47307exported 10951 records 注意：导出的文档都是json格式 2. 使用mongoimport命令恢复数据库相关命令参数： 123456789101112131415$ mongoimport --helpconnected to: 127.0.0.1no collection specified!Import CSV, TSV or JSON data into MongoDB.options: -h [ --host ] arg mongo host to connect to ( &lt;set name&gt;/s1,s2 for sets) -u [ --username ] arg username -p [ --password ] arg password -d [ --db ] arg database to use -c [ --collection ] arg collection to use (some commands) -f [ --fields ] arg comma separated list of field names e.g. -f name,age --file arg file to import from; if not specified stdin is used --drop drop collection first --upsert insert or update objects that already exist 2.1 从domain-bk.json导入文档，导入的数据库名为webmitta2，集合名为domain2。不存在的数据库或集合会被自动创建。123$ mongoimport -d webmitta2 -c domain2 --file domain-bk.jsonconnected to: 127.0.0.1Wed Apr 10 13:26:12 imported 10903 objects 2.2 导入文档，无则插入，有则更新（根据_id）123$ mongoimport -d webmitta2 -c domain2 --file domain-bk.json --upsertconnected to: 127.0.0.1Wed Apr 10 13:26:12 imported 10903 objects 2.3 使用用户名和密码连接远程服务器，并从本地文件domain-bk.json导入文档123$ mongoimport -h id.mongolab.com:47307 -d heroku_app -c domain -u username123 -p password123 --file domain-bk.jsonconnected to: id.mongolab.com:47307Wed Apr 10 13:26:12 imported 10903 objects]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Ubuntu上安装MongoDB]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2F%E5%9C%A8Ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85MongoDB%2F</url>
    <content type="text"><![CDATA[1. 简介可以使用.deb包安装MongoDB社区版，也可以使用Ubuntu自己的MongoDB包。当然官方的MongoDB包会比较新。 mongodb-org-server包提供了初始化脚本，使用/etc/mongod.conf配置文件启动mongod。 包提供的默认/etc/mongod.conf配置文件把bind_ip设置为127.0.0.1。 2. 安装MongoDB第一步 导入包管理系统使用的公钥Ubuntu包管理工具（如dpkg或apt）使用GPG key保证了包的一致性和可靠性。使用如下命令导入GPG key：1sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6 第二步 创建list文件创建/etc/apt/sources.list.d/mongodb-org-3.4.listlist文件1echo &quot;deb [ arch=amd64,arm64 ] http://repo.mongodb.org/apt/ubuntu xenial/mongodb-org/3.4 multiverse&quot; | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list 第三步 reload本地包数据库1sudo apt-get update 第四步 安装MongoDB包安装最新稳定版MongoDB1sudo apt-get install -y mongodb-org 3. 运行MongoDBMongoDB默认把数据存储在/var/lib/mongodb，把日志文件存储在/var/log/mongodb。也可以在/etc/mongod.conf文件中指定日志和数据文件。 第一步 运行MongoDB1sudo service mongod start 第二步 验证MongoDB开启成功检查/var/log/mongodb/mongod.log日志验证mongod是否启动成功[initandlisten] waiting for connections on port &lt;port&gt;&lt;port&gt;是在/etc/mongod.conf中配置的端口号，默认为27017。 第三步 停止MongoDB1sudo service mongod stop 第四步 重启MongoDB1sudo service mongod restart 第五步 开始使用MongoDB4. 卸载MongoDB为了完全删除MongoDB，必须删除MongoDB应用，配置文件以及数据和日志文件。 第一步 停止MongoDB服务1sudo service mongod stop 第二步 移除包1sudo apt-get perge mongodb-org* 第三步 删除数据12sudo rm -r /var/log/mongodbsudo rm -r /var/lib/mongodb]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[在Windows上安装MongoDB]]></title>
    <url>%2F2019%2F04%2F21%2FMongoDB%2F%E5%9C%A8Windows%E4%B8%8A%E5%AE%89%E8%A3%85MongoDB%2F</url>
    <content type="text"><![CDATA[MongoDB的下载和安装与其他软件没有什么区别，在此不再详细介绍，可以直接去官网下载安装。 作为个人学习使用，建议安装社区版。 下面主要介绍MongoDB的配置和运行。 1. 运行MongoDB第一步 安装MongoDB环境MongoDB需要data目录存储数据。它默认的data目录是/data/db。 可以使用--dbpath指定data目录，如：1&quot;C:\Program Files\MongoDB\Server\3.4\bin\mongod.exe&quot; --dbpath d:\test\mongodb\data 如果路径中有空格，可以使用双引号包含路径，如：1&quot;C:\Program Files\MongoDB\Server\3.4\bin\mongod.exe&quot; --dbpath &quot;d:\test\mongo db data&quot; 当然，也可以在配置文件中指定dbpath。 第二步 启动MongoDB运行mongod.exe启动MongoDB。其命令如下：1&quot;C:\Program Files\MongoDB\Server\3.4\bin\mongod.exe&quot; 第三步 连接到MongoDB使用mongo.exe连接MongoDB，其命令如下：1&quot;C:\Program Files\MongoDB\Server\3.4\bin\mongo.exe 停止MongoDB服务，只需在运行mongod的命令行界面按Ctrl+C。 2. 把MongoDB配置成Windows服务第一步 打开管理员命令行打开命令行界面，然后按Ctrl+Shift+Enter以管理员身份运行命令行。然后执行剩下的步骤。 第二步 创建目录创建目录存放db和log文件12mkdir c:\data\dbmkdir c:\data\log ###第三步 创建配置文件 创建配置文件，该文件中必须设置systemLog.path，其他为可选。例如，创建配置文件C:\Program Files\MongoDB\Server\3.4\mongod.cfg，在其中配置systemLog.path和storage.dbPath。12345systemLog: destination: file path: c:\data\log\mongod.logstorage: dbPath: c:\data\db 第四步 安装MongoDB服务注意：必须以管理员权限运行下面的命令行。 运行mongod.exe，并使用两个参数--install和--config，其中--config指定了之前创建的配置文件。 1&quot;C:\Program Files\MongoDB\Server\3.4\bin\mongod.exe&quot; --config &quot;C:\Program Files\MongoDB\Server\3.4\mongod.cfg&quot; --install 第五步 运行MongoDB服务1net start MongoDB 第六步 停止MongoDB服务1net stop MongoDB 移除MongoDB服务：1&quot;C:\Program Files\MongoDB\Server\3.4\bin\mongod.exe&quot; --remove]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ModelAttribute注解的使用]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2F%40ModelAttribute%E6%B3%A8%E8%A7%A3%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[在SpringMVC的Controller中使用@ModelAttribute时，其位置包括下面三种： 应用在方法上 应用在方法的参数上 应用在方法上，并且方法也使用了@RequestMapping 应用在方法上首先说明一下，被@ModelAttribute注解的方法会在Controller每个方法执行之前都执行，因此对于一个Controller中包含多个URL的时候，要谨慎使用。 1）使用@ModelAttribute注解无返回值的方法1234567891011121314@Controller@RequestMapping(value = "/modelattribute")public class ModelAttributeController &#123; @ModelAttribute public void myModel(@RequestParam(required = false) String abc, Model model) &#123; model.addAttribute("attributeName", abc); &#125; @RequestMapping(value = "/method") public String method() &#123; return "method"; &#125;&#125; 这个例子，在请求/modelattribute/method?abc=aaa后，会先执行myModel方法，然后接着执行method方法，参数abc的值被放到Model中后，接着被带到method方法中。 当返回视图/modelattribute/method时，Model会被带到页面上，当然你在使用@RequestParam的时候可以使用required来指定参数是否是必须的。 如果把myModel和method合二为一，代码如下，这也是我们最常用的方法：12345@RequestMapping(value = "/method")public String method(@RequestParam(required = false) String abc, Model model) &#123; model.addAttribute("attributeName", abc); return "method";&#125; 2）使用@ModelAttribute注解带有返回值的方法123456789101112131415@ModelAttributepublic String myModel(@RequestParam(required = false) String abc) &#123; return abc;&#125;@ModelAttributepublic Student myModel(@RequestParam(required = false) String abc) &#123; Student student = new Student(abc); return student;&#125;@ModelAttributepublic int myModel(@RequestParam(required = false) int number) &#123; return number;&#125; 对于这种情况，返回值对象会被默认放到隐含的Model中，在Model中的key为返回值首字母小写，value为返回的值。 上面3种情况等同于：123model.addAttribute(&quot;string&quot;, abc);model.addAttribute(&quot;int&quot;, number);model.addAttribute(&quot;student&quot;, student); 在jsp页面使用${int}表达式时会报错：javax.el.ELException: Failed to parse the expression [${int}]。解决办法：在tomcat的配置文件conf/catalina.properties添加配置org.apache.el.parser.SKIP_IDENTIFIER_CHECK=true 如果只能这样，未免太局限了，我们很难接受key为string、int、float等等这样的。 想自定义其实很简单，只需要给@ModelAttribute添加value属性即可，如下：1234@ModelAttribute(value = "num")public int myModel(@RequestParam(required = false) int number) &#123; return number;&#125; 这样就相当于model.addAttribute(&quot;num&quot;, number);。 使用@ModelAttribute注解的参数12345678910111213141516171819202122@Controller@RequestMapping(value = "/modelattribute")public class ModelAttributeParamController &#123; @ModelAttribute(value = "attributeName") public String myModel(@RequestParam(required = false) String abc) &#123; return abc; &#125; @ModelAttribute public void myModel3(Model model) &#123; model.addAttribute("name", "zong"); model.addAttribute("age", 20); &#125; @RequestMapping(value = "/param") public String param(@ModelAttribute("attributeName") String str, @ModelAttribute("name") String str2, @ModelAttribute("age") int str3) &#123; return "param"; &#125;&#125; 从代码中可以看出，使用@ModelAttribute注解的参数，意思是从前面的Model中提取对应名称的属性。 这里提及一下@SessionAttributes的使用： 如果在类上面使用了@SessionAttributes(&quot;attributeName&quot;)注解，而本类中恰巧存在attributeName，则会将attributeName放入到session作用域。 如果在类上面使用了@SessionAttributes(&quot;attributeName&quot;)注解，SpringMVC会在执行方法之前，自动从session中读取key为attributeName的值，并注入到Model中。所以我们在方法的参数中使用ModelAttribute(&quot;attributeName&quot;)就会正常的从Model读取这个值，也就相当于获取了session中的值。 使用了@SessionAttributes之后，Spring无法知道什么时候要清掉@SessionAttributes存进去的数据，如果要明确告知，也就是在方法中传入SessionStatus对象参数，并调用status.setComplete就可以了。 应用在方法上，并且方法上也使用了@RequestMapping12345678910@Controller@RequestMapping(value = "/modelattribute")public class ModelAttributeController &#123; @RequestMapping(value = "/test") @ModelAttribute("name") public String test(@RequestParam(required = false) String name) &#123; return name; &#125;&#125; 这种情况下，返回值String（或者其他对象）就不再是视图了，而是放入到Model中的值，此时对应的页面就是@RequestMapping的值test。如果类上有@RequestMapping，则视图路径还要加上类的@RequestMapping的值，本例中视图路径为modelattribute/test.jsp。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CrudRepository JpaRepository PagingAndSortingRepository之间的区别]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FCrudRepository%20JpaRepository%20PagingAndSortingRepository%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[1. 简介本文介绍三种不同的Spring Data repository和它们的功能，包含以下三种： CrudRepository PagingAndSortingRepository JpaRepository简单地说，Spring Data中的每个repository都继承自Repository接口，但是，除此之外，它们每个又有不同的功能。 2. Spring Data Repositories首先介绍JpaRepository，它继承自PagingAndSortingRepository，而PagingAndSortingRepository又继承自CrudRepository。每个都有自己的功能： CrudRepository提供CRUD的功能。 PagingAndSortingRepository提供分页和排序功能 JpaRepository提供JPA相关的方法，如刷新持久化数据、批量删除。 由于三者之间的继承关系，所以JpaRepository包含了CrudRepository和PagingAndSortingRepository所有的API。 当我们不需要JpaRepository和PagingAndSortingRepository提供的功能时，可以简单使用CrudRepository。 下面我们通过例子更好地理解它们提供的API。首先创建Product实体：123456789@Entitypublic class Product &#123; @Id private long id; private String name; // getters and setters&#125; 然后实现通过名称查询Product的操作：1234@Repositorypublic interface ProductRepository extends JpaRepository&lt;Product, Long&gt; &#123; Product findByName(String productName);&#125; 这样就实现了通过名称查询Product的方法，Spring Data Repository根据方法名自动生成相应的实现。 3. CrudRepository先看下CrudRepository接口的源码：123456789101112131415public interface CrudRepository&lt;T, ID extends Serializable&gt; extends Repository&lt;T, ID&gt; &#123; &lt;S extends T&gt; S save(S entity); T findOne(ID primaryKey); Iterable&lt;T&gt; findAll(); Long count(); void delete(T entity); boolean exists(ID primaryKey);&#125; 方法功能介绍： save(…) – 保存多个对象。这里，我们可以传多个对象批量保存。 findOne(…) – 根据传入的主键值获取单一对象。 findAll(…) – 查询数据库中所有对象。 count() – 返回表中记录总数。 delete(…) – 根据传入的对象删除记录。 exists(…) – 根据传入的主键值判断对象是否存在。CrudRepository接口看起来非常简单，但实际上它提供了所有基本查询保存等操作。 4. PagingAndSortingRepositoryPagingAndSortingRepository接口的源码如下：1234567public interface PagingAndSortingRepository&lt;T, ID extends Serializable&gt; extends CrudRepository&lt;T, ID&gt; &#123; Iterable&lt;T&gt; findAll(Sort sort); Page&lt;T&gt; findAll(Pageable pageable);&#125; 该接口提供了findAll(Pageable pageable)这个方法，它是实现分页的关键。使用Pageable时，需要创建Pageable对象并至少设置下面3个参数： 1、 每页包含记录数 2、 当前页数 3、 排序假设我们要显示第一页的结果集，一页不超过5条记录，并根据lastName升序排序，下面代码展示如何使用PageRequest和Sort获取这个结果：12Sort sort = new Sort(new Sort.Order(Direction.ASC, "lastName"));Pageable pageable = new PageRequest(0, 5, sort); 5. JpaRepositoryJpaRepository接口源码：123456789101112131415public interface JpaRepository&lt;T, ID extends Serializable&gt; extends PagingAndSortingRepository&lt;T, ID&gt; &#123; List&lt;T&gt; findAll(); List&lt;T&gt; findAll(Sort sort); List&lt;T&gt; save(Iterable&lt;? extends T&gt; entities); void flush(); T saveAndFlush(T entity); void deleteInBatch(Iterable&lt;T&gt; entities);&#125; 简单介绍下每个方法的作用： findAll() – 获取数据库表中所有实体。 findAll(…) – 根据条件获取排序后的所有实体。 save(…) – 保存多个实体。这里可以传入多个对象批量保存。 flush() – 将所有挂起的任务刷新到数据库。 saveAndFlush(…) – 保存实体并实时刷新到数据库。 deleteInBatch(…) – 删除多个实体。这里可以传入多个对象并批量删除。显然，JpaRepository接口继承自PagingAndSortingRepository，PagingAndSortingRepository继承自CrudRepository，所以JpaRepository拥有CrudRepository所有的方法。 6. Spring Data Repositories的缺点尽管Spring Data Repositories有很多优点，但是它们也有缺点： 这种方式把我们的代码和类库以及具体抽象（如：Page和Pageable）绑定。它们不是这个类库独有的，但是我们必须小心不要暴露这些内部实现的细节。 通过继承CrudRepository接口，我们暴露了所有方法。这可能满足大部分情况，但是我们可能会遇到这样的情况：我们希望获得对公开的方法的更细粒度的控制，例如创建一个不包含save(…)和delete(…)的ReadOnlyRepository，此时继承CrudRepository就不满足要求了。 参考文献：CrudRepository, JpaRepository, and PagingAndSortingRepository in Spring Data]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JPA、Hibernate、Spring Data JPA之间的关系]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FJPA%E3%80%81Hibernate%E3%80%81Spring%20Data%20JPA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[什么是JPA？ 全称Java Persistence API，可以通过注解或者XML描述【对象-关系表】之间的映射关系，并将实体对象持久化到数据库中。 为我们提供了： 1）ORM映射元数据：JPA支持XML和注解两种元数据的形式，元数据描述对象和表之间的映射关系，框架据此将实体对象持久化到数据库表中； 如：@Entity、@Table、@Column、@Transient等注解。 2）JPA 的API：用来操作实体对象，执行CRUD操作，框架在后台替我们完成所有的事情，开发者从繁琐的JDBC和SQL代码中解脱出来。 如：entityManager.merge(T t); 3）JPQL查询语言：通过面向对象而非面向数据库的查询语言查询数据，避免程序的SQL语句紧密耦合。 如：from Student s where s.name = ? 但是： JPA仅仅是一种规范，也就是说JPA仅仅定义了一些接口，而接口是需要实现才能工作的。所以底层需要某种实现，而Hibernate就是实现了JPA接口的ORM框架。 也就是说： JPA是一套ORM规范，Hibernate实现了JPA规范！如图： 什么是spring data jpa？ spirng data jpa是spring提供的一套简化JPA开发的框架，按照约定好的【方法命名规则】写dao层接口，就可以在不写接口实现的情况下，实现对数据库的访问和操作。同时提供了很多除了CRUD之外的功能，如分页、排序、复杂查询等等。 Spring Data JPA 可以理解为 JPA 规范的再次封装抽象，底层还是使用了 Hibernate 的 JPA 技术实现。如图： 接口约定命名规则： 关键字 方法命名 where条件 And findByLastnameAndFirstname … where x.lastname = ?1 and x.firstname = ?2 Or findByLastnameOrFirstname … where x.lastname = ?1 or x.firstname = ?2 Is,Equals findByFirstname,findByFirstnameIs,findByFirstnameEquals … where x.firstname = ?1 Between findByStartDateBetween … where x.startDate between ?1 and ?2 LessThan findByAgeLessThan … where x.age &lt; ?1 LessThanEqual findByAgeLessThanEqual … where x.age &lt;= ?1 GreaterThan findByAgeGreaterThan … where x.age &gt; ?1 GreaterThanEqual findByAgeGreaterThanEqual … where x.age &gt;= ?1 After findByStartDateAfter … where x.startDate &gt; ?1 Before findByStartDateBefore … where x.startDate &lt; ?1 IsNull findByAgeIsNull … where x.age is null IsNotNull,NotNull findByAge(Is)NotNull … where x.age not null Like findByFirstnameLike … where x.firstname like ?1 NotLike findByFirstnameNotLike … where x.firstname not like ?1 StartingWith findByFirstnameStartingWith … where x.firstname like ?1 (parameter bound with appended %) EndingWith findByFirstnameEndingWith … where x.firstname like ?1 (parameter bound with prepended %) Containing findByFirstnameContaining … where x.firstname like ?1 (parameter bound wrapped in %) OrderBy findByAgeOrderByLastnameDesc … where x.age = ?1 order by x.lastname desc Not findByLastnameNot … where x.lastname &lt;&gt; ?1 In findByAgeIn(Collection ages) … where x.age in ?1 NotIn findByAgeNotIn(Collection ages) … where x.age not in ?1 True findByActiveTrue() … where x.active = true False findByActiveFalse() … where x.active = false IgnoreCase findByFirstnameIgnoreCase … where UPPER(x.firstame) = UPPER(?1) 实例： 12// 遵循命名规范，执行多条件查询Standard findByNameAndMaxLength(String name, Integer maxLength); 12// 模糊查询Standard findByNameLike(String name); spring boot集成spring data jpa只需两步： 第一步：导入maven坐标 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; 第二步：yml配置文件中配置jpa信息 123456789101112spring: datasource: driver-class-name: com.mysql.jdbc.Driver url: jdbc:mysql://localhost:3306/test?useSSL=false&amp;characterEncoding=utf8 username: root password: root jpa: database: mysql show-sql: true hibernate: ddl-auto: update]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%20AOP%2F</url>
    <content type="text"></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Bean生命周期]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%20Bean%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring MVC tutorialspoint]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%20MVC%20tutorialspoint%2F</url>
    <content type="text"><![CDATA[1. Spring MVC简介Spring MVC提供model-view-controller架构和已有组件，可以开发灵活、松散耦合的web应用。MVC模式把应用分成不同的概念（输入逻辑、业务逻辑和UI逻辑），组件之间松散耦合。 Model封装应用数据，一般上由POJO组成。 View负责渲染模型数据，一般上它会生成HTML，然后由客户端浏览器解析。 Controller负责处理用户请求和响应，构建合适的模型，传给view进行渲染。 1.1 DispatcherServletSpring MVC框架通过DispatcherServlet处理所有的HTTP请求和响应。Spring MVC中DispatcherServlet的请求流程如下图： DispatcherServlet对传入的HTTP请求的事件响应顺序如下： 接收到HTTP请求后，DispatcherServlet通过HandleMapping调用合适的Controller。 Controller收到请求，根据GET或者POST的不同调用合适的service方法。service方法根据业务逻辑设置模型数据，把view名称返回给DispatcherServlet。 DispatcherServlet通过ViewResolver选取定义的view对请求进行响应。 一旦view初始化，DispatcherServlet把模型数据传给view，最后在浏览器渲染。 上面提到的组件，如HandlerMapping、Controller和ViewResolver是WebApplicationContext的一部分，WebApplicationContext是ApplicationContext的扩展，为web应用添加了一些必要的特性。 1.2 必需配置需要在web.xml文件中使用URL映射手动配置DispatcherServlet处理的映射请求。下面代码展示了HelloWeb DispatcherServlet的定义和映射。 12345678910111213141516171819202122&lt;web-app id="WebApp_ID" version="2.4" xmlns="http://java.sun.com/xml/ns/j2ee" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd"&gt; &lt;display-name&gt;Spring MVC Application&lt;/display-name&gt; &lt;servlet&gt; &lt;servlet-name&gt;HelloWeb&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloWeb&lt;/servlet-name&gt; &lt;url-pattern&gt;*.jsp&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;/web-app&gt; web.xml文件放在web应用的WebContent/WEB-INF目录下。首先，在HelloWeb DispatcherServlet初始化时，框架会试图从名为[servlet-name]-servlet.xml的文件加载应用上下文，这个文件位于WebContent/WEB-INF目录下。本例中文件名为HelloWeb-servlet.xml。其次，标签指明由那个DispatcherServlet处理什么URL。这里所有以.jsp结尾的HTTP请求都由HelloWeb DispatcherServlet处理。 如果不想使用像[servlet-name]-servlet.xml的默认名称和WebContent/WEB-INF的默认位置，可以在web.xml中添加ContextLoaderListener的servlet监听器，自定义文件名称和位置，如下： 123456789101112131415&lt;web-app...&gt;&lt;!-------- DispatcherServlet definition goes here-----&gt;....&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/HelloWeb-servlet.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;listener&gt; &lt;listener-class&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;&lt;/web-app&gt; 或1234567891011&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt; org.springframework.web.servlet.DispatcherServlet &lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;WEB-INF/spring-mvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 现在来看下HelloWeb-servlet.xml这个配置文件，它位于WebContent/WEB-INF目录下： 1234567891011121314151617&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:context="http://www.springframework.org/schema/context" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-3.0.xsd"&gt; &lt;context:component-scan base-package="com.tutorialspoint" /&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt;&lt;/beans&gt; 以下是HelloWeb-servlet.xml文件的重要知识点： [servlet-name]-servlet.xml文件用来创建定义的bean，重写全局范围定义的相同名称的bean。 context:component-scan...标签用来激活Spring MVC注解扫描功能，它使用了像@Controller和@RequestMapping等注解。 InternalResourceViewResolver有定义的规则处理视图名称。像上面定义的规则，名为hello的逻辑视图是/WEB-INF/jsp/hello.jsp的代理。下一节介绍怎样创建实际的组件，如Controller、Model和View等。 1.3 定义ControllerDispatcherServlet把请求委托给controller以执行特定的功能。@Controller注解表明指定的类作为controller。@RequestMapping注解用来把URL映射到某个类或特定的处理方法。1234567891011@Controller@RequestMapping("/hello")public class HelloController&#123; @RequestMapping(method = RequestMethod.GET) public String printHello(ModelMap model) &#123; model.addAttribute("message", "Hello Spring MVC Framework!"); return "hello"; &#125;&#125; @Controller注解定义类作为Spring MVC控制器。第一个@RequestMapping表明这个控制器中所有的处理方法都是相对/hello路径的。注解@RequestMapping(method = RequestMethod.GET)用来声明pringHello()方法作为控制器默认的方法处理HTTP GET请求。可以定义另一个方法处理相同URL的POST请求。可以在上面controller的@RequestMapping中添加额外的属性，如下：12345678910@Controllerpublic class HelloController&#123; @RequestMapping(value = "/hello", method = RequestMethod.GET) public String printHello(ModelMap model) &#123; model.addAttribute("message", "Hello Spring MVC Framework!"); return "hello"; &#125;&#125; value属性表明URL和哪个处理方法映射，method属性表明方法处理HTTP GET请求。以下是controller中重要的知识点： 在方法中可以定义需要的业务逻辑。也可以按要求在方法中调用另一个方法。 基于定义的业务逻辑，可以在方法中创建model。可以设定不同的model属性，这些属性可以被view访问以呈现最终结果。例子中创建了一个包含“message”属性的model。 定义的方法可以返回一个包含view名称的字符串，用来渲染model。例子中返回“hello”作为逻辑视图名称。 1.4 创建JSP视图Spring MVC支持多种类型的视图，包括JSP，HTML，PDF，Excel，XML，Velocity模板，XSLT，JSON，Atom，RSS和JasperReport等。但是通常我们使用JSTL来实现JSP模板。hello视图（/WEB-INF/hello/hello.jsp）如下：12345678&lt;html&gt; &lt;head&gt; &lt;title&gt;Hello Spring MVC&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;$&#123;message&#125;&lt;/h2&gt; &lt;/body&gt;&lt;/html&gt; ${message}是在Controller中设置的属性。在视图中可以有多个属性显示。 EL表达式无法获取值的解决办法：修改web.xml1234567&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;web-app id=&quot;mvc&quot; version=&quot;2.4&quot; xmlns=&quot;http://java.sun.com/xml/ns/j2ee&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd&quot;&gt; 1.5 Spring Web MVC实例1.5.1 简单实例通过URLhttp://localhost:8080/HelloWeb/hello访问代码下载 1.5.2 表单提交实例通过URLhttp://localhost:8080/HelloWeb1/student访问return new ModelAndView(&quot;student&quot;, &quot;command&quot;, new Student());如果在JSP文件中使用form:form标签，spring要求一个包含”command”的对象。@ModelAttribute注解参考文章Using @ModelAttribute on a method argument代码下载 1.5.3 重定向实例通过URLhttp://localhost:8080/HelloWeb2/index访问代码下载 1.5.4 静态页面实例通过URLhttp://localhost:8080/HelloWeb3/index访问mvc:resources.../标签用来映射静态页面。mapping属性必须是Ant模式来指定http请求的URL。location属性必须指定一个或多个有静态页面的可用资源目录位置，静态页面可以是images、 stylesheets、JavaScript和其他静态内容。多个资源位置可以使用逗号分隔的值列表。代码下载 1.5.5 异常处理实例通过URLhttp://localhost:8080/HelloWeb4/student访问代码下载 原文：http://www.tutorialspoint.com/spring/spring_web_mvc_framework.htm]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring事务管理]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[什么是事务？事务指的是逻辑上的一组操作，这组操作要么全部成功，要么全部失败。 事务的特性： 原子性 一致性 隔离性 持久性 原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。一致性指事务前后数据的完整性必须保持一致。隔离性指多个用户并发访问数据库时，一个用户的事务不能被其他用户的事务所干扰，多个并发事务之间数据要相互隔离。持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久的，即使数据库发生故障也不应该对其有任何影响。 Spring事务管理高层抽象主要包含3个接口： PlatformTransactionManager 事务管理器 TransactionDefinition 事务定义信息（隔离、传播、超时、只读） TransactionStatus 事务具体运行状态 Spring为不同的持久化框架提供了不同PlatformTransactionManager接口实现 如果不考虑隔离性，会引发安全问题如下：脏读、不可重复读、幻读 脏读：一个事务读取了另一个事务改写但还未提交的数据，如果这些数据被回滚，则读到的数据是无效的。不可重复读：在同一个事务中，多次读取同一数据返回的结果有所不同。幻读：一个事务读取了几行记录后，另一个事务插入一些记录，幻读就发生了。在后来的查询中，第一个事务就会发现有些原来没有的记录。 事务隔离级别（四种）|隔离级别|含义||–|–||DEFAULT|使用后端数据库默认的隔离级别（Spring中的选择项）||READ_UNCOMMITED|允许你读取还未提交的改变了的数据。可能导致脏、幻、不可重复读||READ_COMMITED|允许在并发事务已经提交后读取。可防止脏读，但幻读和不可重复读仍可发生||REPEATABLE_READ|对相同字段的多次读取时一致的，除非数据被事物本身改变。可防止脏、不可重复读，但幻读扔可能发生||SERIALIZABLE|完全服从ACID的隔离级别，确保不发生脏、幻、不可重复读。这在所有的隔离级别中是最慢的，它是典型的通过完全锁定在事务中涉及的数据表来完成的。| 事物的传播行为：解决业务层方法之间的相互调用的问题。 事务传播行为（七种） 事务传播行为类型 说明 PROPAGATION_REQUIRED 支持当前事务，如果不存在就新建一个 PROPAGATION_SUPPORTS 支持当前事务，如果不存在在，就不使用事务 PROPAGATION_MANDATORY 支持当前事务，如果不存在，抛出异常 PROPAGATION_REQUIRES_NEW 如果有事务存在，挂起当前事务，创建一个新的事务 PROPAGATION_NOT_SUPPORTED 以非事务方式运行，如果有事务存在，挂起当前事务 PROPAGATION_NEVER 以非事务方式运行，如果有事务存在，抛出异常 PROPAGATION_NESTED 如果当前事务存在，则嵌套事务执行]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%2F</url>
    <content type="text"><![CDATA[自动检测Bean&lt;context:annotation-config&gt;&lt;context:component-scan base-package=&quot;&quot;&gt;可以指定扫描的包及其子包。base-package属性标识了context:component-scan元素所扫描的包。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring冷门知识点]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%E5%86%B7%E9%97%A8%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[PathVariable类型的参数不需要在Model中明确设置就可以在jsp中获取。1234@RequestMapping(value = &quot;/&#123;name&#125;&quot;)public String pathPage(@PathVariable String name) &#123; return &quot;path&quot;;&#125; 在jsp中可以直接通过${name}获取值。 post请求 @RequestParam 放在body我们都知道get请求的参数都是放在url后面，如：http://localhost:8080/get?id=1，方法声明如下：`public String post1(@RequestParam String id)，@RequestParam表明接受的是url后面的参数；而post请求的参数一般放在body里面，当然post请求的url后面也可以带参数，方法声明如下：public String post(@RequestParam String id, @RequestBody User user)，id就是url后面的参数，user就是body里面的参数。 但是@RequestParam类型的参数也是可以放在post请求的body中的。但是此时Content-Type需要是application/x-www-form-urlencoded`。123456// Content-Type = application/x-www-form-urlencoded@RequestMapping(value = "/post1", method = RequestMethod.POST)@ResponseBodypublic String post1(@RequestParam String id) &#123; return "id=" + id;&#125; body内容为id=1。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring声明式事务的两种实现方式]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%E5%A3%B0%E6%98%8E%E5%BC%8F%E4%BA%8B%E5%8A%A1%E7%9A%84%E4%B8%A4%E7%A7%8D%E5%AE%9E%E7%8E%B0%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Spring中事务分为编程式事务和声明式事务。编程式事务由于需要在代码中硬编码，在实际项目开发中比较少用到。实际开发中用的比较多的就是声明式事务。 声明式事务又分为基于配置的和基于@Transactional注解的。 1. 基于配置的声明式事务 配置事务管理器 123&lt;bean name="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource"&gt;&lt;/property&gt; &lt;/bean&gt; 配置需要加入事务的规则 1234567891011121314151617181920&lt;tx:advice id="iccardTxAdvice" transaction-manager="transactionManager"&gt; &lt;tx:attributes&gt; &lt;tx:method name="delete*" propagation="REQUIRED" read-only="false" rollback-for="java.lang.Exception" no-rollback-for="java.lang.RuntimeException"/&gt; &lt;tx:method name="insert*" propagation="REQUIRED" read-only="false" rollback-for="java.lang.RuntimeException" /&gt; &lt;tx:method name="add*" propagation="REQUIRED" read-only="false" rollback-for="java.lang.RuntimeException" /&gt; &lt;tx:method name="create*" propagation="REQUIRED" read-only="false" rollback-for="java.lang.RuntimeException" /&gt; &lt;tx:method name="update*" propagation="REQUIRED" read-only="false" rollback-for="java.lang.Exception" /&gt; &lt;tx:method name="find*" propagation="SUPPORTS" /&gt; &lt;tx:method name="get*" propagation="SUPPORTS" /&gt; &lt;tx:method name="select*" propagation="SUPPORTS" /&gt; &lt;tx:method name="query*" propagation="SUPPORTS" /&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 把事务控制在service层 --&gt;&lt;aop:config&gt; &lt;aop:pointcut id="txPointcut" expression="execution(public * com.zkzong.service.*.*(..))" /&gt; &lt;aop:advisor pointcut-ref="txPointcut" advice-ref="iccardTxAdvice" /&gt;&lt;/aop:config&gt; 2. 基于@Transactional注解的声明式事务 配置事务管理器 123456&lt;!-- 定义事务管理器 --&gt; &lt;bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager"&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;/bean&gt; &lt;!--使用注释事务 --&gt; &lt;tx:annotation-driven transaction-manager="transactionManager" /&gt; 在需要加入事务的方法或者类上添加@Transactional]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring注解]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[@ModelAttributespring mvc @ModelAttribute 接收前台参数问题直接接收url?后面的参数，如url?id=123&amp;name=456，可直接转换为Pojo @RequestBody接收post请求的body内容–json串，可直接转换为Pojo依赖jackson-databind包 @AutowiredbyType自动装配 @Qualifier当使用@Autowired自动装配Bean时，可能有多个Bean满足装配条件。此时可以使用@Qualifier指定Bean。 @Inject（Java自带）和@Autowired一样，@Inject可以用来自动装配属性、方法和构造器；与@Autowired不同的是，@Inject没有required属性。@Named对应@Qualifier。 @Component通用的构造型注解，标识该类为Spring组件。 @Controller标识将该类定义为Spring MVC Controller。 @Repository标识将该类定义为服务。 @Service标识将该类定义为服务。 @Configuration作为一个标识告知Spring：这个类将包含一个或多个Spring Bean的定义。 @Bean告知Spring这个方法将返回一个对象，该对象应该被注册为Spring应用上下文中的一个Bean。方法名将作为该Bean的ID。 @Value加上static则mvc值为空 Spring MVC注解： @PathVariable：url中“/”之后的参数。如：www.zkzong.com/profile/**username** 12345678@Controllerpublic class ShowProfileController &#123; @RequestMapping("/profile/&#123;username&#125;") public String showProfile(Model model, @PathVariable("username") String username) &#123; model.addAttribute("username", username); return "showProfile"; // the view name &#125;&#125; @RequestParam：@RequestParam注解不是严格需要的。在查询参数与方法参数的名称不匹配的时候，@RequestParam是有用的。基于约定，如果处理方法的所有参数没有使用注解的话，将绑定到同名的查询参数上。 url中问号之后，以“key=value”的形式出现。如：www.zkzong.com/id?**flag=true** form表单提交，参数名称和表单中控件相同 Hibernate注解： @DynamicInsert、@DynamicUpdate：用在数据库表所对应的实体类中。123456@Entity@Table(name = "SYS_USER")@DynamicInsert(false)@DynamicUpdate(false)public class User implements Serializable &#123;&#125;]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring实战]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2FSpring%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[第1章 Spring之旅为了降低Java开发的复杂性，Spring采取了以下4种关键策略： 基于POJO的轻量级和最小侵入性编程 通过依赖注入和面向接口实现松耦合 基于切面和管理惯例进行声明式编程 通过切面和模板减少样板式代码 依赖注入方式： 构造器注入 setter注入 接口注入（spring不支持） Spring装配Bean的方式： XML配置 Spring容器类型： Bean工厂（bean factories，由org.springframework.beans.factory.BeanFactory接口定义） 应用上下文（application由org.springframework.context.ApplicationContext接口定义） ClassPathXmlApplicationContext——从类路径下的XML配置文件中加载上下文定义，把应用上下文定义文件当作类资源 FileSystemXmlApplicationContext——读取文件系统下的XML配置文件并加载上下文定义 XmlWebApplicationContext——读取Web应用下的XML配置文件并装载上下文定义 第2章 装配Bean实际上，Spring是使用反射来创建Bean的。 所有的Spring Bean默认都是单例。为了让Spring在每次请求时都为Bean产生一个新的实例，只需要配置Bean的scope属性为prototype即可。&lt;bean id=&quot;ticket&quot; class=&quot;com.zkzong.springinaction.springidol.Ticket&quot; scope=&quot;prototype&quot;&gt; 内部Bean 使用Spring的命名空间p装配属性123456789&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:p="http://www.springframework.org/schema/p" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id="kenny" class="com.zkzong.springinaction.springidol.Instrumentalist" p:song="Jingle Bells" p:instrument-ref="piano" /&gt; SpEL：Spring表达式语言 第3章 最小化Spring XML配置 自动装配 自动检测 自动装配4种类型的自动装配： byName byType 为了避免因为使用byType自动装配而带来的奇异歧义，Spring提供了另外两种选择：可以为自动装配标识一个首选Bean，或者可以取消某个Bean自动装配的候选资格。 首选Bean：primary=true。primary属性默认为true，所以需要将所有非首选Bean的primary属性设置为false。 如果希望排除排除某些Bean，可以autowire设置这些Bean的autowire-candidate属性为false。 constructor autodetect 注解装配Spring容器默认禁用注解装配。启用方式：&lt;context:annotation-config&gt; Spring 3支持几种不同的用于自动装配的注解： Spring自带的@Autowired注解 JSR-330的@Inject注解 JSR-250的@Resource注解 自动检测Bean&lt;context:annotation-config&gt; 为自动检测标注Bean默认情况下，&lt;context:component-scan&gt;查找使用构造型（stereotype）注解所标注的类，这些特殊的注解如下： @Component——通用的构造型注解，标识该类为Spring组件 @Controller——标识将该类定义为Spring MVC Controller @Repository——标识将该类定义为数据仓库 @Service——标识将该类定义为服务 使用@Component标注的任意自定义注解 第4章 面向切面的SpringSpring切面可以应用5种类型的通知： Before —— 在方法被调用之前调用通知 After —— 在方法完成之后调用通知，无论方法执行是否成功 After-returning —— 在方法成功执行之后调用通知 After-throwing —— 在方法抛出异常后调用通知 Around —— 通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为 AOP框架： AspectJ JBoss AOP Spring AOP Spring提供了4种各具特色的AOP支持： 基于代理的经典AOP @AspectJ注解驱动的切面 纯POJO切面 注入式AspectJ切面（适合Spring各版本） 第5章 征服数据库Spring提供了在Spring上下文中配置数据源Bean的多种方式，包括： 通过JDBC驱动程序定义的数据源 通过JNDI查找的数据源（推荐） 连接池的数据源 Spring为JDBC提供了3个模板类供使用： JdbcTemplate NamedParameterJdbcTemplate SimpleJdbcTemplate 在Spring中集成Hibernate使用Hibernate的主要接口是org.hibernate.Session。Session接口提供了基本的数据访问功能，如保存、更新、删除以及从数据库加载对象的功能。通过Hibernate的Session接口，应用程序的DAO能够满足所有的持久化需求。获取Hibernate Session对象的标准方式是借助于Hibernate的SessionFactory接口的实现类。除了一些其他的任务，SessionFactory主要负责Hibernate Session的打开、关闭以及管理。两种配置SessionFactory方式： XML：LocalSessionFactoryBean 注解：AnnotationSessionFactoryBean JPA LocalEntityManagerFactoryBean LocalContainerEntityManagerFactoryBean 为了在Spring中实现EntityManager注入，我们需要在Spring应用上下文中配置一个PersistenceAnnotationBeanPostProcessor：&lt;bean class=&quot;org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor&quot; /&gt; 第6章 事务管理ACID： 原子性（Atomic） 一致性（Consistent） 隔离性（Isolated） 持久性（Durable） Spring提供了对编码式和声明式事务管理的支持。 传播行为传播规则回答了这样一个问题，即新的事务应该被启动还是被挂起，或者方法是否要在事务环境中运行。 隔离级别隔离级别定义了一个事务可能受其他并发事务影响的程度。另一种考虑隔离级别的方式就是将其想象成事务对于事务性数据的自私程度。 脏读（Dirty reads）：脏读发生在读取了另一个事务改写但尚未提交的数据时。如果改写在稍后被回滚了，那么第一个事务获取的数据就是无效的。 不可重复读（Nonrepeatable read）：不可重复读发生在一个事务执行相同的查询两次或两次以上，但是每次都得到不同的数据时。这通常是因为另一个并发事务在两次查询期间更新了数据。 幻读（Phantom read）：幻读与不可重复读类似。它发生在一个事务（T1）读取了几行数据，接着另一个并发事务（T2）插入了一些数据时。在随后的查询中，第一个事务（T1）就会发现多了一些原本不存在的记录。 声明式事务： 在XML中定义事务 &lt;tx:advice&gt; &lt;aop:config&gt; 定义注解驱动的事务 &lt;tx:annotation-driven&gt; @Transactional Spring MVC]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[为什么要用SLF4J]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8SLF4J%2F</url>
    <content type="text"><![CDATA[阿里巴巴 Java 开发手册前几天阿里巴巴在云栖社区首次公开阿里官方Java代码规范标准，就是一个PDF手册，有命名规范；有集合处理、并发处理、OOM/NPE 异常、魔法值（魔法值：即未经定义的常量）等等好多规范；还有一个关于 Map 遍历的推荐，这个大家应该都知道，推荐使用 entrySet 遍历 Map 类集合 KV，而不是 keySet 方式进行遍历。 因为 keySet 是遍历了 2 次，而 entrySet 只是遍历了一次就把 key 和 value 都放到了 entry 中，效率更高。还有接口类中的方法和属性不要加任何修饰符号（public 也不要加）这些推荐做法，这些都没什么，日常开发中应该做到的规范，但下面这个【强制】，我发现我接触的项目都没做到。 【强制】应用中不可直接使用日志系统（Log4j、Logback）中的 API在手册中的日志规约中，看到有一条这样的规定，说实话我有点懵逼， Log4j 不是 Java 中应用最广的日志系统么？为啥不让用？ 【强制】应用中不可直接使用日志系统（Log4j、Logback）中的API，而应依赖使用日志框架SLF4J中的API，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。1234&gt; import org.slf4j.Logger;&gt; import org.slf4j.LoggerFactory;&gt; private static final Logger logger = LoggerFactory.getLogger(Abc.class);&gt; 在这段规约中看到了推荐使用 SLF4J 这个日志框架，而且还是毫不由分说的【强制】，那它到底好在什么地方？ SLF4J，即简单日志门面（Simple Logging Facade for Java），不是具体的日志解决方案，它只服务于各种各样的日志系统。按照官方的说法，SLF4J是一个用于日志系统的简单Facade，允许最终用户在部署其应用时使用其所希望的日志系统。 大概意思就是说 SLF4J 是一个日志抽象层，允许你使用任何一个日志系统，并且可以随时切换还不需要动到已经写好的程序，这对于第三方组件的引入的不同日志系统来说几乎零学习成本了，况且它的优点不仅仅这一个而已，还有简洁的占位符的使用和日志级别的判断，众所周知的日志读写一定会影响系统的性能，但这些特性都是对系统性能友好的。官网地址：https://www.slf4j.org/ 测试一下说了那么多，下面就建立一个Maven项目，并用 SLF4J 来结合 JDK14、Simple、Logback、Log4j 做日志系统，在上述几个日志系统间随意切换，而且不修改一行代码，甚至不用修改一个字符。 1. 首先建立一个简单的 Java 项目（Maven Project），目录结构如下： 2. 在 pom.xml 中增加 SLF4J API 依赖包 使用的目前最新稳定版 1.7.22 的 SLF4J：12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.22&lt;/version&gt;&lt;/dependency&gt; 接着并在测试项目中的 App.java 中加入日志输出代码，代码如下： 123456789101112131415161718192021222324package xyz.mafly.SLF4JTest;import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * Hello world! **/public class App &#123; final Logger logger = LoggerFactory.getLogger(App.class); private void test() &#123; logger.info(&quot;这是一条日志信息 - &#123;&#125;&quot;, &quot;mafly&quot;); &#125; public static void main(String[] args) &#123; App app = new App(); app.test(); System.out.println(&quot;Hello World!&quot;); &#125;&#125; 到这里，代码就写完了。以后无论在 Log4j 还是 Logback 日志系统切换，都不需要修改这里的代码！一个字符都不需要！！ 3. JDK14 日志系统 pom.xml 中增加 slf4j-jdk14 依赖包：12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-jdk14&lt;/artifactId&gt; &lt;version&gt;1.7.22&lt;/version&gt;&lt;/dependency&gt; 运行程序，即可看到如下图输出： 4. Simple 日志系统 在 pom.xml 中注释掉 JDK14 包节点，增加 slf4j-simple 依赖包：12345&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt; &lt;version&gt;1.7.22&lt;/version&gt;&lt;/dependency&gt; 运行程序，即可看到如下图不同输出： 5. Log4j 日志系统（最常用） 依然是在 pom.xml 中注释掉 Simple 包节点，增加 slf4j-log4j12 依赖包：12345678910&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.22&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt;&lt;/dependency&gt; Log4j 除了导入 jar 包后，还需要增加一下日志格式的配置文件，我新增了一个log4j.properties的日志配置文件，具体 Log4j 详细配置我之前在 《log4j 项目中的详细配置》 这篇博客中写过。运行程序，即可看到如下图输出（输出格式可自己配置）： 6. Logback 日志系统 在 pom.xml 中注释掉 Log4j 包节点，增加 slf4j-logback 依赖包：12345678910&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.1.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.9&lt;/version&gt;&lt;/dependency&gt; 运行程序，也可看到如下图日志输出： 总结一下看完阿里巴巴的这个开发手册，的确学到了一些新知识和规范，SLF4J 只是其中一个知识点而已。说回 SLF4J 这个日志框架，在下一个开源项目或内部类库中都强烈推荐使用 SLF4J ，它的好处不言而喻，这也是阿里巴巴强制使用的原因所在。希望这篇文章对你的项目中日志系统有所帮助，任何一个任何编程语言的开发者，都应该重视日志的重要性和编码规范，对你、团队和未来阅读你代码的人都好。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用Spring Data JPA简化JPA开发]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2F%E4%BD%BF%E7%94%A8Spring%20Data%20JPA%E7%AE%80%E5%8C%96JPA%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[通过解析方法名创建查询通过前面的例子，读者基本上对解析方法名创建查询的方式有了一个大致的了解，这也是 Spring Data JPA 吸引开发者的一个很重要的因素。该功能其实并非 Spring Data JPA 首创，而是源自一个开源的 JPA 框架 Hades，该框架的作者 Oliver Gierke 本身又是 Spring Data JPA 项目的 Leader，所以把 Hades 的优势引入到 Spring Data JPA 也就是顺理成章的了。框架在进行方法名解析时，会先把方法名多余的前缀截取掉，比如 find、findBy、read、readBy、get、getBy，然后对剩下部分进行解析。并且如果方法的最后一个参数是 Sort 或者 Pageable 类型，也会提取相关的信息，以便按规则进行排序或者分页查询。在创建查询时，我们通过在方法名中使用属性名称来表达，比如 findByUserAddressZip ()。框架在解析该方法时，首先剔除 findBy，然后对剩下的属性进行解析，详细规则如下（此处假设该方法针对的域对象为 AccountInfo 类型）： 先判断 userAddressZip （根据 POJO 规范，首字母变为小写，下同）是否为 AccountInfo 的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，继续第二步； 从右往左截取第一个大写字母开头的字符串（此处为 Zip），然后检查剩下的字符串是否为 AccountInfo 的一个属性，如果是，则表示根据该属性进行查询；如果没有该属性，则重复第二步，继续从右往左截取；最后假设 user 为 AccountInfo 的一个属性； 接着处理剩下部分（ AddressZip ），先判断 user 所对应的类型是否有 addressZip 属性，如果有，则表示该方法最终是根据 “AccountInfo.user.addressZip” 的取值进行查询；否则继续按照步骤 2 的规则从右往左截取，最终表示根据 “AccountInfo.user.address.zip” 的值进行查询。 可能会存在一种特殊情况，比如 AccountInfo 包含一个 user 的属性，也有一个 userAddress 属性，此时会存在混淆。读者可以明确在属性之间加上 “_” 以显式表达意图，比如 “findByUser_AddressZip()” 或者 “findByUserAddress_Zip()”。在查询时，通常需要同时根据多个属性进行查询，且查询的条件也格式各样（大于某个值、在某个范围等等），Spring Data JPA 为此提供了一些表达条件查询的关键字，大致如下： And — 等价于 SQL 中的 and 关键字，比如 findByUsernameAndPassword(String user, Striang pwd)； Or — 等价于 SQL 中的 or 关键字，比如 findByUsernameOrAddress(String user, String addr)； Between — 等价于 SQL 中的 between 关键字，比如 findBySalaryBetween(int max, int min)； LessThan — 等价于 SQL 中的 “&lt; findBySalaryLessThanint maxli&gt; GreaterThan — 等价于 SQL 中的”&gt;”，比如 findBySalaryGreaterThan(int min)； IsNull — 等价于 SQL 中的 “is null”，比如 findByUsernameIsNull()； IsNotNull — 等价于 SQL 中的 “is not null”，比如 findByUsernameIsNotNull()； NotNull — 与 IsNotNull 等价； Like — 等价于 SQL 中的 “like”，比如 findByUsernameLike(String user)； NotLike — 等价于 SQL 中的 “not like”，比如 findByUsernameNotLike(String user)； OrderBy — 等价于 SQL 中的 “order by”，比如 findByUsernameOrderBySalaryAsc(String user)； Not — 等价于 SQL 中的 “！ =”，比如 findByUsernameNot(String user)； In — 等价于 SQL 中的 “in”，比如 findByUsernameIn(Collection userList) ，方法的参数可以是 Collection 类型，也可以是数组或者不定长参数； NotIn — 等价于 SQL 中的 “not in”，比如 findByUsernameNotIn(Collection userList) ，方法的参数可以是 Collection 类型，也可以是数组或者不定长参数； https://www.ibm.com/developerworks/cn/opensource/os-cn-spring-jpa/index.html]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[超详细Spring @RequestMapping注解使用技巧]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2F%E8%B6%85%E8%AF%A6%E7%BB%86Spring%20%40RequestMapping%E6%B3%A8%E8%A7%A3%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[@RequestMapping 是 Spring Web 应用程序中最常被用到的注解之一。这个注解会将 HTTP 请求映射到 MVC 和 REST 控制器的处理方法上。 在这篇文章中，你将会看到 @RequestMapping 注解在被用来进行 Spring MVC 控制器方法的映射可以如何发挥其多才多艺的功能的。 RequestMapping 基础用法在 Spring MVC 应用程序中，RequestDispatcher (在 Front Controller 之下) 这个 servlet 负责将进入的 HTTP 请求路由到控制器的处理方法。 在对 Spring MVC 进行配置的时候, 你需要指定请求与处理方法之间的映射关系。 要配置 Web 请求的映射，就需要你用上 @RequestMapping 注解。 @RequestMapping 注解可以在控制器类的级别和/或其中的方法的级别上使用。 在类的级别上的注解会将一个特定请求或者请求模式映射到一个控制器之上。之后你还可以另外添加方法级别的注解来进一步指定到处理方法的映射关系。 下面是一个同时在类和方法上应用了 @RequestMapping 注解的示例： 123456789101112131415@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping("/") String get() &#123; //mapped to hostname:port/home/ return "Hello from get"; &#125; @RequestMapping("/index") String index() &#123; //mapped to hostname:port/home/index/ return "Hello from index"; &#125;&#125; 如上述代码所示，到 /home 的请求会由 get() 方法来处理，而到 /home/index 的请求会由 index() 来处理。 @RequestMapping 来处理多个 URI你可以将多个请求映射到一个方法上去，只需要添加一个带有请求路径值列表的 @RequestMapping 注解就行了。 1234567891011121314@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = &#123; "", "/page", "page*", "view/*,**/msg" &#125;) String indexMultipleMapping() &#123; return "Hello from index multiple mapping."; &#125;&#125; 如你在这段代码中所看到的，@RequestMapping 支持通配符以及ANT风格的路径。前面这段代码中，如下的这些 URL 都会由 indexMultipleMapping() 来处理： localhost:8080/home localhost:8080/home/ localhost:8080/home/page localhost:8080/home/pageabc localhost:8080/home/view/ localhost:8080/home/view/view 带有 @RequestParam 的 @RequestMapping@RequestParam 注解配合 @RequestMapping 一起使用，可以将请求的参数同处理方法的参数绑定在一起。 @RequestParam 注解使用的时候可以有一个值，也可以没有值。这个值指定了需要被映射到处理方法参数的请求参数, 代码如下所示： 12345678910111213141516@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/id") String getIdByValue(@RequestParam("id") String personId) &#123; System.out.println("ID is " + personId); return "Get ID from query string of URL with value element"; &#125; @RequestMapping(value = "/personId") String getId(@RequestParam String personId) &#123; System.out.println("ID is " + personId); return "Get ID from query string of URL without value element"; &#125;&#125; 在代码的第6行，id 这个请求参数被映射到了 thegetIdByValue() 这个处理方法的参数 personId 上。 如果请求参数和处理方法参数的名称一样的话，@RequestParam 注解的 value 这个参数就可省掉了, 如代码的第11行所示。 @RequestParam 注解的 required 这个参数定义了参数值是否是必须要传的。 12345678@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/name") String getName(@RequestParam(value = "person", required = false) String personName) &#123; return "Required element of request param"; &#125;&#125; 在这段代码中，因为 required 被指定为 false，所以 getName() 处理方法对于如下两个 URL 都会进行处理： /home/name?person=xyz /home/name @RequestParam 的 defaultValue 取值就是用来给取值为空的请求参数提供一个默认值的。 12345678@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/name") String getName(@RequestParam(value = "person", defaultValue = "John") String personName) &#123; return "Required element of request param"; &#125;&#125; 在这段代码中，如果 person 这个请求参数为空，那么 getName() 处理方法就会接收 John 这个默认值作为其参数。 用 @RequestMapping 处理 HTTP 的各种方法Spring MVC 的 @RequestMapping 注解能够处理 HTTP 请求的方法, 比如 GET, PUT, POST, DELETE 以及 PATCH。 所有的请求默认都会是 HTTP GET 类型的。 为了能将一个请求映射到一个特定的 HTTP 方法，你需要在 @RequestMapping 中使用 method 来声明 HTTP 请求所使用的方法类型，如下所示： 123456789101112131415161718192021222324@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(method = RequestMethod.GET) String get() &#123; return "Hello from get"; &#125; @RequestMapping(method = RequestMethod.DELETE) String delete() &#123; return "Hello from delete"; &#125; @RequestMapping(method = RequestMethod.POST) String post() &#123; return "Hello from post"; &#125; @RequestMapping(method = RequestMethod.PUT) String put() &#123; return "Hello from put"; &#125; @RequestMapping(method = RequestMethod.PATCH) String patch() &#123; return "Hello from patch"; &#125;&#125; 在上述这段代码中， @RequestMapping 注解中的 method 元素声明了 HTTP 请求的 HTTP 方法的类型。 所有的处理处理方法会处理从这同一个 URL( /home)进来的请求，但要看指定的 HTTP 方法是什么来决定用哪个方法来处理。 例如，一个 POST 类型的请求 /home 会交给 post() 方法来处理，而一个 DELETE 类型的请求 /home 则会由 delete() 方法来处理。 你会看到 Spring MVC 将使用这样相同的逻辑来映射其它的方法。 用 @RequestMapping 来处理生产和消费对象可以使用 @RequestMapping 注解的 produces 和 consumes 这两个元素来缩小请求映射类型的范围。 为了能用请求的媒体类型来产生对象, 你要用到 @RequestMapping 的 produces 元素再结合着 @ResponseBody 注解。 你也可以利用 @RequestMapping 的 comsumes 元素再结合着 @RequestBody 注解用请求的媒体类型来消费对象。 下面这段代码就用到的 @RequestMapping 的生产和消费对象元素： 1234567891011121314151617181920@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/prod", produces = &#123; "application/JSON" &#125;) @ResponseBody String getProduces() &#123; return "Produces attribute"; &#125; @RequestMapping(value = "/cons", consumes = &#123; "application/JSON", "application/XML" &#125;) @RequestBody String getConsumes() &#123; return "Consumes attribute"; &#125;&#125; 在这段代码中， getProduces() 处理方法会产生一个 JSON 响应， getConsumes() 处理方法可以同时处理请求中的 JSON 和 XML 内容。 使用 @RequestMapping 来处理消息头@RequestMapping 注解提供了一个 header 元素来根据请求中的消息头内容缩小请求映射的范围。 在可以指定 header 元素的值，用 myHeader = myValue 这样的格式： 12345678910@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/head", headers = &#123; "content-type=text/plain" &#125;) String post() &#123; return "Mapping applied along with headers"; &#125;&#125; 在上面这段代码中， @RequestMapping 注解的 headers 属性将映射范围缩小到了 post() 方法。有了这个，post() 方法就只会处理到 /home/head 并且 header的content-type 被指定为 text/plain 这个值的请求。 你也可以像下面这样指定多个消息头： 12345678910@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/head", headers = &#123; "content-type=text/plain", "content-type=text/html" &#125;) String post() &#123; return "Mapping applied along with headers"; &#125;&#125; 这样， post() 方法就能同时接受 text/plain 还有 text/html 的请求了。 使用 @RequestMapping 来处理请求参数@RequestMapping 直接的 params 元素可以进一步帮助我们缩小请求映射的定位范围。使用 params 元素，你可以让多个处理方法处理到同一个URL 的请求, 而这些请求的参数是不一样的。 你可以用 myParams = myValue 这种格式来定义参数，也可以使用通配符来指定特定的参数值在请求中是不受支持的。 1234567891011121314151617@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/fetch", params = &#123; "personId=10" &#125;) String getParams(@RequestParam("personId") String id) &#123; return "Fetched parameter using params attribute = " + id; &#125; @RequestMapping(value = "/fetch", params = &#123; "personId=20" &#125;) String getParamsDifferent(@RequestParam("personId") String id) &#123; return "Fetched parameter using params attribute = " + id; &#125;&#125; 在这段代码中，getParams() 和 getParamsDifferent() 两个方法都能处理相同的一个 URL (/home/fetch) ，但是会根据 params 元素的配置不同而决定具体来执行哪一个方法。 例如，当 URL 是 /home/fetch?id=10 的时候，getParams() 会执行，因为 id 的值是10。对于 localhost:8080/home/fetch?personId=20 这个URL，getParamsDifferent() 处理方法会得到执行，因为 id 值是 20。 使用 @RequestMapping 处理动态 URI@RequestMapping 注解可以同 @PathVaraible 注解一起使用，用来处理动态的 URI，URI 的值可以作为控制器中处理方法的参数。你也可以使用正则表达式来只处理可以匹配到正则表达式的动态 URI。 123456789101112131415@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping(value = "/fetch/&#123;id&#125;", method = RequestMethod.GET) String getDynamicUriValue(@PathVariable String id) &#123; System.out.println("ID is " + id); return "Dynamic URI parameter fetched"; &#125; @RequestMapping(value = "/fetch/&#123;id:[a-z]+&#125;/&#123;name&#125;", method = RequestMethod.GET) String getDynamicUriValueRegex(@PathVariable("name") String name) &#123; System.out.println("Name is " + name); return "Dynamic URI parameter fetched using regex"; &#125;&#125; 在这段代码中，方法 getDynamicUriValue() 会在发起到 localhost:8080/home/fetch/10 的请求时执行。这里 getDynamicUriValue() 方法 id 参数也会动态地被填充为 10 这个值。 方法 getDynamicUriValueRegex() 会在发起到 localhost:8080/home/fetch/category/shirt 的请求时执行。不过，如果发起的请求是 /home/fetch/10/shirt 的话，会抛出异常，因为这个URI并不能匹配正则表达式。 @PathVariable 同 @RequestParam的运行方式不同。你使用 @PathVariable 是为了从 URI 里取到查询参数值。换言之，你使用 @RequestParam 是为了从 URI 模板中获取参数值。 @RequestMapping 默认的处理方法在控制器类中，你可以有一个默认的处理方法，它可以在有一个向默认 URI 发起的请求时被执行。 下面是默认处理方法的示例： 12345678@RestController@RequestMapping("/home")public class IndexController &#123; @RequestMapping() String default () &#123; return "This is a default method for the class"; &#125;&#125; 在这段代码中，向 /home 发起的一个请求将会由 default() 来处理，因为注解并没有指定任何值。 @RequestMapping 快捷方式Spring 4.3 引入了方法级注解的变体，也被叫做 @RequestMapping 的组合注解。组合注解可以更好的表达被注解方法的语义。它们所扮演的角色就是针对 @RequestMapping 的封装，而且成了定义端点的标准方法。 例如，@GetMapping 是一个组合注解，它所扮演的是 @RequestMapping(method =RequestMethod.GET) 的一个快捷方式。方法级别的注解变体有如下几个： @GetMapping @PostMapping @PutMapping @DeleteMapping @PatchMapping 如下代码展示了如何使用组合注解： 123456789101112131415161718192021222324252627282930313233@RestController@RequestMapping("/home")public class IndexController &#123; @GetMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; getPerson() &#123; return new ResponseEntity&lt;String&gt; ("Response from GET", HttpStatus.OK); &#125; @GetMapping("/person/&#123;id&#125;") public @ResponseBody ResponseEntity&lt;String&gt; getPersonById(@PathVariable String id) &#123; return new ResponseEntity&lt;String&gt; ("Response from GET with id " + id, HttpStatus.OK); &#125; @PostMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; postPerson() &#123; return new ResponseEntity&lt;String&gt; ("Response from POST method", HttpStatus.OK); &#125; @PutMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; putPerson() &#123; return new ResponseEntity&lt;String&gt; ("Response from PUT method", HttpStatus.OK); &#125; @DeleteMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; deletePerson() &#123; return new ResponseEntity&lt;String&gt; ("Response from DELETE method", HttpStatus.OK); &#125; @PatchMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; patchPerson() &#123; return new ResponseEntity&lt;String&gt; ("Response from PATCH method", HttpStatus.OK); &#125;&#125; 在这段代码中，每一个处理方法都使用 @RequestMapping 的组合变体进行了注解。尽管每个变体都可以使用带有方法属性的 @RequestMapping 注解来互换实现, 但组合变体仍然是一种最佳的实践 — 这主要是因为组合注解减少了在应用程序上要配置的元数据，并且代码也更易读。 @RequestMapping 总结如你在本文中所看到的，@RequestMapping 注解是非常灵活的。你可以使用该注解配置 Spring MVC 来处理大量的场景用例。它可以被用来在 Spring MVC 中配置传统的网页请求，也可以是 REST 风格的 Web 服务。 原文链接：Using the Spring @RequestMapping Annotation超详细 Spring @RequestMapping 注解使用技巧]]></content>
      <categories>
        <category>Spring</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CAP理论]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%20Cloud%2FCAP%E7%90%86%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[8761]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CAP理论及Zookeeper和Eureka对比]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%20Cloud%2FCAP%E7%90%86%E8%AE%BA%E5%8F%8AZookeeper%E5%92%8CEureka%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[分布式系统领域有个重要的CAP理论，该理论由加州大学伯克利分校的Eric Brewer教授提出，由麻省理工学院的Seth Gilbert和Nancy Lynch进行理论证明。该理论提到了分布式系统的CAP三个特性： ❑ Consistency：数据一致性，即数据在存在多副本的情况下，可能由于网络、机器故障、软件系统等问题导致数据写入部分副本成功，部分副本失败，进而造成副本之间数据不一致，存在冲突。满足一致性则要求对数据的更新操作成功之后，多副本的数据保持一致。❑ Availability：在任何时候客户端对集群进行读写操作时，请求能够正常响应，即在一定的延时内完成。❑ Partition Tolerance：分区容忍性，即发生通信故障的时候，整个集群被分割为多个无法相互通信的分区时，集群仍然可用。 对于分布式系统来说，一般网络条件相对不可控，出现网络分区是不可避免的，因此系统必须具备分区容忍性。在这个前提下分布式系统的设计则在AP及CP之间进行选择。不过不能理解为CAP三者之间必须三选二，它们三者之间不是对等和可以相互替换的。在分布式系统领域，P是一个客观存在的事实，不可绕过，所以P与AC之间不是对等关系。 对于ZooKeeper，它是”C”P的，之所以C加引号是因为ZooKeeper默认并不是严格的强一致，比如客户端A提交一个写操作，ZooKeeper在过半数节点操作成功之后就返回，此时假设客户端B的读操作请求到的是A写操作尚未同步到的节点，那么读取到的就不是客户端A写操作成功之后的数据。如果在使用的时候需要强一致，则需要在读取数据的时候先执行一下sync操作，即与leader节点先同步下数据，这样才能保证强一致。在极端的情况下发生网络分区的时候，如果leader节点不在non-quorum分区，那么对这个分区上节点的读写请求将会报错，无法满足Availability特性。 Eureka是在部署在AWS的背景下设计的，其设计者认为，在云端，特别是在大规模部署的情况下，失败是不可避免的，可能因为Eureka自身部署失败，注册的服务不可用，或者由于网络分区导致服务不可用，因此不能回避这个问题。要拥抱这个问题，就需要Eureka在网络分区的时候，还能够正常提供服务注册及发现功能，因此Eureka选择满足Availability这个特性。Peter Kelley在《Eureka! Why You Shouldn’t Use ZooKeeper for Service Discovery》一文中指出，在实际生产实践中，服务注册及发现中心保留可用及过期的数据总比丢失掉可用的数据好。这样的话，应用实例的注册信息在集群的所有节点间并不是强一致的，这就需要客户端能够支持负载均衡及失败重试。在Netflix的生态中，由ribbon提供这个功能。]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud Docker]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%20Cloud%2FSpring%20Cloud%20Docker%2F</url>
    <content type="text"><![CDATA[1. 微服务架构概述1.4.1 微服务架构的优点 易于开发和维护 单个微服务启动较快 局部修改容易部署 技术栈不受限 按需伸缩 1.4.2 微服务架构面临的挑战 运维要求较高 分布式固有的复杂性 接口调整成本高 重复劳动]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%20Cloud%2FSpring%20Cloud%2F</url>
    <content type="text"><![CDATA[Eureka：服务发现模块 Ribbon：Ribbon是一个基于HTTP和TCP客户端的负载均衡器 Feign：Feign是一个声明式的Web Service客户端，它使得编写Web Serivce客户端变得更加简单。我们只需要使用Feign来创建一个接口并用注解来配置它既可完成。它具备可插拔的注解支持，包括Feign注解和JAX-RS注解。Feign也支持可插拔的编码器和解码器。Spring Cloud为Feign增加了对Spring MVC注解的支持，还整合了Ribbon和Eureka来提供均衡负载的HTTP客户端实现。 Hystrix：Hystrix是Netflix开源的微服务框架套件之一，该框架目标在于通过控制那些访问远程系统、服务和第三方库的节点，从而对延迟和故障提供更强大的容错能力。Hystrix具备拥有回退机制和断路器功能的线程和信号隔离，请求缓存和请求打包，以及监控和配置等功能。不需要在Feigh工程中引入Hystix，Feign中已经依赖了Hystrix]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Cloud微服务实战]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%20Cloud%2FSpring%20Cloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Spring BootYAML目前还有—些不足，它无法通过@PropertySource注解来加载配置。 @Value注解加载属性值的时候可以支持两种表达式来进行配置，如下所示。 一种是PlaceHolder方式，格式为＄｛．．．｝，大括号内为PlaceHolder。 另一种是使用SpEL 表达式(Spring Expression Language)，格式为＃｛．．．｝，大括号内为SpEL表达式。 EurekaSpring Cloud Eureka，使用Netflix Eureka来实现服务注册与发现，它既包含了服务端组件，也包含了客户端组件。Eureka服务端，也称为服务注册中心。Eureka客户端，主要处理服务的注册与发现。 在默认设置下，服务注册中心也会将自己作为客户端来尝试注册它自己，所以我们需要禁用它的客户端注册行为，只需在application.properties中增加如下配置：1234567server.port=1111eureka.instance.hostname=localhosteureka.client.register-with-eureka=falseeureka.client.fetch-registry=falseeureka.client.serviceUrl.defaultZone=http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ eureka.client.register-with-eureka: 由于该应用为注册中心，所以设置为false, 代表不向注册中心注册自己。 eureka.client.fetch-registry: 由于注册中心的职责就是维护服务实例，它并不需要去检索服务， 所以也设置为false。 Ribbon是一个基于HTTP和TCP的客户端负载均衡器，它可以在通过客户端中配置的ribbonServerList服务端列表去轮询访问以达到均衡负载的作用。当Ribbon与Eureka联合使用时，Ribbon的服务实例清单RibbonServerList会被DiscoveryEnabledNIWSServerList重写，扩展成从Eureka注册中心中获取服务端列表。同时它也会用NIWSDiscoveryPing来取代IPing，它将职责委托给Eureka来确定服务端是否已经启动。 为了服务注册中心的安全考虑，很多时候我们都会为服务注册中心加入安全校验。这个时候，在配置serviceUrl时，需要在value值的URL中加入相应的安全校验信息，比如http://:@localhost:1111/eureka。其中，为安全校验信息的用户名，为该用户的密码。 RibbonSpring Cloud Ribbon是一个基于HTTP和TCP的客户端负载均衡工具，不需要独立部署。 通过Spring Cloud Ribbon的封装，在微服务架构中使用客户端负载均衡调用非常简单，只需要如下两步： 服务提供者只需要启动多个服务实例并注册到一个注册中心或是多个相关联的服务注册中心。 服务消费者直接通过调用被@LoadBalanced注解修饰过的RestTemplate来实现面向服务的接口调用。 GET请求在RestTemplate中，对GET请求可以通过如下两个方法进行调用实现。 getForEntity函数 getForObject函数 POST请求在RestTemplate中，对POST请求可以通过如下三个方法进行调用实现。 postForEntity函数 postForObject函数 postForLocation函数 PUT请求在RestTemplate中，对PUT请求可以通过put方法进行调用实现。 DELETE请求在RestTemplate中，对DELETE请求可以通过delete方法进行调用实现。 Spring Cloud Ribbon默认实现了区域亲和策略，可以通过Eureka实例的元数据配置来实现区域化的实例配置方案：eureka.instance.metadataMap.zone=shanghai 禁用Eureka对Ribbon服务实例的维护实现：ribbon.eureka.enabled=false Hystrix@SpringCloudApplication FeignSpring Cloud Feign基于Netflix Feign实现，整合了Spring Cloud Ribbon和Spring Cloud Hystrix，除了提供这两者的强大功能之外，它还提供了一种声明式的Web服务客户端定义方式。 在定义各参数绑定时，@RequestParam、@RequestHeader 等可以指定参数名称的注解， 它们的 value 千万不能少。 在 SpringMVC 程序中，这些注解会根据参数名来作为默认值，但是在Feign 中绑定参数必须通过 value 属性来指明具体的参数名，不然会抛出口legalStateException 异常， value 属性不能为空。 Ribbon配置全局配置ribbon.ConnectTimeout=500ribbon.ReadTimeout=SOOO 指定服务配置使用@FeignClient注解，在初始化过程中，Spring Cloud Feign会根据该注解的name属性或value属性指定的服务名，自动创建一个同名的Ribbon客户端。也就是说，在之前的示例中，使用@FeignClient(value = “HELLO-SERVICE”)来创 建 Feign 客户端的时候，同时也创建了一个名为HELLO-SERV工CE的Ribbon客户端。既然如此，我们就可以使用@FeignClient注解中的name或value属性值来设置对应的Ribbon参数，比如：12345HELLO-SERVICE.ribbon.ConnectTimeout=SOOHELLO-SERVICE.ribbon.ReadTimeout=2000HELLO-SERVICE.ribbon.OkToRetryOnAllOperations=trueHELLO-SERVICE.ribbon.MaxAutoRetriesNextServer=2HELLO-SERVICE.ribbon.MaxAutoRetries=l Hystrix配置全局配置hystrix.command.default.execution.isolation.thread.timeoutinMilliseconds=5OOOfeign.hystrix.enabled=falsehystrix.command.default.execution.timeout.enabled=false 禁用Hystrix如果不想全局地关闭Hystrix支持, 而只想针对某个 服务客户端关闭Hystrix 支待时, 需要通过使用@Scope (“prototype”)注解为指定的客户端配置Feign.Builder实例, 详细实现步骤如下所示。 构建一个关闭Hystrix的配置类 12345678@Configurationpublic class DisableHystrixConfiguration &#123; @Bean @Scope(&quot;prototype&quot;) public Feign.Builder feignBuilder() &#123; return Feign.builder(); &#125;&#125; 在HelloService的@FeignClient注解中,通过configuration参数引入上面实现的配置。 1234@FeignClient(name = &quot;HELLO-SERVICE&quot;, configuration = DisableHystrixConfiguration.class)public interface HelloService &#123; ...&#125; 指定命令配置服务降级配置 通过@HystrixCommand注解的fallback参数来指定具体的服务降级处理方法。 为Feign客户端的定义接口编写一个具体的接口实现类。 其他配置请求压缩123456feign.compression.request.enabled=truefeign.compression.response.enabled=truefeign.compression.request.enabled=truefeign.compression.request.mime-types=text/xml,application/xml,application/jsonfeign.compression.request.min-request-size=2048 日志配置logging.level.com.didispace.web.HelloSevice=DEBUG 只是添加了如上配置, 还无法实现对 DEBUG 日志的输出。 这时由于 Feign 客户端默认的 Logger.Level 对象定义为 NONE 级别, 该级别不会记录任何 Feign 调用过程中的信息, 所以我们需要调整它的级别, 针对全局的日志级别, 可以在应用主类中直接加入 Logger.Level 的 Bean 创建, 具体如下:12345678910111213@EnableFeignClients@EnableDiscoveryClient@SpringBootApplicationpublic class ConsumerApplication &#123; @Bean Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125; public static void main(String[J args) &#123; SpringApplication.run(ConsumerApplication.class, args); &#125;&#125; 也可以通过实现配置类，然后在具体的Feign客户端来指定配置类以实现是否要调整不同的日志级别，比如下面的实现:123456789101112@Configurationpublic class FullLogConfiguration &#123; @Bean Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125;@FeignClient(name = &quot;HELLO-SERVICE&quot;, configuration = FullLogConfiguration.class)public interface HelloService &#123; ...&#125; 对于 Feign 的 Logger 级别主要有下面 4 类, 可根据实际需要进行调整使用。 NONE: 不记录任何信息。 BASIC: 仅记录请求方法、URL以及响应状态码和执行时间。 HEADERS: 除了记录BASIC级别的信息之外, 还会记录请求和响应的头信息。 FULL: 记录所有请求与响应的明细, 包括头信息、 请求体、 元数据等。 Zuul 路由规则与服务实例的维护 对于类似签名校验、登录校验在微服务架构中的冗余问题 请求路由请求过滤 继承ZuulFilter实现类 在Application中创建Bean1234@Beanpublic AccessFilter accessFilter() &#123; return new AccessFilter();&#125; 它作为系统的统一入口,屏蔽了系统内部各个微服务的细节。 它可以与服务治理框架结合,实现自动化的服务实例维护以及负载均衡的路由转发。 它可以实现接口权限校验与微服务业务逻辑的解耦。 通过服务网关中的过炖器,在各生命周期中去校验请求的内容,将原本在对外服务层做的校验前移,保证了微服务的无状态性,同时降低了微服务的测试难度,让服务本身更集中关注业务逻辑的处理。 Ant风格的路径表达式： 通配符 说明 ? 匹配任意单个字符 * 匹配任意数量的字符 ** 匹配任意数量的字符，支持多级目录 Hystrix和Ribbon支持当使用path与url的映射关系来配置路由规则的时候，对于路由转发的请求不会采用HystrixCommand来包装，所以这类路由请求没有现成隔离和断路器的保护，并且也不会有负载均衡的能力。因此，在使用Zuul的时候尽量使用path和serviceId的组合来进行配置。 Configclient端的配置文件必须是bootstrap.properties。 获取配置属性的方法有两种： @Value Environment 加密解密 替换JCE 相关端点/encrypt/status：查看加密功能状态的端点。/key：查看密钥的端点。/encrypt：对请求的body内容进行加密的端点。/decrypt：对请求的body内容进行解密的端点。 keytool -genkeypair -alias config-server -keyalg RSA -keystore config-server.keystore 获取远程配置 通过向Config Server发送GET请求以直接的方式获取，可用下面的链接形式 不带{label}分支信息，默认访问master分支，可使用： /{application}-{profile}.yml /{application}-{profile}.properties 带{label}分支信息，可使用： /{label}/{application}-{profile}.yml /{application}/{profile}[/{label}] /{label}/{application}-{profile}.properties 通过客户端配置方式加载的内容如下所示。 spring.application.name：对应配置文件中的{apptication}内容。 spring.cloud.config.profile：对应配置文件中{profile} 内容。 spring.cloud.config.label：对应分支内容，如不配置，默认为master。 动态刷新配置1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 通过POST请求发送到http://localhost:7002/refresh。 BusKafka12345678910111213tar -xzf kafka_2.11-0.11.0.0.tgz# 启动zkbin/zookeeper-server-start.sh config/zookeeper.properties# 启动Kafkabin/kafka-server-start.sh config/server.properties# 创建topicbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test# 查看topicbin/kafka-topics.sh --list --zookeeper localhost:2181# 生产者bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test# 消费者bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hystrix]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%20Cloud%2Fhystrix%2F</url>
    <content type="text"><![CDATA[Hytrix使用的两种方式 1.1 通用方式整合Hytrix 在Controller的方法上加上@HystrixCommand注解 12345678910111213141516171819202122232425262728@RestControllerpublic class MovieController &#123; private static final Logger LOGGER = LoggerFactory.getLogger(MovieController.class); @Autowired private RestTemplate restTemplate; @Autowired private LoadBalancerClient loadBalancerClient; @HystrixCommand(fallbackMethod = &quot;findByIdFallback&quot;) @GetMapping(&quot;/user/&#123;id&#125;&quot;) public User findById(@PathVariable Long id) &#123; return this.restTemplate.getForObject(&quot;http://microservice-provider-user/&quot; + id, User.class); &#125; public User findByIdFallback(Long id) &#123; User user = new User(); user.setId(-1L); user.setName(&quot;默认用户&quot;); return user; &#125; @GetMapping(&quot;/log-user-instance&quot;) public void logUserInstance() &#123; ServiceInstance serviceInstance = this.loadBalancerClient.choose(&quot;microservice-provider-user&quot;); // 打印当前选择的是哪个节点 MovieController.LOGGER.info(&quot;&#123;&#125;:&#123;&#125;:&#123;&#125;&quot;, serviceInstance.getServiceId(), serviceInstance.getHost(), serviceInstance.getPort()); &#125;&#125; 1.2 Feign使用Hystrix 为Feign添加回退 123456789101112131415161718192021222324252627/** * Feign的fallback测试 * 使用@FeignClient的fallback属性指定回退类 * @author 周立 */@FeignClient(name = &quot;microservice-provider-user&quot;, fallback = FeignClientFallback.class)public interface UserFeignClient &#123; @RequestMapping(value = &quot;/&#123;id&#125;&quot;, method = RequestMethod.GET) public User findById(@PathVariable(&quot;id&quot;) Long id);&#125;/** * 回退类FeignClientFallback需实现Feign Client接口 * FeignClientFallback也可以是public class，没有区别 * @author 周立 */@Componentclass FeignClientFallback implements UserFeignClient &#123; @Override public User findById(Long id) &#123; User user = new User(); user.setId(-1L); user.setUsername(&quot;默认用户&quot;); return user; &#125;&#125; 通过Fallback Factory检查回退原因 123456789101112131415161718192021222324252627282930313233@FeignClient(name = &quot;microservice-provider-user&quot;, fallbackFactory = FeignClientFallbackFactory.class)public interface UserFeignClient &#123; @RequestMapping(value = &quot;/&#123;id&#125;&quot;, method = RequestMethod.GET) public User findById(@PathVariable(&quot;id&quot;) Long id);&#125;/** * UserFeignClient的fallbackFactory类，该类需实现FallbackFactory接口，并覆写create方法 * The fallback factory must produce instances of fallback classes that * implement the interface annotated by &#123;@link FeignClient&#125;. * @author 周立 */@Componentclass FeignClientFallbackFactory implements FallbackFactory&lt;UserFeignClient&gt; &#123; private static final Logger LOGGER = LoggerFactory.getLogger(FeignClientFallbackFactory.class); @Override public UserFeignClient create(Throwable cause) &#123; return new UserFeignClient() &#123; @Override public User findById(Long id) &#123; // 日志最好放在各个fallback方法中，而不要直接放在create方法中。 // 否则在引用启动时，就会打印该日志。 // 详见https://github.com/spring-cloud/spring-cloud-netflix/issues/1471 FeignClientFallbackFactory.LOGGER.info(&quot;fallback; reason was:&quot;, cause); User user = new User(); user.setId(-1L); user.setUsername(&quot;默认用户&quot;); return user; &#125; &#125;; &#125;&#125; 说明：请务必注意，在Spring Cloud Dalston中，Feign默认是不开启Hystrix的。因此，如使用Dalston请务必额外设置属性：feign.hystrix.enabled=true，否则断路器不会生效。而，Spring Cloud Angel/Brixton/Camden中，Feign默认都是开启Hystrix的。无需设置该属性。]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Zuul]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%20Cloud%2Fzuul%2F</url>
    <content type="text"><![CDATA[默认情况下，Zuul会代理所有注册到Eureka Server的微服务，并且Zuul的路由规则如下：http://ZUUL_HOST:ZUUL_PORT/微服务在Eureka上的serviceId/**会被转发到serviceId对应的微服务。 Zuul可以使用Ribbon达到负载均衡的效果。 Zuul整合了Hystrix。 路由配置详解： 自定义指定微服务的访问路径配置zuul.routes.指定微服务的serviceId = 指定路径即可。如：123zuul: routes: microservice-provider-user: /user/** 这样设置，microservice-provider-user微服务就会被映射到/user/**路径。 忽略指定微服务使用zuul.ignored-services配置需要忽略的服务，多个用逗号分隔，如：12zuul: ignored-services: microservice-provider-user,microservice-consumer-movie 这样就可让zuul忽略microservice-provider-user和microservice-consumer-movie微服务，只代理其他微服务。 忽略所有微服务，只路由指定微服务。 同时指定微服务的serviceId和对应路径。 同时指定path和url 同时指定path和url，并且不破坏zuul的hystrix、ribbon特性。 使用正则表达式指定zuul的路由匹配规则 路由前缀 忽略某些路径]]></content>
      <categories>
        <category>Spring Cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[vsftpd安装与配置]]></title>
    <url>%2F2019%2F04%2F19%2FFTP%2Fvsftpd%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[企业环境公司为了宣传最新的产品信息，计划搭建 FTP 服务器，为客户提供相关文档的下载。对所有互联网开放共享目录，允许下载产品信息，禁止上传。公司的合作单位能够使用 FTP 服务器进行上传和下载，但不可以删除数据。并且保证服务器的稳定性，进行适当优化设置。 需求分析根据企业的需求，对于不同用户进行不同的权限限制，FTP 服务器需要实现用户的审核。需考虑到服务器的安全性，所以关闭实体用户登录，使用虚拟帐号验证机制，并对不同虚拟帐号设置不同的权限。为了保证服务器的性能，还需要根据用户的等级，限制客户端的连接数及下载速度。 安装1yum -y install vsftpd 解决方案1、创建用户数据库（1）创建用户文本文件先建立用户文本文件 vsftpd_virtualuser.txt，添加两个虚拟帐号，公共帐号 ftp 及客户帐号 vip。12touch /etc/vsftpd/vsftpd_virtualuser.txtvim /etc/vsftpd/vsftpd_virtualuser.txt 格式：1234虚拟帐号 1密码虚拟帐号 2密码 本文使用账号和密码：1234ftp123456vip456789 保存退出。（2）生成数据库保存虚拟帐号和密码的文本文件无法被系统帐号直接调用。我们需要使用 db_load 命令生成 db 数据库文件。1db_load -T -t hash -f /etc/vsftpd/vsftpd_virtualuser.txt /etc/vsftpd/vsftpd_virtualuser.db （3）修改数据库文件访问权限数据库文件中保存着虚拟帐号的密码信息，为了防止非法用户盗取，我们可以修改该文件的访问权限。生成的认证文件的权限应设置为只对 root 用户可读可写，即 600。1chmod 600 /etc/vsftpd/vsftpd_virtualuser.db 2、配置 PAM 文件为了使服务器能够使用数据库文件，对客户端进行身份验证，需要调用系统的 PAM 模块.PAM(Plugable Authentication Module)为可插拔认证模块，不必重新安装应用系统，通过修改指定的配置文件，调整对该程序的认证方式。PAM 模块配置文件路径为/etc/pam.d/目录，此目录下保存着大量与认证有关的配置文件，并以服务名称命名。修改 vsftpd 对应的 PAM 配置文件/etc/pam.d/vsftpd，将默认配置使用“#”全部注释，添加相应字段。12auth required pam_userdb.so db=/etc/vsftpd/vsftpd_virtualuseraccount required pam_userdb.so db=/etc/vsftpd/vsftpd_virtualuser 3、创建虚拟帐号对应的系统用户对于公共帐号和客户帐号，因为需要配置不同的权限，所以可以将两个帐号的目录进行隔离，控制用户的文件访问。公共帐号 ftp 对应系统帐号 ftpuser，并指定其主目录为/var/ftp/share，而客户帐号 vip 对应系统帐号 ftpvip，指定主目录为/var/ftp/vip。 如果不设置可执行用户登录会报不能更改目录错误。12345useradd -d /var/ftp/share ftpuseruseradd -d /var/ftp/vip ftpvipchmod -R 500 /var/ftp/share/chmod -R 700 /var/ftp/vip/ 公共帐号 ftp 只允许下载，修改 share 目录其他用户权限为 rx 可读可执行。1chmod -R 500 /var/ftp/share/ 客户帐号 vip 允许上传和下载，所以对 vip 目录权限设置为 rwx，可读可写可执行。1chmod -R 700 /var/ftp/vip/ 4、建立配置文件设置多个虚拟帐号的不同权限，若使用一个配置文件无法实现此功能，需要为每个虚拟帐号建立独立的配置文件，并根据需要进行相应的设置。（1）修改 vsftpd.conf 主配置文件配置主配置文件 /etc/vsftpd/vsftpd.conf 添加虚拟帐号的共同设置并添加 user_config_dir 字段，定义虚拟帐号的配置文件目录。禁用匿名用户登录并启用本地用户登录设置：12anonymous_enable=NOlocal_enable=YES 12345678910# 将所有本地用户限制在家目录中，NO 则不限制chroot_local_user=YES# 配置 vsftpd 使用的 PAM 模块为 vsftpdpam_service_name=vsftpd# 设置虚拟帐号的主目录为/vuserconfiguser_config_dir=/etc/vsftpd/vuserconfig# 设置 FTP 服务器最大接入客户端数为 300 个max_clients=300# 设置每个 IP 地址最大连接数为 10 个max_per_ip=10 （2）建立虚拟帐号配置文件在 user_config_dir 指定路径下，建立与虚拟帐号同名的配置文件并添加相应的配置字段。首先建立公共帐号 ftp 的配置文件：1vim /etc/vsftpd/vuserconfig/ftp 12345678# 开启虚拟帐号登录guest_enable=yes# 设置 ftp 对应的系统帐号为 ftpuserguest_username=ftpuser# 允许匿名用户浏览器整个服务器的文件系统anon_world_readable_only=no# 限定传输速率为 50KB/sanon_max_rate=50000 注意：vsftpd 对于文件传输速度限制并不是绝对锁定在一个数值上哈，而是在 80%~120%之间变化。比如设置 100KB/s ，则实际是速度在 80KB/s~120KB/s 之间变化。 下面是客户帐号的配置文件 vip：1vim /etc/vsftpd/vuserconfig/vip 1234567891011121314# 开启虚拟帐号登录guest_enable=yes# 设置 ftp 对应的系统帐号为 ftpvipguest_username=ftpvip# 允许匿名用户浏览器整个服务器的文件系统anon_world_readable_only=no# 允许在文件系统写入权限write_enable=yes# 允许创建文件夹anon_mkdir_write_enable=yes# 开启匿名帐号的上传功能anon_upload_enable=yes# 限定传输速度为 100KB/sanon_max_rate=100000 如果需要删除权限，可在配置中添加：anon_other_write_enable=YES 5、重启 vsftpd 使配置生效1systemctl restart vsftpd 6、测试（1）公共帐号 ftp 测试在公共帐号测试前，我们先建立个测试文件productinfo.xls。公共帐号登录 ftp 服务器登录成功测试下载，ok，成功测试上传文件及文件夹，不成功最后测试限速 50KB/s达成目标。（2）客户帐号 vip 测试客户帐号 vip 登录登录成功测试上传，ok，成功测试下载，ok，成功测试删除，ok，不成功测试限速下载 100KB/s达成目标需求。 常见错误及解决办法 cannot change directory关闭SeLinux： 1setenforce 0 500 OOPS: vsftpd: refusing to run with writable root inside chroot()vsftpd.conf添加配置： 1allow_writeable_chroot=YES]]></content>
      <categories>
        <category>FTP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(七)-jvm调优-工具篇]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E4%B8%83)-jvm%E8%B0%83%E4%BC%98-%E5%B7%A5%E5%85%B7%E7%AF%87%2F</url>
    <content type="text"><![CDATA[工具做为图形化界面来展示更能直观的发现问题，另一方面一些耗费性能的分析（dump文件分析）一般也不会在生产直接分析，往往dump下来的文件达1G左右，人工分析效率较低，因此利用工具来分析jvm相关问题，长长可以到达事半功倍的效果来。 jvm监控分析工具一般分为两类，一种是jdk自带的工具，一种是第三方的分析工具。jdk自带工具一般在jdk bin目录下面，以exe的形式直接点击就可以使用，其中包含分析工具已经很强大，几乎涉及了方方面面，但是我们最常使用的只有两款：jconsole.exe和jvisualvm.exe；第三方的分析工具有很多，各自的侧重点不同，比较有代表性的：MAT(Memory Analyzer Tool)、GChisto等。 对于大型 JAVA 应用程序来说，再精细的测试也难以堵住所有的漏洞，即便我们在测试阶段进行了大量卓有成效的工作，很多问题还是会在生产环境下暴露出来，并且很难在测试环境中进行重现。JVM 能够记录下问题发生时系统的部分运行状态，并将其存储在堆转储 (Heap Dump) 文件中，从而为我们分析和诊断问题提供了重要的依据。其中VisualVM和MAT是dump文件的分析利器。 jdk自带的工具jconsoleJconsole（Java Monitoring and Management Console）是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控，是一个基于JMX（java management extensions）的GUI性能监测工具。jconsole使用jvm的扩展机制获取并展示虚拟机中运行的应用程序的性能和资源消耗等信息。 直接在jdk/bin目录下点击jconsole.exe即可启动，界面如下: 在弹出的框中可以选择本机的监控本机的java应用，也可以选择远程的java服务来监控，如果监控远程服务需要在tomcat启动脚本中添加如下代码： 123-Dcom.sun.management.jmxremote.port=6969 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false 连接进去之后，就可以看到jconsole概览图和主要的功能：概述、内存、线程、类、VM、MBeans 概述，以图表的方式显示出堆内存使用量，活动线程数，已加载的类，CUP占用率的折线图，可以非常清晰的观察在程序执行过程中的变动情况。 内存，主要展示了内存的使用情况，同时可以查看堆和非堆内存的变化值对比，也可以点击执行GC来处罚GC的执行 线程，主界面展示线程数的活动数和峰值，同时点击左下方线程可以查看线程的详细信息，比如线程的状态是什么，堆栈内容等，同时也可以点击“检测死锁”来检查线程之间是否有死锁的情况。 类，主要展示已加载类的相关信息。 VM 概要，展示JVM所有信息总览，包括基本信息、线程相关、堆相关、操作系统、VM参数等。 Mbean,查看Mbean的属性，方法等。 VisualVM简介 VisualVM 是一个工具，它提供了一个可视界面，用于查看 Java 虚拟机 (Java Virtual Machine, JVM) 上运行的基于 Java 技术的应用程序（Java 应用程序）的详细信息。VisualVM 对 Java Development Kit (JDK) 工具所检索的 JVM 软件相关数据进行组织，并通过一种使您可以快速查看有关多个 Java 应用程序的数据的方式提供该信息。您可以查看本地应用程序以及远程主机上运行的应用程序的相关数据。此外，还可以捕获有关 JVM 软件实例的数据，并将该数据保存到本地系统，以供后期查看或与其他用户共享。 VisualVM 是javajdk自带的最牛逼的调优工具了吧，也是我平时使用最多调优工具，几乎涉及了jvm调优的方方面面。同样是在jdk/bin目录下面双击jvisualvm.exe既可使用，启动起来后和jconsole 一样同样可以选择本地和远程，如果需要监控远程同样需要配置相关参数，主界面如下； VisualVM可以根据需要安装不同的插件，每个插件的关注点都不同，有的主要监控GC，有的主要监控内存，有的监控线程等。 如何安装： 1、从主菜单中选择“工具”&gt;“插件”。2、在“可用插件”标签中，选中该插件的“安装”复选框。单击“安装”。3、逐步完成插件安装程序。 我这里以 Eclipse(pid 22296)为例，双击后直接展开，主界面展示了系统和jvm两大块内容，点击右下方jvm参数和系统属性可以参考详细的参数信息. 因为VisualVM的插件太多，我这里主要介绍三个我主要使用几个：监控、线程、Visual GC。 监控的主页其实也就是cpu、内存、类、线程的图表。 线程和jconsole功能没有太大的区别 Visual GC 是常常使用的一个功能，可以明显的看到年轻代、老年代的内存变化，以及gc频率、gc的时间等。 以上的功能其实jconsole几乎也有，VisualVM更全面更直观一些，另外VisualVM非常多的其它功能，可以分析dump的内存快照，dump出来的线程快照并且进行分析等，还有其它很多的插件大家可以去探索。 第三方调优工具MATMAT是什么？ MAT(Memory Analyzer Tool)，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗。使用内存分析工具从众多的对象中进行分析，快速的计算出在内存中对象的占用大小，看看是谁阻止了垃圾收集器的回收工作，并可以通过报表直观的查看到可能造成这种结果的对象。 通常内存泄露分析被认为是一件很有难度的工作，一般由团队中的资深人士进行。不过要介绍的 MAT（Eclipse Memory Analyzer）被认为是一个“傻瓜式”的堆转储文件分析工具，你只需要轻轻点击一下鼠标就可以生成一个专业的分析报告。和其他内存泄露分析工具相比，MAT 的使用非常容易，基本可以实现一键到位，即使是新手也能够很快上手使用。 MAT以eclipse 插件的形式来安装，具体的安装过程就不在描述了，可以利用visualvm或者是 jmap命令生产堆文件，导入eclipse mat中生成分析报告： 生成这张报表的同时也会在dump文件的同级目录下生成三份（dump_Top_Consumers.zip、dump_Leak_Suspects.zip、dump_Top_Components.zip）分析结果的html文件，方便发送给相关同事来查看。 需要关注的是下面的Actions、Reports、Step by Step区域： Histogram：列出内存中的对象，对象的个数以及大小，支持正则表达式查找，也可以计算出该类所有对象的retained size Dominator Tree：列出最大的对象以及其依赖存活的Object （大小是以Retained Heap为标准排序的） Top Consumers ： 通过图形列出最大的object duplicate classes ：检测由多个类装载器加载的类 Leak Suspects ：内存泄漏分析 Top Components: 列出大于总堆数的百分之1的报表。 Component Report:分析对象属于同一个包或者被同一个类加载器加载 以上只是一个初级的介绍，MAT还有更强大的使用，比如对比堆内存，在生产环境中往往为了定位问题，每隔几分钟dump出一下内存快照，随后在对比不同时间的堆内存的变化来发现问题。 GChistoGChisto是一款专业分析gc日志的工具，可以通过gc日志来分析：Minor GC、full gc的时间、频率等等，通过列表、报表、图表等不同的形式来反应gc的情况。虽然界面略显粗糙，但是功能还是不错的。 配置好本地的jdk环境之后，双击GChisto.jar，在弹出的输入框中点击 add ，选择gc.log日志。 GC Pause Stats:可以查看GC 的次数、GC的时间、GC的开销、最大GC时间和最小GC时间等，以及相应的柱状图 GC Pause Distribution:查看GC停顿的详细分布，x轴表示垃圾收集停顿时间，y轴表示是停顿次数。 GC Timeline：显示整个时间线上的垃圾收集。 不过这款工具已经不再维护，不能识别最新jdk的日志文件。 gcviewerGCViewer也是一款分析小工具，用于可视化查看由Sun / Oracle，IBM，HP 和 BEA Java 虚拟机产生的垃圾收集器的日志，gcviewer个人感觉显示的界面比较乱，没有GChisto专业。 GC Easy这是一个web工具,在线使用非常方便。 地址: http://gceasy.io 进入官网，讲打包好的zip或者gz为后缀的压缩包上传，过一会就会拿到分析结果。 推荐使用此工具进行gc分析。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(一)-java类的加载机制]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E4%B8%80)-java%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1、什么是类的加载类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError错误）。如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误。 加载.class文件的方式 从本地系统中直接加载 通过网络下载.class文件 从zip，jar等归档文件中加载.class文件 从专有数据库中提取.class文件 将Java源文件动态编译为.class文件 2、类的生命周期 其中类加载的过程包括了加载、验证、准备、解析、初始化五个阶段。在这五个阶段中，加载、验证、准备和初始化这四个阶段发生的顺序是确定的，而解析阶段则不一定，它在某些情况下可以在初始化阶段之后开始，这是为了支持Java语言的运行时绑定（也成为动态绑定或晚期绑定）。另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。 加载查找并加载类的二进制数据。加载是类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情： 通过一个类的全限定名来获取其定义的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。 相对于类加载的其他阶段而言，加载阶段（准确地说，是加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。 加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区之中，而且在Java堆中也创建一个java.lang.Class类的对象，这样便可以通过该对象访问方法区中的这些数据。 连接验证：确保被加载的类的正确性 验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作： 文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 准备：为类的静态变量分配内存，并将其初始化为默认值 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 1、这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。 2、这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。 假设一个类变量的定义为：public static int value = 3; 那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的public static指令是在程序编译后，存放于类构造器&lt;clinit&gt;()方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。 这里还需要注意如下几点： 对基本数据类型来说，对于类变量（static）和全局变量，如果不显式地对其赋值而直接使用，则系统会为其赋予默认的零值，而对于局部变量来说，在使用前必须显式地为其赋值，否则编译时不通过。 对于同时被static和final修饰的常量，必须在声明的时候就为其显式地赋值，否则编译时不通过；而只被final修饰的常量则既可以在声明时显式地为其赋值，也可以在类初始化时显式地为其赋值，总之，在使用前必须为其显式地赋值，系统不会为其赋予默认零值。 对于引用数据类型reference来说，如数组引用、对象引用等，如果没有对其进行显式地赋值而直接使用，系统都会为其赋予默认的零值，即null。 如果在数组初始化时没有对数组中的各元素赋值，那么其中的元素将根据对应的数据类型而被赋予默认的零值。 3、如果类字段的字段属性表中存在ConstantValue属性，即同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值。 假设上面的类变量value被定义为： public static final int value = 3； 编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据ConstantValue的设置将value赋值为3。我们可以理解为static final常量在编译期就将其结果放入了调用它的类的常量池中 解析：把类中的符号引用转换为直接引用 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 符号引用就是一组符号来描述目标，可以是任何字面量。 直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 初始化 初始化，为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式： ①声明类变量时指定初始值 ②使用静态代码块为类变量指定初始值 JVM初始化步骤 1、假如这个类还没有被加载和连接，则程序先加载并连接该类 2、假如该类的直接父类还没有被初始化，则先初始化其直接父类 3、假如类中有初始化语句，则系统依次执行这些初始化语句 类初始化时机：只有当对类的主动使用的时候才会导致类的初始化，类的主动使用包括以下六种： 创建类的实例，也就是new的方式 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（如Class.forName(“com.shengsiyuan.Test”)） 初始化某个类的子类，则其父类也会被初始化 Java虚拟机启动时被标明为启动类的类（Java Test），直接使用java.exe命令来运行某个主类 结束生命周期 在如下几种情况下，Java虚拟机将结束生命周期： 执行了System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致Java虚拟机进程终止 3、类加载器寻找类加载器，先来一个小例子 123456789package com.neo.classloader;public class ClassLoaderTest &#123; public static void main(String[] args) &#123; ClassLoader loader = Thread.currentThread().getContextClassLoader(); System.out.println(loader); System.out.println(loader.getParent()); System.out.println(loader.getParent().getParent()); &#125;&#125; 运行后，输出结果： 123sun.misc.Launcher$AppClassLoader@64fef26asun.misc.Launcher$ExtClassLoader@1ddd40f3null 从上面的结果可以看出，并没有获取到ExtClassLoader的父Loader，原因是Bootstrap Loader（引导类加载器）是用C语言实现的，找不到一个确定的返回父Loader的方式，于是就返回null。 这几种类加载器的层次关系如下图所示： 注意：这里父类加载器并不是通过继承关系来实现的，而是采用组合实现的。 站在Java虚拟机的角度来讲，只存在两种不同的类加载器：启动类加载器：它使用C++实现（这里仅限于Hotspot，也就是JDK1.5之后默认的虚拟机，有很多其他的虚拟机是用Java语言实现的），是虚拟机自身的一部分；所有其它的类加载器：这些类加载器都由Java语言实现，独立于虚拟机之外，并且全部继承自抽象类java.lang.ClassLoader，这些类加载器需要由启动类加载器加载到内存中之后才能去加载其他的类。 站在Java开发人员的角度来看，类加载器可以大致划分为以下三类： 启动类加载器：Bootstrap ClassLoader，负责加载存放在JDK\jre\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库（如rt.jar，所有的java.*开头的类均被Bootstrap ClassLoader加载）。启动类加载器是无法被Java程序直接引用的。 扩展类加载器：Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载JDK\jre\lib\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。 应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 应用程序都是由这三种类加载器互相配合进行加载的，如果有必要，我们还可以加入自定义的类加载器。因为JVM自带的ClassLoader只是懂得从本地文件系统加载标准的java class文件，因此如果编写了自己的ClassLoader，便可以做到如下几点： 1、在执行非置信代码之前，自动验证数字签名。 2、动态地创建符合用户特定需要的定制化构建类。 3、从特定的场所取得java class，例如数据库中和网络中。 JVM类加载机制 全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 4、类的加载类加载有三种方式： 1、命令行启动应用时候由JVM初始化加载 2、通过Class.forName()方法动态加载 3、通过ClassLoader.loadClass()方法动态加载 例子： 12345678910111213package com.neo.classloader;public class LoaderTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; ClassLoader loader = Test2.class.getClassLoader(); System.out.println(loader); //使用ClassLoader.loadClass()来加载类，不会执行初始化块 loader.loadClass("Test2"); //使用Class.forName()来加载类，默认会执行初始化块 //Class.forName("Test2"); //使用Class.forName()来加载类，并指定ClassLoader，初始化时不执行静态块 //Class.forName("Test2", false, loader); &#125; &#125; demo类 12345public class Test2 &#123; static &#123; System.out.println("静态初始化块执行了！"); &#125; &#125; 分别切换加载方式，会有不同的输出结果。 Class.forName()和ClassLoader.loadClass()区别 Class.forName()：将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块。 ClassLoader.loadClass()：只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容，只有在newInstance时才会去执行static块。 Class.forName(name, initialize, loader)：带参函数可控制是否加载static块。initialize为true时加载，为false时不加载。 5、双亲委派模型双亲委派模型的工作流程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 双亲委派机制： 1、当AppClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtClassLoader去完成。 2、当ExtClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。 3、如果BootStrapClassLoader加载失败（例如在$JAVA_HOME/jre/lib里未查找到该class），会使用ExtClassLoader来尝试加载； 4、若ExtClassLoader也加载失败，则会使用AppClassLoader来加载，如果AppClassLoader也加载失败，则会报出异常ClassNotFoundException。 ClassLoader源码分析： 123456789101112131415161718192021222324252627public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false);&#125;protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve)throws ClassNotFoundException &#123; // 首先判断该类型是否已经被加载 Class c = findLoadedClass(name); if (c == null) &#123; //如果没有被加载，就委托给父类加载或者委派给启动类加载器加载 try &#123; if (parent != null) &#123; //如果存在父类加载器，就委派给父类加载器加载 c = parent.loadClass(name, false); &#125; else &#123; //如果不存在父类加载器，就检查是否是由启动类加载器加载的类，通过调用本地方法native Class findBootstrapClass(String name) c = findBootstrapClass0(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // 如果父类加载器和启动类加载器都不能完成加载任务，才调用自身的加载功能 c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; 双亲委派模型意义： 系统类防止内存中出现多份同样的字节码 保证Java程序安全稳定运行 6、自定义类加载器通常情况下，我们都是直接使用系统类加载器。但是，有的时候，我们也需要自定义类加载器。比如应用是通过网络来传输 Java类的字节码，为保证安全性，这些字节码经过了加密处理，这时系统类加载器就无法对其进行加载，这样则需要自定义类加载器来实现。自定义类加载器一般都是继承自ClassLoader类，从上面对loadClass方法来分析来看，我们只需要重写 findClass 方法即可。下面我们通过一个示例来演示自定义类加载器的流程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.neo.classloader;import java.io.*;public class MyClassLoader extends ClassLoader &#123; private String root; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = loadClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] loadClassData(String className) &#123; String fileName = root + File.separatorChar + className.replace('.', File.separatorChar) + ".class"; try &#123; InputStream ins = new FileInputStream(fileName); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 1024; byte[] buffer = new byte[bufferSize]; int length = 0; while ((length = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, length); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; public String getRoot() &#123; return root; &#125; public void setRoot(String root) &#123; this.root = root; &#125; public static void main(String[] args) &#123; MyClassLoader classLoader = new MyClassLoader(); classLoader.setRoot("E:\\temp"); Class&lt;?&gt; testClass = null; try &#123; testClass = classLoader.loadClass("com.neo.classloader.Test2"); Object object = testClass.newInstance(); System.out.println(object.getClass().getClassLoader()); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 自定义类加载器的核心在于对字节码文件的获取，如果是加密的字节码则需要在该类中对文件进行解密。由于这里只是演示，我并未对class文件进行加密，因此没有解密的过程。这里有几点需要注意： 1、这里传递的文件名需要是类的全限定性名称，即com.paddx.test.classloading.Test格式的，因为 defineClass 方法是按这种格式进行处理的。 2、最好不要重写loadClass方法，因为这样容易破坏双亲委托模式。 3、这类Test 类本身可以被 AppClassLoader类加载，因此我们不能把com/paddx/test/classloading/Test.class放在类路径下。否则，由于双亲委托机制的存在，会直接导致该类由AppClassLoader加载，而不会通过我们自定义类加载器来加载。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(三)-GC算法-垃圾收集器]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E4%B8%89)-GC%E7%AE%97%E6%B3%95-%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[这篇文件将给大家介绍GC都有哪几种算法，以及JVM都有那些垃圾回收器，它们的工作原理。 概述垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的。 对象存活判断判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。 在Java语言中，GC Roots包括： 虚拟机栈中引用的对象。 方法区中类静态属性实体引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI引用的对象。 垃圾收集算法标记-清除算法“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。 标记-压缩算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。 “分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。 垃圾收集器 如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现 Serial收集器串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停） 参数控制：-XX:+UseSerialGC 串行收集器 ParNew收集器 ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩 参数控制： -XX:+UseParNewGC ParNew收集器-XX:ParallelGCThreads 限制线程数量 Parallel收集器Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩 参数控制：-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行 Parallel Old 收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。这个收集器是在JDK 1.6中才开始提供 参数控制： -XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。 从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew） 优点: 并发收集、低停顿缺点: 产生大量空间碎片、并发阶段会降低吞吐量 参数控制： -XX:+UseConcMarkSweepGC 使用CMS收集器-XX:+UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长-XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理-XX:ParallelCMSThreads 设定CMS的线程数量（一般情况约等于可用CPU数量） G1收集器G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点： 空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。 可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。 上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。 G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。 收集步骤： 1、标记阶段，首先初始标记(Initial-Mark)，这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark) 2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。 3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 4、Remark，再标记，会有短暂停顿(STW)。再标记阶段是用来收集并发标记阶段产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法：snapshot-at-the-beginning (SATB)。 5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。 6、复制/清除过程后，回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。 常用的收集器组合 服务器31 新生代GC策略 老年老代GC策略 说明 组合1 Serial Serial Old Serial和Serial Old都是单线程进行GC，特点就是GC时暂停所有应用线程。 组合2 Serial CMS+Serial Old CMS（Concurrent Mark Sweep）是并发GC，实现GC线程和应用线程并发工作，不需要暂停所有应用线程。另外，当CMS进行GC失败时，会自动使用Serial Old策略进行GC。 组合3 ParNew CMS 使用-XX:+UseParNewGC选项来开启。ParNew是Serial的并行版本，可以指定GC线程数，默认GC线程数为CPU的数量。可以使用-XX:ParallelGCThreads选项指定GC的线程数。如果指定了选项-XX:+UseConcMarkSweepGC选项，则新生代默认使用ParNew GC策略。 组合4 ParNew Serial Old 使用-XX:+UseParNewGC选项来开启。新生代使用ParNew GC策略，年老代默认使用Serial Old GC策略。 组合5 Parallel Scavenge Serial Old Parallel Scavenge策略主要是关注一个可控的吞吐量：应用程序运行时间 / (应用程序运行时间 + GC时间)，可见这会使得CPU的利用率尽可能的高，适用于后台持久运行的应用程序，而不适用于交互较多的应用程序。 组合6 Parallel Scavenge Parallel Old Parallel Old是Serial Old的并行版本 组合7 G1GC G1GC -XX:+UnlockExperimentalVMOptions -XX:+UseG1GC #开启；-XX:MaxGCPauseMillis =50 #暂停时间目标；-XX:GCPauseIntervalMillis =200 #暂停间隔目标；-XX:+G1YoungGenSize=512m #年轻代大小；-XX:SurvivorRatio=6#幸存区比例]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(九)-如何优化Java-GC]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E4%B9%9D)-%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96Java-GC%2F</url>
    <content type="text"><![CDATA[本文由CrowHawk翻译，地址：如何优化Java GC「译」，是Java GC调优的经典佳作。 Sangmin Lee发表在Cubrid上的“Become a Java GC Expert”系列文章的第三篇《How to Tune Java Garbage Collection》，本文的作者是韩国人，写在JDK 1.8发布之前，虽然有些地方有些许过时，但整体内容还是非常有价值的。译者此前也看到有人翻译了本文，发现其中有许多错漏生硬和语焉不详之处，因此决定自己翻译一份，供大家分享。 本文是“成为Java GC专家”系列文章的第三篇，在系列的第一篇文章《理解Java GC》中，我们了解到了不同GC算法的执行过程、GC的工作原理、新生代和老年代的概念、JDK 7中你需要了解的5种GC类型以及每一种GC对性能的影响。 在系列的第二篇文章《如何监控Java GC》中笔者已经解释了JVM进行实时GC的原理、监控GC的方法以及可以使这一过程更加迅速高效的工具。 在第三篇文章中，笔者将基于实际生产环境中的案例，介绍几个GC优化的最佳参数设置。在此我们假设你已经理解了本系列前两篇文章的内容，因此为了更深入的理解本文所讲内容，我建议你在阅读本篇文章之前先仔细阅读这两篇文章。 GC优化是必要的吗？或者更准确地说，GC优化对Java基础服务来说是必要的吗？答案是否定的，事实上GC优化对Java基础服务来说在有些场合是可以省去的，但前提是这些正在运行的Java系统，必须包含以下参数或行为： 内存大小已经通过-Xms和-Xmx参数指定过 运行在server模式下（使用-server参数） 系统中没有残留超时日志之类的错误日志 换句话说，如果你在运行时没有手动设置内存大小并且打印出了过多的超时日志，那你就需要对系统进行GC优化。 不过你需要时刻谨记一句话：GC tuning is the last task to be done. 现在来想一想GC优化的最根本原因，垃圾收集器的工作就是清除Java创建的对象，垃圾收集器需要清理的对象数量以及要执行的GC数量均取决于已创建的对象数量。因此，为了使你的系统在GC上表现良好，首先需要减少创建对象的数量。 俗话说“冰冻三尺非一日之寒”，我们在编码时要首先要把下面这些小细节做好，否则一些琐碎的不良代码累积起来将让GC的工作变得繁重而难于管理： 使用StringBuilder或StringBuffer来代替String 尽量少输出日志 尽管如此，仍然会有我们束手无策的情况。XML和JSON解析过程往往占用了最多的内存，即使我们已经尽可能地少用String、少输出日志，仍然会有大量的临时内存（大约10-100MB）被用来解析XML或JSON文件，但我们又很难弃用XML和JSON。在此，你只需要知道这一过程会占据大量内存即可。 如果在经过几次重复的优化后应用程序的内存用量情况有所改善，那么久可以启动GC优化了。 笔者总结了GC优化的两个目的： 将进入老年代的对象数量降到最低 减少Full GC的执行时间 将进入老年代的对象数量降到最低除了可以在JDK 7及更高版本中使用的G1收集器以外，其他分代GC都是由Oracle JVM提供的。关于分代GC，就是对象在Eden区被创建，随后被转移到Survivor区，在此之后剩余的对象会被转入老年代。也有一些对象由于占用内存过大，在Eden区被创建后会直接被传入老年代。老年代GC相对来说会比新生代GC更耗时，因此，减少进入老年代的对象数量可以显著降低Full GC的频率。你可能会以为减少进入老年代的对象数量意味着把它们留在新生代，事实正好相反，新生代内存的大小是可以调节的。 降低Full GC的时间Full GC的执行时间比Minor GC要长很多，因此，如果在Full GC上花费过多的时间（超过1s），将可能出现超时错误。 如果通过减小老年代内存来减少Full GC时间，可能会引起OutOfMemoryError或者导致Full GC的频率升高。 另外，如果通过增加老年代内存来降低Full GC的频率，Full GC的时间可能因此增加。 因此，你需要把老年代的大小设置成一个“合适”的值。 影响GC性能的参数正如我在系列的第一篇文章《理解Java GC》末尾提到的，不要幻想着“如果有人用他设置的GC参数获取了不错的性能，我们为什么不复制他的参数设置呢？”，因为对于不用的Web服务，它们创建的对象大小和生命周期都不相同。 举一个简单的例子，如果一个任务的执行条件是A，B，C，D和E，另一个完全相同的任务执行条件只有A和B，那么哪一个任务执行速度更快呢？作为常识来讲，答案很明显是后者。 Java GC参数的设置也是这个道理，设置好几个参数并不会提升GC执行的速度，反而会使它变得更慢。GC优化的基本原则是将不同的GC参数应用到两个及以上的服务器上然后比较它们的性能，然后将那些被证明可以提高性能或减少GC执行时间的参数应用于最终的工作服务器上。 下面这张表展示了与内存大小相关且会影响GC性能的GC参数 表1：GC优化需要考虑的JVM参数 类型 参数 描述 堆内存大小 -Xms 启动JVM时堆内存的大小 -Xmx 堆内存最大限制 新生代空间大小 -XX:NewRatio 新生代和老年代的内存比 -XX:NewSize 新生代内存大小 -XX:SurvivorRatio Eden区和Survivor区的内存比 笔者在进行GC优化时最常用的参数是-Xms，-Xmx和-XX:NewRatio。-Xms和-Xmx参数通常是必须的，所以NewRatio的值将对GC性能产生重要的影响。 有些人可能会问如何设置永久代内存大小，你可以用-XX:PermSize和-XX:MaxPermSize参数来进行设置，但是要记住，只有当出现OutOfMemoryError错误时你才需要去设置永久代内存。 还有一个会影响GC性能的因素是垃圾收集器的类型，下表展示了关于GC类型的可选参数（基于JDK 6.0）： 表2：GC类型可选参数 GC类型 参数 备注 Serial GC -XX:+UseSerialGC Parallel GC -XX:+UseParallelGC-XX:ParallelGCThreads=value Parallel Compacting GC -XX:+UseParallelOldGC CMS GC -XX:+UseConcMarkSweepGC-XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled-XX:CMSInitiatingOccupancyFraction=value-XX:+UseCMSInitiatingOccupancyOnly G1 -XX:+UnlockExperimentalVMOptions-XX:+UseG1GC 在JDK 6中这两个参数必须配合使用 除了G1收集器外，可以通过设置上表中每种类型第一行的参数来切换GC类型，最常见的非侵入式GC就是Serial GC，它针对客户端系统进行了特别的优化。 会影响GC性能的参数还有很多，但是上述的参数会带来最显著的效果，请切记，设置太多的参数并不一定会提升GC的性能。 GC优化的过程GC优化的过程和大多数常见的提升性能的过程相似，下面是笔者使用的流程： 1. 监控GC状态你需要监控GC从而检查系统中运行的GC的各种状态，具体方法请查看系列的第二篇文章《如何监控Java GC》。 2. 分析监控结果后决定是否需要优化GC在检查GC状态后，你需要分析监控结构并决定是否需要进行GC优化。如果分析结果显示运行GC的时间只有0.1-0.3秒，那么就不需要把时间浪费在GC优化上，但如果运行GC的时间达到1-3秒，甚至大于10秒，那么GC优化将是很有必要的。 但是，如果你已经分配了大约10GB内存给Java，并且这些内存无法省下，那么就无法进行GC优化了。在进行GC优化之前，你需要考虑为什么你需要分配这么大的内存空间，如果你分配了1GB或2GB大小的内存并且出现了OutOfMemoryError，那你就应该执行堆转储（heap dump）来消除导致异常的原因。 注意：堆转储（heap dump）是一个用来检查Java内存中的对象和数据的内存文件。该文件可以通过执行JDK中的jmap命令来创建。在创建文件的过程中，所有Java程序都将暂停，因此，不要在系统执行过程中创建该文件。你可以在互联网上搜索heap dump的详细说明。对于韩国读者，可以直接参考我去年发布的书：《The story of troubleshooting for Java developers and system operators》 (Sangmin Lee, Hanbit Media, 2011, 416 pages) 3. 设置GC类型/内存大小如果你决定要进行GC优化，那么你需要选择一个GC类型并且为它设置内存大小。此时如果你有多个服务器，请如上文提到的那样，在每台机器上设置不同的GC参数并分析它们的区别。 4. 分析结果在设置完GC参数后就可以开始收集数据，请在收集至少24小时后再进行结果分析。如果你足够幸运，你可能会找到系统的最佳GC参数。如若不然，你还需要分析输出日志并检查分配的内存，然后需要通过不断调整GC类型/内存大小来找到系统的最佳参数。 5. 如果结果令人满意，将参数应用到所有服务器上并结束GC优化如果GC优化的结果令人满意，就可以将参数应用到所有服务器上，并停止GC优化。 在下面的章节中，你将会看到上述每一步所做的具体工作。 监控GC状态并分析结果在运行中的Web应用服务器（Web Application Server, WAS）上查看GC状态的最佳方式就是使用jstat命令。笔者在《如何监控Java GC》中已经介绍过了jstat命令，所以在本篇文章中我将着重关注数据部分。 下面的例子展示了某个还没有执行GC优化的JVM的状态（虽然它并不是运行服务器）。 1234$ jstat -gcutil 21719 1sS0 S1 E O P YGC YGCT FGC FGCT GCT48.66 0.00 48.10 49.70 77.45 3428 172.623 3 59.050 231.67348.66 0.00 48.10 49.70 77.45 3428 172.623 3 59.050 231.673 我们先看一下YGC（从应用程序启动到采样时发生 Young GC 的次数）和YGCT（从应用程序启动到采样时 Young GC 所用的时间(秒)），计算YGCT/YGC会得出，平均每次新生代的GC耗时50ms，这是一个很小的数字，通过这个结果可以看出，我们大可不必关注新生代GC对GC性能的影响。 现在来看一下FGC（ 从应用程序启动到采样时发生 Full GC 的次数）和FGCT（从应用程序启动到采样时 Full GC 所用的时间(秒)），计算FGCT/FGC会得出，平均每次老年代的GC耗时19.68s。有可能是执行了三次Full GC，每次耗时19.68s，也有可能是有两次只花了1s,另一次花了58s。不管是哪一种情况，GC优化都是很有必要的。 使用jstat命令可以很容易地查看GC状态，但是分析GC的最佳方式是加上-verbosegc参数来生成日志。在之前的文章中笔者已经解释了如何分析这些日志。HPJMeter是笔者最喜欢的用于分析-verbosegc生成的日志的工具，它简单易用，使用HPJmeter可以很容易地查看GC执行时间以及GC发生频率。 此外，如果GC执行时间满足下列所有条件，就没有必要进行GC优化了： Minor GC执行非常迅速（50ms以内） Minor GC没有频繁执行（大约10s执行一次） Full GC执行非常迅速（1s以内） Full GC没有频繁执行（大约10min执行一次） 括号中的数字并不是绝对的，它们也随着服务的状态而变化。有些服务可能要求一次Full GC在0.9s以内，而有些则会放得更宽一些。因此，对于不同的服务，需要按照不同的标准考虑是否需要执行GC优化。 当检查GC状态时，不能只查看Minor GC和Full GC的时间，还必须要关注GC执行的次数。如果新生代空间太小，Minor GC将会非常频繁地执行（有时每秒会执行一次，甚至更多）。此外，传入老年代的对象数目会上升，从而导致Full GC的频率升高。因此，在执行jstat命令时，请使用-gccapacity参数来查看具体占用了多少空间。 设置GC类型/内存大小设置GC类型Oracle JVM有5种垃圾收集器，但是在JDK 7以前的版本中，你只能在Parallel GC，Parallel Compacting GC 和CMS GC之中选择，至于具体选择哪个，则没有具体的原则和规则。 既然这样的话，我们如何来选择GC呢？最好的方法是把三种都用上，但是有一点必须明确——CMS GC通常比其他并行（Parallel）GC都要快（这是因为CMS GC是并发的GC），如果确实如此，那只选择CMS GC就可以了，不过CMS GC也不总是更快，当出现concurrent mode failure时，CMS GC就会比并行GC更慢了。 Concurrent mode failure 现在让我们来深入地了解一下concurrent mode failure。 并行GC和CMS GC的最大区别是并行GC采用“标记-整理”(Mark-Compact)算法而CMS GC采用“标记-清除”(Mark-Sweep)算法（具体内容可参照译者的文章《GC算法与内存分配策略》），compact步骤就是通过移动内存来消除内存碎片，从而消除分配的内存之间的空白区域。 对于并行GC来说，无论何时执行Full GC，都会进行compact工作，这消耗了太多的时间。不过在执行完Full GC后，下次内存分配将会变得更快（因为直接顺序分配相邻的内存）。 相反，CMS GC没有compact的过程，因此CMS GC运行的速度更快。但是也是由于没有整理内存，在进行磁盘清理之前，内存中会有很多零碎的空白区域，这也导致没有足够的空间分配给大对象。例如，在老年代还有300MB可用空间，但是连一个10MB的对象都没有办法被顺序存储在老年代中，在这种情况下，会报出“concurrent mode failure”的warning，然后系统执行compact操作。但是CMS GC在这种情况下执行的compact操作耗时要比并行GC高很多，并且这还会导致另一个问题，关于“concurrent mode failure”的详细说明，可用参考Oracle工程师撰写的《Understanding CMS GC Logs》。 综上所述，你需要根据你的系统情况为其选择一个最适合的GC类型。 每个系统都有最适合它的GC类型等着你去寻找，如果你有6台服务器，我建议你每两个服务器设置相同的参数，然后加上-verbosegc参数再分析结果。 设置内存大小下面展示了内存大小、GC运行次数和GC运行时间之间的关系： 大内存空间 减少了GC的次数 提高了GC的运行时间 小内存空间 增多了GC的次数 降低了GC的运行时间 关于如何设置内存的大小，没有一个标准答案，如果服务器资源充足并且Full GC能在1s内完成，把内存设为10GB也是可以的，但是大部分服务器并不处在这种状态中，当内存设为10GB时，Full GC会耗时10-30s，具体的时间自然与对象的大小有关。 既然如此，我们该如何设置内存大小呢？通常我推荐设为500MB，这不是说你要通过-Xms500m和-Xmx500m参数来设置WAS内存。根据GC优化之前的状态，如果Full GC后还剩余300MB的空间，那么把内存设为1GB是一个不错的选择（300MB（默认程序占用）+ 500MB（老年代最小空间）+200MB（空闲内存））。这意味着你需要为老年代设置至少500MB空间，因此如果你有三个运行服务器，可以把它们的内存分别设置为1GB，1.5GB，2GB，然后检查结果。 理论上来说，GC执行速度应该遵循1GB &gt; 1.5GB &gt; 2GB，1GB内存时GC执行速度最快。然而，理论上的1GB内存Full GC消耗1s、2GB内存Full GC消耗2s在现实里是无法保证的，实际的运行时间还依赖于服务器的性能和对象大小。因此，最好的方法是创建尽可能多的测量数据并监控它们。 在设置内存空间大小时，你还需要设置一个参数：NewRatio。NewRatio的值是新生代和老年代空间大小的比例。如果XX:NewRatio=1，则新生代空间:老年代空间=1:1，如果堆内存为1GB，则新生代:老年代=500MB:500MB。如果NewRatio等于2，则新生代:老年代=1:2，因此，NewRatio的值设置得越大，则老年代空间越大，新生代空间越小。 你可能会认为把NewRatio设为1会是最好的选择，然而事实并非如此，根据笔者的经验，当NewRatio设为2或3时，整个GC的状态表现得更好。 完成GC优化最快地方法是什么？答案是比较性能测试的结果。为了给每台服务器设置不同的参数并监控它们，最好查看的是一或两天后的数据。当通过性能测试来进行GC优化时，你需要在不同的测试时保证它们有相同的负载和运行环境。然而，即使是专业的性能测试人员，想精确地控制负载也很困难，并且需要大量的时间准备。因此，更加方便容易的方式是直接设置参数来运行，然后等待运行的结果（即使这需要消耗更多的时间）。 分析GC优化的结果在设置了GC参数和-verbosegc参数后，可以使用tail命令确保日志被正确地生成。如果参数设置得不正确或日志未生成，那你的时间就被白白浪费了。如果日志收集没有问题的话，在收集一或两天数据后再检查结果。最简单的方法是把日志从服务器移到你的本地PC上，然后用HPJMeter分析数据。 在分析结果时，请关注下列几点（这个优先级是笔者根据自己的经验拟定的，我认为选取GC参数时应考虑的最重要的因素是Full GC的运行时间。）： 单次Full GC运行时间 单次Minor GC运行时间 Full GC运行间隔 Minor GC运行间隔 整个Full GC的时间 整个Minor GC的运行时间 整个GC的运行时间 Full GC的执行次数 Minor GC的执行次数 找到最佳的GC参数是件非常幸运的，然而在大多数时候，我们并不会如此幸运，在进行GC优化时一定要小心谨慎，因为当你试图一次完成所有的优化工作时，可能会出现OutOfMemoryError错误。 优化案例到目前为止，我们一直在从理论上介绍GC优化，现在是时候将这些理论付诸实践了，我们将通过几个例子来更深入地理解GC优化。 示例1下面这个例子是针对Service S的优化，对于最近刚开发出来的Service S，执行Full GC需要消耗过多的时间。 现在看一下执行jstat -gcutil的结果：12S0 S1 E O P YGC YGCT FGC FGCT GCT12.16 0.00 5.18 63.78 20.32 54 2.047 5 6.946 8.993 左边的Perm区的值对于最初的GC优化并不重要，而YGC参数的值更加对于这次优化更为重要。 平均执行一次Minor GC和Full GC消耗的时间如下表所示： 表3：Service S的Minor GC 和Full GC的平均执行时间 GC类型 GC执行次数 GC执行时间 平均值 Minor GC 54 2.047s 37ms Full GC 5 6.946s 1.389s 37ms对于Minor GC来说还不赖，但1.389s对于Full GC来说意味着当GC发生在数据库Timeout设置为1s的系统中时，可能会频繁出现超时现象。 首先，你需要检查开始GC优化前内存的使用情况。使用jstat -gccapacity命令可以检查内存用量情况。在笔者的服务器上查看到的结果如下： 12NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC PGCMN PGCMX PGC PC YGC FGC212992.0 212992.0 212992.0 21248.0 21248.0 170496.0 1884160.0 1884160.0 1884160.0 1884160.0 262144.0 262144.0 262144.0 262144.0 54 5 其中的关键值如下： 新生代内存用量：212,992 KB 老年代内存用量：1,884,160 KB 因此，除了永久代以外，被分配的内存空间加起来有2GB，并且新生代:老年代=1:9，为了得到比使用jstat更细致的结果，还需加上-verbosegc参数获取日志，并把三台服务器按照如下方式设置（除此以外没有使用任何其他参数）： NewRatio=2 NewRatio=3 NewRatio=4 一天后我得到了系统的GC log，幸运的是，在设置完NewRatio后系统没有发生任何Full GC。 这是为什么呢？这是因为大部分对象在创建后很快就被回收了，所有这些对象没有被传入老年代，而是在新生代就被销毁回收了。 在这样的情况下，就没有必要去改变其他的参数值了，只要选择一个最合适的NewRatio值即可。那么，如何确定最佳的NewRatio值呢？为此，我们分析一下每种NewRatio值下Minor GC的平均响应时间。 在每种参数下Minor GC的平均响应时间如下： NewRatio=2：45ms NewRatio=3：34ms NewRatio=4：30ms 我们可以根据GC时间的长短得出NewRatio=4是最佳的参数值（尽管NewRatio=4时新生代空间是最小的）。在设置完GC参数后，服务器没有发生Full GC。 为了说明这个问题，下面是服务执行一段时间后执行jstat –gcutil的结果： 12S0 S1 E O P YGC YGCT FGC FGCT GCT8.61 0.00 30.67 24.62 22.38 2424 30.219 0 0.000 30.219 你可能会认为是服务器接收的请求少才使得GC发生的频率较低，实际上，虽然Full GC没有执行过，但Minor GC被执行了2424次。 示例2这是一个Service A的例子。我们通过公司内部的应用性能管理系统（APM）发现JVM暂停了相当长的时间（超过8秒），因此我们进行了GC优化。我们努力寻找JVM暂停的原因，后来发现是因为Full GC执行时间过长，因此我们决定进行GC优化。 在GC优化的开始阶段，我们加上了-verbosegc参数，结果如下图所示： 上图是由HPJMeter生成的图片之一。横坐标表示JVM执行的时间，纵坐标表示每次GC的时间。CMS为绿点，表示Full GC的结果，而Parallel Scavenge为蓝点，表示Minor GC的结果。 之前我说过CMS GC是最快的GC，但是上面的结果显示在一些时候CMS耗时达到了15s。是什么导致了这一结果？请记住我之前说的：CMS在执行compact（整理）操作时会显著变慢。此外，服务的内存通过-Xms1g和=Xmx4g设置了，而分配的内存只有4GB。 因此笔者将GC类型从CMS GC改为了Parallel GC，把内存大小设为2GB，并把NewRatio设为3。在执行jstat -gcutil几小时后的结果如下： 12S0 S1 E O P YGC YGCT FGC FGCT GCT0.00 30.48 3.31 26.54 37.01 226 11.131 4 11.758 22.890 Full GC的时间缩短了，变成了每次3s，跟15s比有了显著提升。但是3s依然不够快，为此笔者创建了以下6种情况： Case 1: -XX:+UseParallelGC -Xms1536m -Xmx1536m -XX:NewRatio=2 Case 2: -XX:+UseParallelGC -Xms1536m -Xmx1536m -XX:NewRatio=3 Case 3: -XX:+UseParallelGC -Xms1g -Xmx1g -XX:NewRatio=3 Case 4: -XX:+UseParallelOldGC -Xms1536m -Xmx1536m -XX:NewRatio=2 Case 5: -XX:+UseParallelOldGC -Xms1536m -Xmx1536m -XX:NewRatio=3 Case 6: -XX:+UseParallelOldGC -Xms1g -Xmx1g -XX:NewRatio=3 上面哪一种情况最快？结果显示，内存空间越小，运行结果最少。下图展示了性能最好的Case 6的结果图，它的最慢响应时间只有1.7s，并且响应时间的平均值已经被控制到了1s以内。 基于上图的结果，按照Case 6调整了GC参数，但这却导致每晚都会发生OutOfMemoryError。很难解释发生异常的具体原因，简单地说，应该是批处理程序导致了内存泄漏，我们正在解决相关的问题。 如果只对GC日志做一些短时间的分析就将相关参数部署到所有服务器上来执行GC优化，这将是非常危险的。切记，只有当你同时仔细分析服务的执行情况和GC日志后，才能保证GC优化没有错误地执行。 在上文中，我们通过两个GC优化的例子来说明了GC优化是怎样执行的。正如上文中提到的，例子中设置的GC参数可以设置在相同的服务器之上，但前提是他们具有相同的CPU、操作系统、JDK版本并且运行着相同的服务。此外，不要把我使用的参数照搬到你的应用上，它们可能在你的机器上并不能起到同样良好的效果。 总结笔者没有执行heap dump并分析内存的详细内容，而是通过自己的经验进行GC优化。精确地分析内存可以得到更好的优化效果，不过这种分析一般只适用于内存使用量相对固定的场景。如果服务严重过载并占有了大量的内存，则建议你根据之前的经验进行GC优化。 笔者已经在一些服务上设置了G1 GC参数并进行了性能测试，但还没有应用于正式的生产环境。G1 GC的速度快于任何其他的GC类型，但是你必须要升级到JDK 7。此外，暂时还无法保证它的稳定性，没有人知道运行时是否会出现致命的错误，因此G1 GC暂时还不适合投入应用。 等未来JDK 7真正稳定了（这并不是说它现在不稳定），并且WAS针对JDK 7进行优化后，G1 GC最终能按照预期的那样来工作，等到那一天我们可能就不再需要GC优化了。 想了解关于GC优化的更多细节，请前往Slideshare.com 查看相关资料。强烈推荐Everything I Ever Learned About JVM Performance Tuning @Twitter,作者是Attila Szegedi, 一名Twitter工程师，请花些时间好好阅读它。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(二)-JVM内存结构]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E4%BA%8C)-JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[所有的Java开发人员可能会遇到这样的困惑？我该为堆内存设置多大空间呢？OutOfMemoryError的异常到底涉及到运行时数据的哪块区域？该怎么解决呢？其实如果你经常解决服务器性能问题，那么这些问题就会变的非常常见，了解JVM内存也是为了服务器出现性能问题的时候可以快速的了解哪块的内存区域出现问题，以便于快速的解决生产故障。 先看一张图，这张图能很清晰的说明JVM内存结构布局。 JVM内存结构主要有三大块：堆内存、方法区和栈。 堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间，默认情况下年轻代按照8:1:1的比例来分配； 方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)； 栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。 在通过一张图来了解如何通过参数来控制各区域的内存大小 控制参数： -Xms设置堆的最小空间大小。 -Xmx设置堆的最大空间大小。 -XX:NewSize设置新生代最小空间大小。 -XX:MaxNewSize设置新生代最大空间大小。 -XX:PermSize设置永久代最小空间大小。 -XX:MaxPermSize设置永久代最大空间大小。 -Xss设置每个线程的堆栈大小。 没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。 老年代空间大小=堆空间大小-年轻代空间大小 从更高的一个维度再次来看JVM和系统调用之间的关系： 方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序计数器是运行是线程私有的内存区域。 下面我们详细介绍每个区域的作用。 Java堆（Heap）对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。 根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 方法区（Method Area）方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。 Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 方法区有时被称为持久代（PermGen）。 所有的对象在实例化后的整个运行周期内，都被存放在堆内存中。堆内存又被划分成不同的部分：伊甸区(Eden)，幸存者区域(Survivor Sapce)，老年代（Old Generation Space）。 方法的执行都是伴随着线程的。原始类型的本地变量以及引用都存放在线程栈中。而引用关联的对象比如String，都存在在堆中。为了更好的理解上面这段话，我们可以看一个例子： 123456789101112import java.text.SimpleDateFormat;import java.util.Date;import org.apache.log4j.Logger;public class HelloWorld &#123; private static Logger LOGGER = Logger.getLogger(HelloWorld.class.getName()); public void sayHello(String message) &#123; SimpleDateFormat formatter = new SimpleDateFormat(&quot;dd.MM.YYYY&quot;); String today = formatter.format(new Date()); LOGGER.info(today + &quot;: &quot; + message); &#125;&#125; 这段程序的数据在内存中的存放如下： 通过JConsole工具可以查看运行中的Java程序（比如Eclipse）的一些信息：堆内存的分配，线程的数量以及加载的类的个数； 程序计数器（Program Counter Register）程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 JVM栈（JVM Stacks）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。 本地方法栈（Native Method Stacks）本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 哪儿的OutOfMemoryError对内存结构清晰的认识同样可以帮助理解不同OutOfMemoryErrors： 1Exception in thread “main”: java.lang.OutOfMemoryError: Java heap space 原因：对象不能被分配到堆内存中 1Exception in thread “main”: java.lang.OutOfMemoryError: PermGen space 原因：类或者方法不能被加载到持久代。它可能出现在一个程序加载很多类的时候，比如引用了很多第三方的库； 1Exception in thread “main”: java.lang.OutOfMemoryError: Requested array size exceeds VM limit 原因：创建的数组大于堆内存的空间 1Exception in thread “main”: java.lang.OutOfMemoryError: request &lt;size&gt; bytes for &lt;reason&gt;. Out of swap space? 原因：分配本地分配失败。JNI、本地库或者Java虚拟机都会从本地堆中分配内存空间。 1Exception in thread “main”: java.lang.OutOfMemoryError: &lt;reason&gt; &lt;stack trace&gt;（Native method） 原因：同样是本地方法内存分配失败，只不过是JNI或者本地方法或者Java虚拟机发现]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(五)-Java-GC-分析]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E4%BA%94)-Java-GC-%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Java GC就是JVM记录仪，书画了JVM各个分区的表演。 什么是 Java GCJava GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一。作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清理机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。 在Java语言出现之前，就有GC机制的存在，如Lisp语言），Java GC机制已经日臻完善，几乎可以自动的为我们做绝大多数的事情。然而，如果我们从事较大型的应用软件开发，曾经出现过内存优化的需求，就必定要研究Java GC机制。 简单总结一下，Java GC就是通过GC收集器回收不在存活的对象，保证JVM更加高效的运转。如果不了解GC算法和垃圾回收器可以参考这篇文章：jvm系列(三):GC算法 垃圾收集器。 如何获取 Java GC日志一般情况可以通过两种方式来获取GC日志，一种是使用命令动态查看，一种是在容器中设置相关参数打印GC日志。 命令动态查看Java 自动的工具行命令，jstat可以用来动态监控JVM内存的使用，统计垃圾回收的各项信息。 比如常用命令，jstat -gc 统计垃圾回收堆的行为 123$ jstat -gc 1262 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 26112.0 24064.0 6562.5 0.0 564224.0 76274.5 434176.0 388518.3 524288.0 42724.7 320 6.417 1 0.398 6.815 也可以设置间隔固定时间来打印： 1$ jstat -gc 1262 2000 20 这个命令意思就是每隔2000ms输出1262的gc情况，一共输出20次 更详细的内容参考这篇文章：jvm系列(四):jvm调优-命令篇 GC参数JVM的GC日志的主要参数包括如下几个： -XX:+PrintGC 输出GC日志 -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2017-09-04T21:53:59.234+0800） -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log 日志文件的输出路径 在生产环境中，根据需要配置相应的参数来监控JVM运行情况。 Tomcat 设置示例我们经常在tomcat的启动参数中添加JVM相关参数，这里有一个典型的示例： 123456JAVA_OPTS=&quot;-server -Xms2000m -Xmx2000m -Xmn800m -XX:PermSize=64m -XX:MaxPermSize=256m -XX:SurvivorRatio=4-verbose:gc -Xloggc:$CATALINA_HOME/logs/gc.log -Djava.awt.headless=true -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Dsun.rmi.dgc.server.gcInterval=600000 -Dsun.rmi.dgc.client.gcInterval=600000-XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=15&quot; 根据上面的参数我们来做一下解析： -Xms2000m -Xmx2000m -Xmn800m -XX:PermSize=64m -XX:MaxPermSize=256mXms，即为jvm启动时得JVM初始堆大小，Xmx为jvm的最大堆大小，xmn为新生代的大小，permsize为永久代的初始大小，MaxPermSize为永久代的最大空间。 -XX:SurvivorRatio=4SurvivorRatio为新生代空间中的Eden区和救助空间Survivor区的大小比值，默认是8，则两个Survivor区与一个Eden区的比值为2:8，一个Survivor区占整个年轻代的1/10。调小这个参数将增大survivor区，让对象尽量在survivor区待长一点，减少进入年老代的对象。去掉救助空间的想法是让大部分不能马上回收的数据尽快进入年老代，加快年老代的回收频率，减少年老代暴涨的可能性，这个是通过将-XX:SurvivorRatio设置成比较大的值（比如65536）来做到。 -verbose:gc -Xloggc:$CATALINA_HOME/logs/gc.log将虚拟机每次垃圾回收的信息写到日志文件中，文件名由file指定，文件格式是平文件，内容和-verbose:gc输出内容相同。 -Djava.awt.headless=true Headless模式是系统的一种配置模式。在该模式下，系统缺少了显示设备、键盘或鼠标。 -XX:+PrintGCTimeStamps -XX:+PrintGCDetails设置gc日志的格式 -Dsun.rmi.dgc.server.gcInterval=600000 -Dsun.rmi.dgc.client.gcInterval=600000指定rmi调用时gc的时间间隔 -XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=15 采用并发gc方式，经过15次minor gc 后进入年老代 如何分析GC日志摘录GC日志一部分 Young GC回收日志： 12016-07-05T10:43:18.093+0800: 25.395: [GC [PSYoungGen: 274931K-&gt;10738K(274944K)] 371093K-&gt;147186K(450048K), 0.0668480 secs] [Times: user=0.17 sys=0.08, real=0.07 secs] Full GC回收日志： 12016-07-05T10:43:18.160+0800: 25.462: [Full GC [PSYoungGen: 10738K-&gt;0K(274944K)] [ParOldGen: 136447K-&gt;140379K(302592K)] 147186K-&gt;140379K(577536K) [PSPermGen: 85411K-&gt;85376K(171008K)], 0.6763541 secs] [Times: user=1.75 sys=0.02, real=0.68 secs] 通过上面日志分析得出，PSYoungGen、ParOldGen、PSPermGen属于Parallel收集器。其中PSYoungGen表示gc回收前后年轻代的内存变化；ParOldGen表示gc回收前后老年代的内存变化；PSPermGen表示gc回收前后永久区的内存变化。young gc 主要是针对年轻代进行内存回收比较频繁，耗时短；full gc 会对整个堆内存进行回城，耗时长，因此一般尽量减少full gc的次数。 通过两张图非常明显看出gc日志构成： Young GC日志： Full GC日志： GC分析工具GChistoGChisto是一款专业分析gc日志的工具，可以通过gc日志来分析：Minor GC、full gc的时间、频率等等，通过列表、报表、图表等不同的形式来反应gc的情况。虽然界面略显粗糙，但是功能还是不错的。 配置好本地的jdk环境之后，双击GChisto.jar，在弹出的输入框中点击 add ，选择gc.log日志 GC Pause Stats：可以查看GC 的次数、GC的时间、GC的开销、最大GC时间和最小GC时间等，以及相应的柱状图。 GC Pause Distribution：查看GC停顿的详细分布，x轴表示垃圾收集停顿时间，y轴表示是停顿次数。 GC Timeline：显示整个时间线上的垃圾收集。 不过这款工具已经不再维护。 GC Easy这是一个web工具，在线使用非常方便。 地址: http://gceasy.io 进入官网，将打包好的zip或者gz为后缀的压缩包上传，过一会就会拿到分析结果。 推荐使用此工具进行gc分析。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(八)-jvm知识点总览]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E5%85%AB)-jvm%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E8%A7%88%2F</url>
    <content type="text"><![CDATA[在江湖中要练就绝世武功必须内外兼备，精妙的招式和深厚的内功，武功的基础是内功。对于武功低（就像江南七怪）的人，招式更重要，因为他们不能靠内功直接去伤人，只能靠招式，利刃上优势来取胜了，但是练到高手之后，内功就更主要了。一个内功低的人招式再奇妙也打不过一个内功高的人。比如，你剑法再厉害，一剑刺过来，别人一掌打断你的剑，你还怎么使剑法，你一掌打到一个武功高的人身上，那人没什么事，却把你震伤了，你还怎么打。同样两者也是相辅相成的，内功深厚之后，原来普通的一招一式威力也会倍增。 对于搞开发的我们其实也是一样，现在流行的框架越来越多，封装的也越来越完善，各种框架可以搞定一切，几乎不用关注底层的实现，初级程序员只要熟悉基本的使用方法，便可以快速的开发上线；但对于高级程序员来讲，内功的修炼却越发的重要，比如算法、设计模式、底层原理等，只有把这些基础熟练之后，才能在开发过程中知其然知其所以然，出现问题时能快速定位到问题的本质。 对于Java程序员来讲，spring全家桶几乎可以搞定一切，spring全家桶便是精妙的招式，jvm就是内功心法很重要的一块，线上出现性能问题，jvm调优更是不可回避的问题。因此JVM基础知识对于高级程序员的重要性不必言语。本篇文章会根据之前写的jvm系列文章梳理出jvm需要关注的所有考察点。 jvm 总体梳理jvm体系总体分四大块： 类的加载机制 jvm内存结构 GC算法 垃圾回收 GC分析 命令调优 当然这些知识点在之前的文章中都有详细的介绍，这里只做主干的梳理 这里画了一个思维导图，将所有的知识点进行了陈列，因为图比较大可以点击右键下载了放大查看。 类的加载机制主要关注点： 什么是类的加载 类的生命周期 类加载器 双亲委派模型 什么是类的加载 类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 类的生命周期 类的生命周期包括这几个部分，加载、连接、初始化、使用和卸载，其中前三部是类的加载的过程,如下图； 加载：查找并加载类的二进制数据，在Java堆中也创建一个java.lang.Class类的对象 连接：连接又包含三块内容：验证、准备、解析。1）验证：文件格式、元数据、字节码、符号引用验证；2）准备：为类的静态变量分配内存，并将其初始化为默认值；3）解析：把类中的符号引用转换为直接引用 初始化：为类的静态变量赋予正确的初始值 使用：new出对象程序中使用 卸载：执行垃圾回收 几个小问题？1、JVM初始化步骤 ？ 2、类初始化时机 ？3、哪几种情况下，Java虚拟机将结束生命周期？答案参考这篇文章jvm系列(一):java类的加载机制 类加载器 启动类加载器：Bootstrap ClassLoader，负责加载存放在JDK\jre\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库。 扩展类加载器：Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\jre\lib\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。 应用程序类加载器：Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器。 类加载机制 全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 jvm内存结构主要关注点： jvm内存结构都是什么 对象分配规则 jvm内存结构 方法区和堆是所有线程共享的内存区域；而java栈、本地方法栈和程序计数器是运行是线程私有的内存区域。 Java堆（Heap）,是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 方法区（Method Area），方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。 程序计数器（Program Counter Register），程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。 JVM栈（JVM Stacks），与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 本地方法栈（Native Method Stacks），本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。 对象分配规则 对象优先分配在Eden区，如果Eden区没有足够的空间时，虚拟机执行一次Minor GC。 大对象直接进入老年代（大对象是指需要大量连续内存空间的对象）。这样做的目的是避免在Eden区和两个Survivor区之间发生大量的内存拷贝（新生代采用复制算法收集内存）。 长期存活的对象进入老年代。虚拟机为每个对象定义了一个年龄计数器，如果对象经过了1次Minor GC那么对象会进入Survivor区，之后每经过一次Minor GC那么对象的年龄加1，知道达到阀值对象进入老年区。 动态判断对象的年龄。如果Survivor区中相同年龄的所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象可以直接进入老年代。 空间分配担保。每次进行Minor GC时，JVM会计算Survivor区移至老年区的对象的平均大小，如果这个值大于老年区的剩余值大小则进行一次Full GC，如果小于检查HandlePromotionFailure设置，如果true则只进行Monitor GC,如果false则进行Full GC。 如何通过参数来控制个各个内存区域参考此文章：jvm系列(二):JVM内存结构 GC算法 垃圾回收主要关注点： 对象存活判断 GC算法 垃圾回收器 对象存活判断 判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的，不可达对象。 GC算法 GC最基础的算法有三种：标记 -清除算法、复制算法、标记-压缩算法，我们常用的垃圾回收器一般都采用分代收集算法。 标记 -清除算法，“标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。 复制算法，“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 标记-压缩算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存。 分代收集算法，“分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。 垃圾回收器 Serial收集器，串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。 ParNew收集器，ParNew收集器其实就是Serial收集器的多线程版本。 Parallel收集器，Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。 Parallel Old 收集器，Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。 CMS收集器，CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。 G1收集器，G1 (Garbage-First)是一款面向服务器的垃圾收集器，主要针对配备多颗处理器及大容量内存的机器，以极高概率满足GC停顿时间要求的同时，还具备高吞吐量性能特征。 GC算法和垃圾回收器算法图解以及更详细内容参考 jvm系列(三):GC算法 垃圾收集器 GC分析 命令调优主要关注点： GC日志分析 调优命令 调优工具 GC日志分析 摘录GC日志一部分（前部分为年轻代gc回收；后部分为full gc回收）： 122016-07-05T10:43:18.093+0800: 25.395: [GC [PSYoungGen: 274931K-&gt;10738K(274944K)] 371093K-&gt;147186K(450048K), 0.0668480 secs] [Times: user=0.17 sys=0.08, real=0.07 secs] 2016-07-05T10:43:18.160+0800: 25.462: [Full GC [PSYoungGen: 10738K-&gt;0K(274944K)] [ParOldGen: 136447K-&gt;140379K(302592K)] 147186K-&gt;140379K(577536K) [PSPermGen: 85411K-&gt;85376K(171008K)], 0.6763541 secs] [Times: user=1.75 sys=0.02, real=0.68 secs] 通过上面日志分析得出，PSYoungGen、ParOldGen、PSPermGen属于Parallel收集器。其中PSYoungGen表示gc回收前后年轻代的内存变化；ParOldGen表示gc回收前后老年代的内存变化；PSPermGen表示gc回收前后永久区的内存变化。young gc 主要是针对年轻代进行内存回收比较频繁，耗时短；full gc 会对整个堆内存进行回城，耗时长，因此一般尽量减少full gc的次数。 Young GC日志： Full GC日志： 调优命令 Sun JDK监控和故障处理命令有jps、jstat、jmap、jhat、jstack、jinfo jps，JVM Process Status Tool，显示指定系统内所有的HotSpot虚拟机进程。 jstat，JVM statistics Monitoring是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 jmap，JVM Memory Map命令用于生成heap dump文件。 jhat，JVM Heap Analysis Tool命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看。 jstack，用于生成java虚拟机当前时刻的线程快照。 jinfo，JVM Configuration info 这个命令作用是实时查看和调整虚拟机运行参数。 详细的命令使用参考这里jvm系列(四):jvm调优-命令篇 调优工具 常用调优工具分为两类,jdk自带监控工具：jconsole和jvisualvm，第三方有：MAT(Memory Analyzer Tool)、GChisto。 jconsole，Java Monitoring and Management Console是从java5开始，在JDK中自带的java监控和管理控制台，用于对JVM中内存，线程和类等的监控。 jvisualvm，jdk自带全能工具，可以分析内存快照、线程快照；监控内存变化、GC变化等。 MAT，Memory Analyzer Tool，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗。 GChisto，一款专业分析gc日志的工具。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(六)-Java服务GC参数调优案例]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E5%85%AD)-Java%E6%9C%8D%E5%8A%A1GC%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[本文介绍了一次生产环境的JVM GC相关参数的调优过程，通过参数的调整避免了GC卡顿对JAVA服务成功率的影响。 这段时间在整理jvm系列的文章，无意中发现本文，作者思路清晰通过步步分析最终解决问题。我个人特别喜欢这种实战类的内容，经原作者的授权同意，将文章分享于此。原文链接：Java服务GC参数调优案例，下面为转载此文的内容，备注部分为本人添加，主要起到说明的作用。 背景以及遇到的问题我们的Java HTTP服务属于OLTP类型，对成功率和响应时间的要求比较高，在生产环境中出现偶现的成功率突然下降然后又自动恢复的情况，如图所示： JVM和GC相关的参数如下： 123456-Xmx22528m-Xms22528m-XX:NewRatio=2-XX:+UseConcMarkSweepGC-XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled 总结来说，由于服务中大量使用了Cache，所以堆大小开到了22G。GC算法使用CMS（UseConcMarkSweepGC），开启了降低标记停顿（CMSParallelRemarkEnabled），设置年轻代为并行收集（UseParNewGC），年轻代和老年代的比例为1:2 （NewRatio＝2）。 JVM GC日志相关的参数如下： 123456789-Xloggc:/data/gc.log-XX:GCLogFileSize=10M-XX:NumberOfGCLogFiles=10-XX:+UseGCLogFileRotation-XX:+PrintGCDateStamps-XX:+PrintGCTimeStamps-XX:+PrintGCDetails-XX:+DisableExplicitGC-verbose:gc 问题解决过程排除应用程序的内存使用问题首先使用jmap查看内存使用情况： 1jmap -histo:live PID 这个命令把程序中当前的对象按照个数和占用的空间排序以后打印出来。这里没有发现使用异常的对象。 排除Cache内容过多的问题如果Cache内容过多也会导致JVM老年代容易被用满导致频繁GC，因此调出GC日志进行查看，发现每次GC以后内存使用一般是从20G降低到5G左右，因此常驻内存的Cache不是导致GC长时间卡顿的根本原因。对于GC LOG的查看有多种方式，使用VisualVM比较直观，需要使用VisualGC： 从图中我们可以看到Eden区和老年代的空间分配，由于整体内存是20G，设置-XX:NewRatio=2。因此老年代是14G，Eden区＋S0+S1=7G。 调整GC时间点（成功率抖动问题加重）如果GC需要处理的内存量比较大，执行的时间也就比较长，STW （Stop the World）时间也就更长。按照这个思路调整CMS启动的时间点，希望提早GC，也就是让GC变得更加频繁但是期望每次执行的时间较少。添加了下面这两个参数： 12-XX:+UseCMSInitiatingOccupancyOnly-XX:CMSInitiatingOccupancyFraction=50 意思是说在Old区使用了50%的时候触发GC。实验后发现GC的频率有所增加，但是每次GC造成的成功率降低现象并没有减弱，因此弃用这两个参数。 调整对象在年轻代内存中驻留的时间（效果不明显）如果能够降低老年代GC的频率也可以达到降低GC影响的目的，因此尝试让对象在年轻代内存中进行更长时间的驻留，提升这些对象在年轻代GC时候被销毁的概率。使用参数-XX:MaxTenuringThreshold=31调整以后收效不明显。 备注：1、MaxTenuringThreshold 在1.5.0_05之前最大值可以设置为31，1.5.0_06以后最大值可以设置为15，超过15会被认为无限大。参考：Never set GC parameter -XX:MaxTenuringThreshold greater than 152、提升年轻代GC被销毁的概率，只是调整这个参数效果不大，第二次age的值会重新计算，参考：说说MaxTenuringThreshold这个参数 CMS-Remark之前强制进行年轻代的GC首先补充一下CMS的相关知识，在CMS整个过程中有两个步骤是STW的，如图红色部分： CMS并非没有暂停，而是用两次短暂停来替代串行标记整理算法的长暂停，它的收集周期是这样： 1、初始标记(CMS-initial-mark)，从root对象开始标记存活的对象 2、并发标记(CMS-concurrent-mark) 3、重新标记(CMS-remark)，暂停所有应用程序线程，重新标记并发标记阶段遗漏的对象（在并发标记阶段结束后对象状态的更新导致） 4、并发清除(CMS-concurrent-sweep) 5、并发重设状态等待下次CMS的触发(CMS-concurrent-reset) 通过GC日志和成功率下降的时间点进行比对发现并不是每一次老年代GC都会导致成功率的下降，但是从中发现了一个规律： 前两次GC CMS-Remark过程在4s左右造成了成功率的下降，但是第三次GC并没有对成功率造成明显的影响，CMS-Remark只有0.18s。Java HTTP 服务是通过Nginx进行反向代理的，nginx设置的超时时间是3s，所以如果GC卡顿在3s以内就不会对成功率造成太大的影响。 从GC日志中又发现一个信息： 在文档和相关资料中没有找到蓝色部分的含义，猜测是remark处理的内存量，处理的越多就越慢。添加下面两个参数强制在remark阶段和FULL GC阶段之前先在进行一次年轻代的GC，这样需要进行处理的内存量就不会太多。 备注：1、蓝色部分的含义：remark标记需要清理对象的容量。关于如何分析CMS日志，可以参考这篇文章：了解 CMS 垃圾回收日志2、FULL GC阶段之前先在进行一次年轻代的GC的意义是：Young区对象引用了Old区的对象，如果在Old区进行清理之前不进行Young区清理，就会导致Old区被young区引用的对象无法释放。可以参考这篇文章：假笨说-又抓了一个导致频繁GC的鬼–数组动态扩容 12-XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark 调优以后效果很明显，下面是两台配置完全相同的服务器在同一时间段的成功率和响应时间监控图，第一个没有添加强制年轻代GC的参数。 结论1、在CMS-remark阶段需要对堆中所有的内存对象进行处理，如果在这个阶段之前强制执行一次年轻代的GC会大量减少remark需要处理的内存数量，进而降低JVM卡顿对成功率的影响。2、对于Java HTTP服务，JVM的卡顿时间应该小于HTTP客户端的调用超时时间，否则JVM卡顿会对成功率造成影响。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(四)-jvm调优-命令篇]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2FJVM%E7%B3%BB%E5%88%97(%E5%9B%9B)-jvm%E8%B0%83%E4%BC%98-%E5%91%BD%E4%BB%A4%E7%AF%87%2F</url>
    <content type="text"><![CDATA[运用jvm自带的命令可以方便的在生产监控和打印堆栈的日志信息帮忙我们来定位问题！虽然jvm调优成熟的工具已经有很多：jconsole、大名鼎鼎的VisualVM，IBM的Memory Analyzer等等，但是在生产环境出现问题的时候，一方面工具的使用会有所限制，另一方面喜欢装X的我们，总喜欢在出现问题的时候在终端输入一些命令来解决。所有的工具几乎都是依赖于jdk的接口和底层的这些命令，研究这些命令的使用也让我们更能了解jvm构成和特性。 Sun JDK监控和故障处理命令有jps、jstat、jmap、jhat、jstack、jinfo，下面做一一介绍。 jpsJVM Process Status Tool，显示指定系统内所有的HotSpot虚拟机进程。 命令格式1jps [options] [hostid] option参数 -l : 输出主类全名或jar路径 -q : 只输出LVMID -m : 输出JVM启动时传递给main()的参数 -v : 输出JVM启动时显示指定的JVM参数 其中[option]、[hostid]参数也可以不写。 示例1234$ jps -l -m 28920 org.apache.catalina.startup.Bootstrap start 11589 org.apache.catalina.startup.Bootstrap start 25816 sun.tools.jps.Jps -l -m jstatjstat(JVM statistics Monitoring)是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 命令格式1jstat [option] LVMID [interval] [count] 参数 [option] : 操作参数 LVMID : 本地虚拟机进程ID [interval] : 连续输出的时间间隔 [count] : 连续输出的次数 option 参数总览 Option Displays… class class loader的行为统计。Statistics on the behavior of the class loader. compiler HotSpt JIT编译器行为统计。Statistics of the behavior of the HotSpot Just-in-Time compiler. gc 垃圾回收堆的行为统计。Statistics of the behavior of the garbage collected heap. gccapacity 各个垃圾回收代容量(young,old,perm)和他们相应的空间统计。Statistics of the capacities of the generations and their corresponding spaces. gcutil 垃圾回收统计概述。Summary of garbage collection statistics. gccause 垃圾收集统计概述（同-gcutil），附加最近两次垃圾回收事件的原因。Summary of garbage collection statistics (same as -gcutil), with the cause of the last and gcnew 新生代行为统计。Statistics of the behavior of the new generation. gcnewcapacity 新生代与其相应的内存空间的统计。Statistics of the sizes of the new generations and its corresponding spaces. gcold 年老代和永生代行为统计。Statistics of the behavior of the old and permanent generations. gcoldcapacity 年老代行为统计。Statistics of the sizes of the old generation. gcpermcapacity 永生代行为统计。Statistics of the sizes of the permanent generation. printcompilation HotSpot编译方法统计。HotSpot compilation method statistics. option 参数详解-class监视类装载、卸载数量、总空间以及耗费的时间 123$ jstat -class 11589 Loaded Bytes Unloaded Bytes Time 7035 14506.3 0 0.0 3.67 Loaded : 加载class的数量 Bytes : class字节大小 Unloaded : 未加载class的数量 Bytes : 未加载class的字节大小 Time : 加载时间 -compiler输出JIT编译过的方法数量耗时等 123$ jstat -compiler 1262Compiled Failed Invalid Time FailedType FailedMethod 2573 1 0 47.60 1 org/apache/catalina/loader/WebappClassLoader findResourceInternal Compiled : 编译数量 Failed : 编译失败数量 Invalid : 无效数量 Time : 编译耗时 FailedType : 失败类型 FailedMethod : 失败方法的全限定名 -gc垃圾回收堆的行为统计 123$ jstat -gc 1262 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 26112.0 24064.0 6562.5 0.0 564224.0 76274.5 434176.0 388518.3 524288.0 42724.7 320 6.417 1 0.398 6.815 C即Capacity 总容量，U即Used 已使用的容量 S0C : survivor0区的总容量 S1C : survivor1区的总容量 S0U : survivor0区已使用的容量 S1U : survivor1区已使用的容量 EC : Eden区的总容量 EU : Eden区已使用的容量 OC : Old区的总容量 OU : Old区已使用的容量 PC : 当前perm的容量 (KB) PU : perm的使用 (KB) YGC : 新生代垃圾回收次数 YGCT : 新生代垃圾回收时间 FGC : 老年代垃圾回收次数 FGCT : 老年代垃圾回收时间 GCT : 垃圾回收总消耗时间 1$ jstat -gc 1262 2000 20 这个命令意思就是每隔2000ms输出1262的gc情况，一共输出20次 -gccapacity同-gc，不过还会输出Java堆各区域使用到的最大、最小空间 123$ jstat -gccapacity 1262 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC PGCMN PGCMX PGC PC YGC FGC 614400.0 614400.0 614400.0 26112.0 24064.0 564224.0 434176.0 434176.0 434176.0 434176.0 524288.0 1048576.0 524288.0 524288.0 320 1 NGCMN : 新生代占用的最小空间 NGCMX : 新生代占用的最大空间 OGCMN : 老年代占用的最小空间 OGCMX : 老年代占用的最大空间 OGC：当前年老代的容量 (KB) OC：当前年老代的空间 (KB) PGCMN : perm占用的最小空间 PGCMX : perm占用的最大空间 -gcutil同-gc，不过输出的是已使用空间占总空间的百分比 123$ jstat -gcutil 28920 S0 S1 E O P YGC YGCT FGC FGCT GCT 12.45 0.00 33.85 0.00 4.44 4 0.242 0 0.000 0.242 -gccause垃圾收集统计概述（同-gcutil），附加最近两次垃圾回收事件的原因 123$ jstat -gccause 28920 S0 S1 E O P YGC YGCT FGC FGCT GCT LGCC GCC 12.45 0.00 33.85 0.00 4.44 4 0.242 0 0.000 0.242 Allocation Failure No GC LGCC：最近垃圾回收的原因 GCC：当前垃圾回收的原因 -gcnew统计新生代的行为 123$ jstat -gcnew 28920 S0C S1C S0U S1U TT MTT DSS EC EU YGC YGCT 419392.0 419392.0 52231.8 0.0 6 6 209696.0 3355520.0 1172246.0 4 0.242 TT：Tenuring threshold(提升阈值) MTT：最大的tenuring threshold DSS：survivor区域大小 (KB) -gcnewcapacity新生代与其相应的内存空间的统计 123$ jstat -gcnewcapacity 28920 NGCMN NGCMX NGC S0CMX S0C S1CMX S1C ECMX EC YGC FGC 4194304.0 4194304.0 4194304.0 419392.0 419392.0 419392.0 419392.0 3355520.0 3355520.0 4 0 NGC:当前年轻代的容量 (KB) S0CMX:最大的S0空间 (KB) S0C:当前S0空间 (KB) ECMX:最大eden空间 (KB) EC:当前eden空间 (KB) -gcold统计旧生代的行为 123$ jstat -gcold 28920 PC PU OC OU YGC FGC FGCT GCT 1048576.0 46561.7 6291456.0 0.0 4 0 0.000 0.242 -gcoldcapacity统计旧生代的大小和空间 123$ jstat -gcoldcapacity 28920 OGCMN OGCMX OGC OC YGC FGC FGCT GCT 6291456.0 6291456.0 6291456.0 6291456.0 4 0 0.000 0.242 -gcpermcapacity永生代行为统计 123$ jstat -gcpermcapacity 28920 PGCMN PGCMX PGC PC YGC FGC FGCT GCT 1048576.0 2097152.0 1048576.0 1048576.0 4 0 0.000 0.242 -printcompilationhotspot编译方法统计 123$ jstat -printcompilation 28920 Compiled Size Type Method 1291 78 1 java/util/ArrayList indexOf Compiled：被执行的编译任务的数量 Size：方法字节码的字节数 Type：编译类型 Method：编译方法的类名和方法名。类名使用”/” 代替 “.” 作为空间分隔符. 方法名是给出类的方法名. 格式是一致于HotSpot - XX:+PrintComplation 选项 jmapjmap(JVM Memory Map)命令用于生成heap dump文件，如果不使用这个命令，还可以使用-XX:+HeapDumpOnOutOfMemoryError参数来让虚拟机出现OOM的时候自动生成dump文件。 jmap不仅能生成dump文件，还可以查询finalize执行队列、Java堆和永久代的详细信息，如当前使用率、当前使用的是哪种收集器等。 命令格式1jmap [option] LVMID option参数 dump : 生成堆转储快照 finalizerinfo : 显示在F-Queue队列等待Finalizer线程执行finalizer方法的对象 heap : 显示Java堆详细信息 histo : 显示堆中对象的统计信息 permstat : to print permanent generation statistics F : 当-dump没有响应时，强制生成dump快照 示例-dump常用格式 1-dump::live,format=b,file=&lt;filename&gt; pid dump堆到文件，format指定输出格式，live指明是活着的对象，file指定文件名 123$ jmap -dump:live,format=b,file=dump.hprof 28920 Dumping heap to /home/xxx/dump.hprof ... Heap dump file created dump.hprof这个后缀是为了后续可以直接用MAT(Memory Anlysis Tool)打开。 -finalizerinfo打印等待回收对象的信息 123456$ jmap -finalizerinfo 28920 Attaching to process ID 28920, please wait... Debugger attached successfully. Server compiler detected. JVM version is 24.71-b01 Number of objects pending for finalization: 0 可以看到当前F-QUEUE队列中并没有等待Finalizer线程执行finalizer方法的对象。 -heap打印heap的概要信息，GC使用的算法，heap的配置及wise heap的使用情况，可以用此来判断内存目前的使用情况以及垃圾回收情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ jmap -heap 28920 Attaching to process ID 28920, please wait... Debugger attached successfully. Server compiler detected. JVM version is 24.71-b01 using thread-local object allocation. Parallel GC with 4 thread(s)//GC 方式 Heap Configuration: //堆内存初始化配置 MinHeapFreeRatio = 0 //对应jvm启动参数-XX:MinHeapFreeRatio设置JVM堆最小空闲比率(default 40) MaxHeapFreeRatio = 100 //对应jvm启动参数 -XX:MaxHeapFreeRatio设置JVM堆最大空闲比率(default 70) MaxHeapSize = 2082471936 (1986.0MB) //对应jvm启动参数-XX:MaxHeapSize=设置JVM堆的最大大小 NewSize = 1310720 (1.25MB)//对应jvm启动参数-XX:NewSize=设置JVM堆的‘新生代’的默认大小 MaxNewSize = 17592186044415 MB//对应jvm启动参数-XX:MaxNewSize=设置JVM堆的‘新生代’的最大大小 OldSize = 5439488 (5.1875MB)//对应jvm启动参数-XX:OldSize=&lt;value&gt;:设置JVM堆的‘老生代’的大小 NewRatio = 2 //对应jvm启动参数-XX:NewRatio=:‘新生代’和‘老生代’的大小比率 SurvivorRatio = 8 //对应jvm启动参数-XX:SurvivorRatio=设置年轻代中Eden区与Survivor区的大小比值 PermSize = 21757952 (20.75MB) //对应jvm启动参数-XX:PermSize=&lt;value&gt;:设置JVM堆的‘永生代’的初始大小 MaxPermSize = 85983232 (82.0MB)//对应jvm启动参数-XX:MaxPermSize=&lt;value&gt;:设置JVM堆的‘永生代’的最大大小 G1HeapRegionSize = 0 (0.0MB) Heap Usage://堆内存使用情况 PS Young Generation Eden Space://Eden区内存分布 capacity = 33030144 (31.5MB)//Eden区总容量 used = 1524040 (1.4534378051757812MB) //Eden区已使用 free = 31506104 (30.04656219482422MB) //Eden区剩余容量 4.614088270399305% used //Eden区使用比率 From Space: //其中一个Survivor区的内存分布 capacity = 5242880 (5.0MB) used = 0 (0.0MB) free = 5242880 (5.0MB) 0.0% used To Space: //另一个Survivor区的内存分布 capacity = 5242880 (5.0MB) used = 0 (0.0MB) free = 5242880 (5.0MB) 0.0% used PS Old Generation //当前的Old区内存分布 capacity = 86507520 (82.5MB) used = 0 (0.0MB) free = 86507520 (82.5MB) 0.0% used PS Perm Generation//当前的 “永生代” 内存分布 capacity = 22020096 (21.0MB) used = 2496528 (2.3808746337890625MB) free = 19523568 (18.619125366210938MB) 11.337498256138392% used 670 interned Strings occupying 43720 bytes. 可以很清楚的看到Java堆中各个区域目前的情况。 -histo打印堆的对象统计，包括对象数、内存大小等等 （因为在dump:live前会进行full gc，如果带上live则只统计活对象，因此不加live的堆大小要大于加live堆的大小 ） 1234567891011121314$ jmap -histo:live 28920 | more num #instances #bytes class name---------------------------------------------- 1: 83613 12012248 &lt;constMethodKlass&gt; 2: 23868 11450280 [B 3: 83613 10716064 &lt;methodKlass&gt; 4: 76287 10412128 [C 5: 8227 9021176 &lt;constantPoolKlass&gt; 6: 8227 5830256 &lt;instanceKlassKlass&gt; 7: 7031 5156480 &lt;constantPoolCacheKlass&gt; 8: 73627 1767048 java.lang.String 9: 2260 1348848 &lt;methodDataKlass&gt; 10: 8856 849296 java.lang.Class .... 仅仅打印了前10行 class name是对象类型，说明如下： 123456789B byteC charD doubleF floatI intJ longZ boolean[ 数组，如[I表示int[][L+类名 其他对象 -permstat打印Java堆内存的永久保存区域的类加载器的智能统计信息。对于每个类加载器而言，它的名称、活跃度、地址、父类加载器、它所加载的类的数量和大小都会被打印。此外，包含的字符串数量和大小也会被打印。 123456789101112131415$ jmap -permstat 28920 Attaching to process ID 28920, please wait... Debugger attached successfully. Server compiler detected. JVM version is 24.71-b01 finding class loader instances ..done. computing per loader stat ..done. please wait.. computing liveness.liveness analysis may be inaccurate ... class_loader classes bytes parent_loader alive? type &lt;bootstrap&gt; 3111 18154296 null live &lt;internal&gt; 0x0000000600905cf8 1 1888 0x0000000600087f08 dead sun/reflect/DelegatingClassLoader@0x00000007800500a0 0x00000006008fcb48 1 1888 0x0000000600087f08 dead sun/reflect/DelegatingClassLoader@0x00000007800500a0 0x00000006016db798 0 0 0x00000006008d3fc0 dead java/util/ResourceBundle$RBClassLoader@0x0000000780626ec0 0x00000006008d6810 1 3056 null dead sun/reflect/DelegatingClassLoader@0x00000007800500a0 -F强制模式。如果指定的pid没有响应，请使用jmap -dump或jmap -histo选项。此模式下，不支持live子选项。 jhatjhat(JVM Heap Analysis Tool)命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看。在此要注意，一般不会直接在服务器上进行分析，因为jhat是一个耗时并且耗费硬件资源的过程，一般把服务器生成的dump文件复制到本地或其他机器上进行分析。 命令格式1jhat [dumpfile] 参数 -stack false|true 关闭对象分配调用栈跟踪(tracking object allocation call stack)。 如果分配位置信息在堆转储中不可用，则必须将此标志设置为 false。默认值为 true。 -refs false|true 关闭对象引用跟踪(tracking of references to objects)。 默认值为 true。默认情况下，返回的指针是指向其他特定对象的对象，如反向链接或输入引用(referrers or incoming references)，会统计/计算堆中的所有对象。 -port port-number 设置 jhat HTTP server 的端口号。默认值 7000。 -exclude exclude-file 指定对象查询时需要排除的数据成员列表文件(a file that lists data members that should be excluded from the reachable objects query)。 例如，如果文件列列出了 java.lang.String.value，那么当从某个特定对象 Object o 计算可达的对象列表时，引用路径涉及 java.lang.String.value 的都会被排除。 -baseline exclude-file 指定一个基准堆转储(baseline heap dump)。 在两个 heap dumps 中有相同 object ID 的对象会被标记为不是新的(marked as not being new)，其他对象被标记为新的(new)。在比较两个不同的堆转储时很有用。 -debug int 设置 debug 级别。0 表示不输出调试信息。 值越大则表示输出更详细的 debug 信息。 -version 启动后只显示版本信息就退出。 -J&lt; flag &gt; 因为 jhat 命令实际上会启动一个JVM来执行，通过 -J 可以在启动JVM时传入一些启动参数。例如，-J-Xmx512m 则指定运行 jhat 的Java虚拟机使用的最大堆内存为 512 MB。如果需要使用多个JVM启动参数,则传入多个 -Jxxxxxx。 示例12345678910$ jhat -J-Xmx512m dump.hprof eading from dump.hprof... Dump file created Fri Mar 11 17:13:42 CST 2016 Snapshot read, resolving... Resolving 271678 objects... Chasing references, expect 54 dots...................................................... Eliminating duplicate references...................................................... Snapshot resolved. Started HTTP server on port 7000 Server is ready. 中间的-J-Xmx512m是在dump快照很大的情况下分配512M内存去启动HTTP服务器，运行完之后就可在浏览器打开http://localhost:7000进行快照分析。堆快照分析主要在最后面的Heap Histogram里，里面根据class列出了dump的时候所有存活对象。 分析同样一个dump快照，MAT需要的额外内存比jhat要小的多的多，所以建议使用MAT来进行分析，当然也看个人偏好。 分析打开浏览器http://localhost:7000，该页面提供了几个查询功能可供使用： 1234567All classes including platformShow all members of the rootsetShow instance counts for all classes (including platform)Show instance counts for all classes (excluding platform)Show heap histogramShow finalizer summaryExecute Object Query Language (OQL) query 一般查看堆异常情况主要看这个两个部分： Show instance counts for all classes (excluding platform)，平台外的所有对象信息。如下图： Show heap histogram 以树状图形式展示堆情况。如下图： 具体排查时需要结合代码，观察是否大量应该被回收的对象在一直被引用或者是否有占用内存特别大的对象无法被回收。一般情况，会下载到客户端用工具来分析 jstackjstack用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息，如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 命令格式1jstack [option] LVMID option参数 -F : 当正常输出请求不被响应时，强制输出线程堆栈 -l : 除堆栈外，显示关于锁的附加信息 -m : 如果调用到本地方法的话，可以显示C/C++的堆栈 示例12345678910111213141516171819202122232425262728$ jstack -l 11494|more2016-07-28 13:40:04Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.71-b01 mixed mode):&quot;Attach Listener&quot; daemon prio=10 tid=0x00007febb0002000 nid=0x6b6f waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE Locked ownable synchronizers: - None&quot;http-bio-8005-exec-2&quot; daemon prio=10 tid=0x00007feb94028000 nid=0x7b8c waiting on condition [0x00007fea8f56e000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000000cae09b80&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:104) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:32) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745) Locked ownable synchronizers: - None ..... 分析这里有一篇文章解释的很好 分析打印出的文件内容 jinfojinfo(JVM Configuration info)这个命令作用是实时查看和调整虚拟机运行参数。 之前的jps -v口令只能查看到显示指定的参数，如果想要查看未被显示指定的参数的值就要使用jinfo命令。 命令格式1jinfo [option] [args] LVMID option参数 -flag : 输出指定args参数的值 -flags : 不需要args参数，输出所有JVM参数的值 -sysprops : 输出系统属性，等同于System.getProperties() 示例12$ jinfo -flag 11494-XX:CMSInitiatingOccupancyFraction=80]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[内存泄漏-内存溢出]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2F%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F-%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%2F</url>
    <content type="text"><![CDATA[一、概念与区别内存溢出（out of memory）是指程序在申请内存时，没有足够的内存空间供其使用，出现out of memory；比如申请了一个integer，但给它存了long才能存下的数，那就是内存溢出。内存泄露（memory leak）是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存，迟早会被占光。 memory leak会最终会导致out of memory！ 内存溢出就是你要求分配的内存超出了系统能给你的，系统不能满足需求，于是产生溢出。 二、内存泄露内存泄漏是指你向系统申请分配内存进行使用(new)，可是使用完了以后却不归还(delete)，结果你申请到的那块内存你自己也不能再访问（也许你把它的地址给弄丢了），而系统也不能再次将它分配给需要的程序。一个盘子用尽各种方法只能装4个果子，你装了5个，结果掉倒地上不能吃了。这就是溢出！比方说栈，栈满时再做进栈必定产生空间溢出，叫上溢；栈空时再做退栈也产生空间溢出，称为下溢。就是分配的内存不足以放下数据项序列,称为内存溢出。 以发生的方式来分类，内存泄漏可以分为4类： 常发性内存泄漏：发生内存泄漏的代码会被多次执行到，每次被执行的时候都会导致一块内存泄漏。 偶发性内存泄漏：发生内存泄漏的代码只有在某些特定环境或操作过程下才会发生。常发性和偶发性是相对的。对于特定的环境，偶发性的也许就变成了常发性的。所以测试环境和测试方法对检测内存泄漏至关重要。 一次性内存泄漏：发生内存泄漏的代码只会被执行一次，或者由于算法上的缺陷，导致总会有一块仅且一块内存发生泄漏。比如，在类的构造函数中分配内存，在析构函数中却没有释放该内存，所以内存泄漏只会发生一次。 隐式内存泄漏：程序在运行过程中不停的分配内存，但是直到结束的时候才释放内存。严格的说这里并没有发生内存泄漏，因为最终程序释放了所有申请的内存。但是对于一个服务器程序，需要运行几天，几周甚至几个月，不及时释放内存也可能导致最终耗尽系统的所有内存。所以，我们称这类内存泄漏为隐式内存泄漏。 从用户使用程序的角度来看，内存泄漏本身不会产生什么危害，作为一般的用户，根本感觉不到内存泄漏的存在。 真正有危害的是内存泄漏的堆积，这会最终消耗尽系统所有的内存。从这个角度来说，一次性内存泄漏并没有什么危害，因为它不会堆积，而隐式内存泄漏危害性则非常大，因为较之于常发性和偶发性内存泄漏它更难被检测到。 三、内存溢出内存溢出的原因以及解决方法引起内存溢出的原因有很多种，小编列举一下常见的有以下几种： 内存中加载的数据量过于庞大，如一次从数据库取出过多数据； 集合类中有对对象的引用，使用完后未清空，使得JVM不能回收； 代码中存在死循环或循环产生过多重复的对象实体； 使用的第三方软件中的BUG； 启动参数内存值设定的过小； 内存溢出的解决方案：第一步，修改JVM启动参数，直接增加内存。(-Xms，-Xmx参数一定不要忘记加。)第二步，检查错误日志，查看“OutOfMemory”错误前是否有其它异常或错误。第三步，对代码进行走查和分析，找出可能发生内存溢出的位置。重点排查以下几点： 检查对数据库查询中，是否有一次获得全部数据的查询。一般来说，如果一次取十万条记录到内存，就可能引起内存溢出。这个问题比较隐蔽，在上线前，数据库中数据较少，不容易出问题，上线后，数据库中数据多了，一次查询就有可能引起内存溢出。因此对于数据库查询尽量采用分页的方式查询。 检查代码中是否有死循环或递归调用。 检查是否有大循环重复产生新对象实体。 检查对数据库查询中，是否有一次获得全部数据的查询。一般来说，如果一次取十万条记录到内存，就可能引起内存溢出。这个问题比较隐蔽，在上线前，数据库中数据较少，不容易出问题，上线后，数据库中数据多了，一次查询就有可能引起内存溢出。因此对于数据库查询尽量采用分页的方式查询。 检查List、Map等集合对象是否有使用完后，未清除的问题。List、Map等集合对象会始终存有对对象的引用，使得这些对象不能被GC回收。 第四步，使用内存查看工具动态查看内存使用情况。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JVM系列(十)-JVM演讲PPT分享]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2Fjvm%E7%B3%BB%E5%88%97(%E5%8D%81)-JVM%E6%BC%94%E8%AE%B2PPT%E5%88%86%E4%BA%AB%2F</url>
    <content type="text"></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深入理解Java虚拟机]]></title>
    <url>%2F2019%2F04%2F19%2FJVM%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[第2章 Java内存区域与内存溢出异常2.2 运行时数据区域2.2.1 程序计数器程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方法去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。 2.2.2 Java虚拟机栈2.2.3 本地方法栈2.2.4 Java堆2.2.5 方法区2.2.6 运行时常量2.2.7 直接内存]]></content>
      <categories>
        <category>JVM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka安装与使用]]></title>
    <url>%2F2019%2F04%2F19%2FKafka%2FKafka%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文假设你是第一次使用Kafka，并且机器上不存在Kafka和ZooKeeper数据。Kafka的命令脚本在Linux和Windows平台上是不一样的，Linux的脚本存放在bin目录下，以.sh为扩展名，而Windows的脚本存放在bin/windows目录下，以.bat为扩展名。 1. 下载安装包下载2.1.0版本安装包并解压。12&gt; tar -xzf kafka_2.11-2.1.0.tgz&gt; cd kafka_2.11-2.1.0 2. 启动服务因为Kafka使用了ZooKeeper，所以在启动Kafka服务之前首先要启动ZooKeeper服务。Kafka安装包自带了ZooKeeper服务，但是只能作为单机使用，在生产环境还是需要搭建ZooKeeper集群。启动ZooKeeper服务：123&gt; bin/zookeeper-server-start.sh config/zookeeper.properties[2013-04-22 15:01:37,495] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)... 然后启动Kafka服务：1234&gt; bin/kafka-server-start.sh config/server.properties[2013-04-22 15:01:47,028] INFO Verifying properties (kafka.utils.VerifiableProperties)[2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)... 3. 创建topic创建一个叫test的topic，只有一个分区和副本。1&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 运行如下命令可以查看该topic：12&gt; bin/kafka-topics.sh --list --zookeeper localhost:2181test 除了手动创建topic，也可以通过配置broker，当发布的topic不存在时，自动创建topic。 4. 发布消息Kafka自带命令行客户端，可以通过文件输入或标准输入发布消息到Kafka集群。默认每一行是一个单独的消息。 运行producer，在控制台输入消息并发送。123&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testThis is a messageThis is another message 5. 启动consumerKafka还有一个命令行consumer把接收到的消息输出。123&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginningThis is a messageThis is another message 如果以上的命令在不同的终端运行，你可以在producer终端输入消息，他们会在不同的consumer终端显示。所有的命令行工具都有附加选项：运行不带参数的命令会显示该命令的详细使用文档。 6. 集群安装以上是单个broker的安装使用。对Kafka来说，单个broker就是只有一个节点的集群，和启动多个broker实例没什么区别。下面我们把集群的节点数扩展为3个（在同一台机器上）。 首先，为每个broker拷贝配置文件。12&gt; cp config/server.properties config/server-1.properties&gt; cp config/server.properties config/server-2.properties 修改相关配置：123456789config/server-1.properties: broker.id=1 listeners=PLAINTEXT://:9093 log.dirs=/tmp/kafka-logs-1 config/server-2.properties: broker.id=2 listeners=PLAINTEXT://:9094 log.dirs=/tmp/kafka-logs-2 在集群里每个节点broker.id属性是唯一的。因为在一台机器上运行三个节点，所以必须修改端口和日志目录，防止所有的节点注册到一个端口或覆盖彼此的数据。单机ZooKeeper已经启动，所以只需启动两个新节点：1234&gt; bin/kafka-server-start.sh config/server-1.properties &amp;...&gt; bin/kafka-server-start.sh config/server-2.properties &amp;... 创建一个新topic，设置replication-factor为3：1&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic 现在我们有了3个节点的集群，但是怎么知道每个节点在干什么？可以使用describe topics命令查看：123&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 1 Replicas: 1,2,0 Isr: 1,2,0 下面对输出内容作下说明：第一行是所有分区的汇总，后面每一行是一个分区的信息。因为我们该topic只有一个分区，所以只有一行。 Leader是负责分区所有读写的节点。分区内随机选择一个节点作为leader。 Replicas是节点列表，不管它们是不是leader或者是否活跃的它们都会复制分区日志。 Isr是in-sync副本集。它是replicas的子集，只有活跃的且能被leader捕获的才会在这个子集。 注意：上面例子中，节点1是该topic下分区的leader。 我们可以在原来的topic上运行相同的命令查看它的位置：123&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic testTopic:test PartitionCount:1 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 可以看到，原来的topic没有副本，在server 0上，这也是集群中唯一的server。 往新topic上发布信息：12345&gt; bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic...my test message 1my test message 2^C 订阅这些消息：12345&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic...my test message 1my test message 2^C 现在测试下容错。Broker 1是leader，kill掉它：123&gt; ps aux | grep server-1.properties7564 ttys002 0:15.91 /System/Library/Frameworks/JavaVM.framework/Versions/1.8/Home/bin/java...&gt; kill -9 7564 Windows上使用如下命令：1234&gt; wmic process where &quot;caption = &apos;java.exe&apos; and commandline like &apos;%server-1.properties%&apos;&quot; get processidProcessId6016&gt; taskkill /pid 6016 /f leader切换到slave中的某一个，节点1不再是in-sync副本集：123&gt; bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topicTopic:my-replicated-topic PartitionCount:1 ReplicationFactor:3 Configs: Topic: my-replicated-topic Partition: 0 Leader: 2 Replicas: 1,2,0 Isr: 2,0 但是尽管原来写消息的leader挂掉了，消息仍能被订阅。12345&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic...my test message 1my test message 2^C 7. 使用Kafka Connect导入/导出数据从console写入数据和向console写入数据虽然方便，但是可能会有从其他源文件导入或导出数据到其他系统的需求。对于很多系统来说，除了编程的方式，也可以使用Kafka Connect导入/导出数据。Kafka Connect是Kafka自带的工具，用来导入导出数据。它是运行connectors的扩展工具，通过特殊的逻辑实现和外部系统的交互。下面介绍如何使用Kafka Connect导入/导出数据。 首先，准备测试数据：1&gt; echo -e &quot;foo\nbar&quot; &gt; test.txt Windows下可以使用如下命令：12&gt; echo foo&gt; test.txt&gt; echo bar&gt;&gt; test.txt 然后，以standalone模式启动两个connectors。standalone模式就是他们以单独、本地、专一的进程运行。我们提供了三个配置文件作为参数。第一个是Kafka Connect进程的配置，包含通用配置比如Kafka broker1&gt; bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Kafka简单入门]]></title>
    <url>%2F2019%2F04%2F19%2FKafka%2FKafka%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1. kafka介绍1.1. 主要功能根据官网的介绍，ApacheKafka®是一个分布式流媒体平台，它主要有3种功能： 1：It lets you publish and subscribe to streams of records.发布和订阅消息流，这个功能类似于消息队列，这也是kafka归类为消息队列框架的原因 2：It lets you store streams of records in a fault-tolerant way.以容错的方式记录消息流，kafka以文件的方式来存储消息流 3：It lets you process streams of records as they occur.可以再消息发布的时候进行处理 1.2. 使用场景1：Building real-time streaming data pipelines that reliably get data between systems or applications.在系统或应用程序之间构建可靠的用于传输实时数据的管道，消息队列功能 2：Building real-time streaming applications that transform or react to the streams of data。构建实时的流数据处理程序来变换或处理数据流，数据处理功能 1.3. 详细介绍Kafka目前主要作为一个分布式的发布订阅式的消息系统使用，下面简单介绍一下kafka的基本机制 1.3.1 消息传输流程 Producer即生产者，向Kafka集群发送消息，在发送消息之前，会对消息进行分类，即Topic，上图展示了两个producer发送了分类为topic1的消息，另外一个发送了topic2的消息。 Topic即主题，通过对消息指定主题可以将消息分类，消费者可以只关注自己需要的Topic中的消息 Consumer即消费者，消费者通过与kafka集群建立长连接的方式，不断地从集群中拉取消息，然后可以对这些消息进行处理。 从上图中就可以看出同一个Topic下的消费者和生产者的数量并不是对应的。 1.3.2 kafka**服务器消息存储策略** 谈到kafka的存储，就不得不提到分区，即partitions，创建一个topic时，同时可以指定分区数目，分区数越多，其吞吐量也越大，但是需要的资源也越多，同时也会导致更高的不可用性，kafka在接收到生产者发送的消息之后，会根据均衡策略将消息存储到不同的分区中。 在每个分区中，消息以顺序存储，最晚接收的的消息会最后被消费。 1.3.3 与生产者的交互 生产者在向kafka集群发送消息的时候，可以通过指定分区来发送到指定的分区中 也可以通过指定均衡策略来将消息发送到不同的分区中 如果不指定，就会采用默认的随机均衡策略，将消息随机的存储到不同的分区中 1.3.4 与消费者的交互 在消费者消费消息时，kafka使用offset来记录当前消费的位置 在kafka的设计中，可以有多个不同的group来同时消费同一个topic下的消息，如图，我们有两个不同的group同时消费，他们的的消费的记录位置offset各不相同，不互相干扰。 对于一个group而言，消费者的数量不应该多余分区的数量，因为在一个group中，每个分区至多只能绑定到一个消费者上，即一个消费者可以消费多个分区，一个分区只能给一个消费者消费 因此，若一个group中的消费者数量大于分区数量的话，多余的消费者将不会收到任何消息。 2. Kafka安装与使用2.1. 下载 你可以在kafka官网 http://kafka.apache.org/downloads下载到最新的kafka安装包，选择下载二进制版本的tgz文件，根据网络状态可能需要fq，这里我们选择的版本是0.11.0.1，目前的最新版 2.2. 安装 Kafka是使用scala编写的运行与jvm虚拟机上的程序，虽然也可以在windows上使用，但是kafka基本上是运行在linux服务器上，因此我们这里也使用linux来开始今天的实战。 首先确保你的机器上安装了jdk，kafka需要java运行环境，以前的kafka还需要zookeeper，新版的kafka已经内置了一个zookeeper环境，所以我们可以直接使用 说是安装，如果只需要进行最简单的尝试的话我们只需要解压到任意目录即可，这里我们将kafka压缩包解压到/home目录 2.3. 配置 在kafka解压目录下下有一个config的文件夹，里面放置的是我们的配置文件 consumer.properites 消费者配置，这个配置文件用于配置于2.5节中开启的消费者，此处我们使用默认的即可 producer.properties 生产者配置，这个配置文件用于配置于2.5节中开启的生产者，此处我们使用默认的即可 server.properties kafka服务器的配置，此配置文件用来配置kafka服务器，目前仅介绍几个最基础的配置 broker.id 申明当前kafka服务器在集群中的唯一ID，需配置为integer,并且集群中的每一个kafka服务器的id都应是唯一的，我们这里采用默认配置即可 listeners 申明此kafka服务器需要监听的端口号，如果是在本机上跑虚拟机运行可以不用配置本项，默认会使用localhost的地址，如果是在远程服务器上运行则必须配置，例如： listeners=PLAINTEXT:// 192.168.180.128:9092。并确保服务器的9092端口能够访问 3.zookeeper.connect 申明kafka所连接的zookeeper的地址 ，需配置为zookeeper的地址，由于本次使用的是kafka高版本中自带zookeeper，使用默认配置即可 zookeeper.connect=localhost:2181 2.4. 运行 启动zookeeper cd进入kafka解压目录，输入 bin/zookeeper-server-start.sh config/zookeeper.properties 启动zookeeper成功后会看到如下的输出 2.启动kafka cd进入kafka解压目录，输入 bin/kafka-server-start.sh config/server.properties 启动kafka成功后会看到如下的输出 2.5. 创建第一个消息 2.5.1 创建一个topic Kafka通过topic对同一类的数据进行管理，同一类的数据使用同一个topic可以在处理数据时更加的便捷 在kafka解压目录打开终端，输入 bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic `test` 创建一个名为test的topic 在创建topic后可以通过输入 `bin/kafka-topics.sh --list --zookeeper localhost:2181` 来查看已经创建的topic 2.4.2 `创建一个消息消费者` 在kafka解压目录打开终端，输入 bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic `test–from-beginning` 可以创建一个用于消费topic为test的消费者 消费者创建完成之后，因为还没有发送任何数据，因此这里在执行后没有打印出任何数据 不过别着急，不要关闭这个终端，打开一个新的终端，接下来我们创建第一个消息生产者 2.4.3 创建一个消息生产者 在kafka解压目录打开一个新的终端，输入 bin/kafka-console-producer.sh --broker-list localhost:9092 --topic `test` 在执行完毕后会进入的编辑器页面 ![image](http://upload-images.jianshu.io/upload_images/292448-7e555fcc4a48cd01.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 在发送完消息之后，可以回到我们的消息消费者终端中，可以看到，终端中已经打印出了我们刚才发送的消息 3. 在java程序中使用kafka 跟上节中一样，我们现在在java程序中尝试使用kafka 3.1 创建Topic ; “复制代码”) public static void main(String[] args) { //创建topic Properties props = new Properties(); props.put("bootstrap.servers", "192.168.180.128:9092"); AdminClient adminClient = AdminClient.create(props); ArrayList topics = new ArrayList(); NewTopic newTopic = new NewTopic("topic-test", 1, (short) 1); topics.add(newTopic); CreateTopicsResult result = adminClient.createTopics(topics); try { result.all().get(); } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } } ; “复制代码”) 使用AdminClient API可以来控制对kafka服务器进行配置，我们这里使用 NewTopic(String name, int numPartitions, short replicationFactor) 的构造方法来创建了一个名为“topic-test”，分区数为1，复制因子为1的Topic. 3.2 Producer生产者发送消息 ; “复制代码”) public static void main(String[] args){ Properties props = new Properties(); props.put("bootstrap.servers", "192.168.180.128:9092"); props.put("acks", "all"); props.put("retries", 0); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer producer = new KafkaProducer(props); for (int i = 0; i < 100; i++) producer.send(new ProducerRecord("topic-test", Integer.toString(i), Integer.toString(i))); producer.close(); } ; “复制代码”) 使用producer发送完消息可以通过2.5中提到的服务器端消费者监听到消息。也可以使用接下来介绍的java消费者程序来消费消息 3.3 Consumer消费者消费消息 ; “复制代码”) public static void main(String[] args){ Properties props = new Properties(); props.put("bootstrap.servers", "192.168.12.65:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); final KafkaConsumer consumer = new KafkaConsumer(props); consumer.subscribe(Arrays.asList("topic-test"),new ConsumerRebalanceListener() { public void onPartitionsRevoked(Collection collection) { } public void onPartitionsAssigned(Collection collection) { //将偏移设置到最开始 consumer.seekToBeginning(collection); } }); while (true) { ConsumerRecords records = consumer.poll(100); for (ConsumerRecord record : records) System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); } } ; “复制代码”) 这里我们使用Consume API 来创建了一个普通的java消费者程序来监听名为“topic-test”的Topic，每当有生产者向kafka服务器发送消息，我们的消费者就能收到发送的消息。 4. 使用spring-kafkaSpring-kafka是正处于孵化阶段的一个spring子项目，能够使用spring的特性来让我们更方便的使用kafka 4.1 基本配置信息 与其他spring的项目一样，总是离不开配置，这里我们使用java配置来配置我们的kafka消费者和生产者。 引入pom文件 ; “复制代码”) org.apache.kafka kafka-clients 0.11.0.1 org.apache.kafka kafka-streams 0.11.0.1 org.springframework.kafka spring-kafka 1.3.0.RELEASE ; “复制代码”) 创建配置类 我们在主目录下新建名为KafkaConfig的类 @Configuration @EnableKafka public class KafkaConfig { } 配置Topic 在kafkaConfig类中添加配置 ; “复制代码”) @Configuration @EnableKafka public class KafkaConfig { //topic config Topic的配置开始 @Bean public KafkaAdmin admin() { Map configs = new HashMap(); configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,"192.168.180.128:9092"); return new KafkaAdmin(configs); } @Bean public NewTopic topic1() { return new NewTopic("foo", 10, (short) 2); } //topic的配置结束 } ; “复制代码”) 配置生产者Factory及Template 在上面的配置文件中追加 ; “复制代码”) //producer config start @Bean public ProducerFactory producerFactory() { return new DefaultKafkaProducerFactory(producerConfigs()); } @Bean public Map producerConfigs() { Map props = new HashMap(); props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "192.168.180.128:9092"); props.put("acks", "all"); props.put("retries", 0); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", "org.apache.kafka.common.serialization.IntegerSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); return props; } @Bean public KafkaTemplate kafkaTemplate() { return new KafkaTemplate(producerFactory()); } //producer config end ; “复制代码”) 5.配置ConsumerFactory 在上面的配置文件中追加 ; “复制代码”) //consumer config start @Bean public ConcurrentKafkaListenerContainerFactory kafkaListenerContainerFactory(){ ConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory(); factory.setConsumerFactory(consumerFactory()); return factory; } @Bean public ConsumerFactory consumerFactory(){ return new DefaultKafkaConsumerFactory(consumerConfigs()); } @Bean public Map consumerConfigs(){ HashMap props = new HashMap(); props.put("bootstrap.servers", "192.168.180.128:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); return props; } //consumer config end ; “复制代码”) 4.2 创建消息生产者 ; “复制代码”) //使用spring-kafka的template发送一条消息 发送多条消息只需要循环多次即可 public static void main(String[] args) throws ExecutionException, InterruptedException { AnnotationConfigApplicationContext ctx = new AnnotationConfigApplicationContext(KafkaConfig.class); KafkaTemplate kafkaTemplate = (KafkaTemplate) ctx.getBean("kafkaTemplate"); String data="this is a test message"; ListenableFuture send = kafkaTemplate.send("topic-test", 1, data); send.addCallback(new ListenableFutureCallback() { public void onFailure(Throwable throwable) { } public void onSuccess(SendResult integerStringSendResult) { } }); }]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL explain详解]]></title>
    <url>%2F2019%2F04%2F19%2FMySQL%2FMySQL%20explain%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Explain简介本文主要讲述如何通过 explain 命令获取 select 语句的执行计划，通过 explain 我们可以知道以下信息：表的读取顺序，数据读取操作的类型，哪些索引可以使用，哪些索引实际使用了，表之间的引用，每张表有多少行被优化器查询等信息。 本文中使用的表和数据：12345678910111213141516171819202122232425262728293031323334353637383940DROP TABLE IF EXISTS `actor`;CREATE TABLE `actor` ( `id` int(11) NOT NULL, `name` varchar(45) DEFAULT NULL, `update_time` datetime DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `actor` (`id`, `name`, `update_time`) VALUES (1,'a','2017-12-22 15:27:18'), (2,'b','2017-12-22 15:27:18'), (3,'c','2017-12-22 15:27:18');DROP TABLE IF EXISTS `film`;CREATE TABLE `film` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `film` (`id`, `name`) VALUES (3,'film0'),(1,'film1'),(2,'film2');DROP TABLE IF EXISTS `film_actor`;CREATE TABLE `film_actor` ( `id` int(11) NOT NULL, `film_id` int(11) NOT NULL, `actor_id` int(11) NOT NULL, PRIMARY KEY (`id`), KEY `idx_film_actor_id` (`film_id`,`actor_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `film_actor` (`id`, `film_id`, `actor_id`) VALUES (1,1,1),(2,1,2),(3,2,1);DROP TABLE IF EXISTS `role`;CREATE TABLE `role` ( `id` int(11) NOT NULL, `tenant_id` int(11) NOT NULL, PRIMARY KEY (`id`), KEY `idx_tenant_id` (`tenant_id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;INSERT INTO `role` (`id`, `tenant_id`) VALUES (11011, 8888); 下面是使用 explain 的例子： 在 select 语句之前增加 explain 关键字，MySQL 会在查询上设置一个标记，执行查询时，会返回执行计划的信息，而不是执行这条SQL（如果 from 中包含子查询，仍会执行该子查询，将结果放入临时表中）。 123456mysql&gt; explain select * from actor;+----+-------------+-------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | actor | ALL | NULL | NULL | NULL | NULL | 2 | NULL |+----+-------------+-------+------+---------------+------+---------+------+------+-------+ 在查询中的每个表会输出一行，如果有两个表通过 join 连接查询，那么会输出两行。表的意义相当广泛：可以是子查询、一个 union 结果等。 explain 有两个变种： 1）explain extended：会在 explain 的基础上额外提供一些查询优化的信息。紧随其后通过 show warnings 命令可以得到优化后的查询语句，从而看出优化器优化了什么。额外还有 filtered 列，是一个百分比的值，rows * filtered/100 可以估算出将要和 explain 中前一个表进行连接的行数（前一个表指 explain 中的id值比当前表id值小的表）。 12345678910111213mysql&gt; explain extended select * from film where id = 1;+----+-------------+-------+-------+---------------+---------+---------+-------+------+----------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+----------+-------+| 1 | SIMPLE | film | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+----------+-------+mysql&gt; show warnings;+-------+------+--------------------------------------------------------------------------------+| Level | Code | Message |+-------+------+--------------------------------------------------------------------------------+| Note | 1003 | /* select#1 */ select '1' AS `id`,'film1' AS `name` from `test`.`film` where 1 |+-------+------+--------------------------------------------------------------------------------+ 2）explain partitions：相比 explain 多了个 partitions 字段，如果查询是基于分区表的话，会显示查询将访问的分区。 explain 中的列接下来我们将展示 explain 中每个列的信息。 1. id列id列的编号是 select 的序列号，有几个 select 就有几个id，并且id的顺序是按 select 出现的顺序增长的。MySQL将 select 查询分为简单查询和复杂查询。复杂查询分为三类：简单子查询、派生表（from语句中的子查询）、union 查询。 1）简单子查询 1234567mysql&gt; explain select (select 1 from actor limit 1) from film;+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| 1 | PRIMARY | film | index | NULL | idx_name | 32 | NULL | 1 | Using index || 2 | SUBQUERY | actor | index | NULL | PRIMARY | 4 | NULL | 2 | Using index |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+ 2）from子句中的子查询 1234567mysql&gt; explain select id from (select id from film) as der;+----+-------------+------------+-------+---------------+----------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+---------------+----------+---------+------+------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 2 | NULL || 2 | DERIVED | film | index | NULL | idx_name | 32 | NULL | 1 | Using index |+----+-------------+------------+-------+---------------+----------+---------+------+------+-------------+ 这个查询执行时有个临时表别名为der，外部 select 查询引用了这个临时表。 3）union查询 12345678mysql&gt; explain select 1 union all select 1;+----+--------------+------------+------+---------------+------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------+------------+------+---------------+------+---------+------+------+-----------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || 2 | UNION | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------+------------+------+---------------+------+---------+------+------+-----------------+ union结果总是放在一个匿名临时表中，临时表不在SQL中出现，因此它的id是NULL。 2. select_type列select_type 表示对应行是简单还是复杂的查询，如果是复杂的查询，又是上述三种复杂查询中的哪一种。 1）simple：简单查询。查询不包含子查询和union 123456mysql&gt; explain select * from film where id = 2;+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+| 1 | SIMPLE | film | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL |+----+-------------+-------+-------+---------------+---------+---------+-------+------+-------+ 2）primary：复杂查询中最外层的 select 3）subquery：包含在 select 中的子查询（不在 from 子句中） 4）derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义） 用这个例子来了解 primary、subquery 和 derived 类型 12345678mysql&gt; explain select (select 1 from actor where id = 1) from (select * from film where id = 1) der;+----+-------------+------------+--------+---------------+---------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------+---------+---------+-------+------+-------------+| 1 | PRIMARY | &lt;derived3&gt; | system | NULL | NULL | NULL | NULL | 1 | NULL || 3 | DERIVED | film | const | PRIMARY | PRIMARY | 4 | const | 1 | NULL || 2 | SUBQUERY | actor | const | PRIMARY | PRIMARY | 4 | const | 1 | Using index |+----+-------------+------------+--------+---------------+---------+---------+-------+------+-------------+ 5）union：在 union 中的第二个和随后的 select 6）union result：从 union 临时表检索结果的 select 用这个例子来了解 union 和 union result 类型： 12345678mysql&gt; explain select 1 union all select 1;+----+--------------+------------+------+---------------+------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+--------------+------------+------+---------------+------+---------+------+------+-----------------+| 1 | PRIMARY | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || 2 | UNION | NULL | NULL | NULL | NULL | NULL | NULL | NULL | No tables used || NULL | UNION RESULT | &lt;union1,2&gt; | ALL | NULL | NULL | NULL | NULL | NULL | Using temporary |+----+--------------+------------+------+---------------+------+---------+------+------+-----------------+ 3. table列这一列表示 explain 的一行正在访问哪个表。 当 from 子句中有子查询时，table列是 格式，表示当前查询依赖 id=N 的查询，于是先执行 id=N 的查询。当有 union 时，UNION RESULT 的 table 列的值为 &lt;union1,2&gt;，1和2表示参与 union 的 select 行id。 4. type列这一列表示关联类型或访问类型，即MySQL决定如何查找表中的行。 依次从最优到最差分别为：system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL NULL：mysql能够在优化阶段分解查询语句，在执行阶段用不着再访问表或索引。例如：在索引列中选取最小值，可以单独查找索引来完成，不需要在执行时访问表 123456mysql&gt; explain select min(id) from film;+----+-------------+-------+------+---------------+------+---------+------+------+------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+------------------------------+| 1 | SIMPLE | NULL | NULL | NULL | NULL | NULL | NULL | NULL | Select tables optimized away |+----+-------------+-------+------+---------------+------+---------+------+------+------------------------------+ const, system：mysql能对查询的某部分进行优化并将其转化成一个常量（可以看show warnings 的结果）。用于 primary key 或 unique key 的所有列与常数比较时，所以表最多有一个匹配行，读取1次，速度比较快。 1234567891011121314mysql&gt; explain extended select * from (select * from film where id = 1) tmp;+----+-------------+------------+--------+---------------+---------+---------+-------+------+----------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------+--------+---------------+---------+---------+-------+------+----------+-------+| 1 | PRIMARY | &lt;derived2&gt; | system | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL || 2 | DERIVED | film | const | PRIMARY | PRIMARY | 4 | const | 1 | 100.00 | NULL |+----+-------------+------------+--------+---------------+---------+---------+-------+------+----------+-------+mysql&gt; show warnings;+-------+------+---------------------------------------------------------------+| Level | Code | Message |+-------+------+---------------------------------------------------------------+| Note | 1003 | /* select#1 */ select '1' AS `id`,'film1' AS `name` from dual |+-------+------+---------------------------------------------------------------+ eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。 1234567mysql&gt; explain select * from film_actor left join film on film_actor.film_id = film.id;+----+-------------+------------+--------+---------------+-------------------+---------+-------------------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------+-------------------+---------+-------------------------+------+-------------+| 1 | SIMPLE | film_actor | index | NULL | idx_film_actor_id | 8 | NULL | 3 | Using index || 1 | SIMPLE | film | eq_ref | PRIMARY | PRIMARY | 4 | test.film_actor.film_id | 1 | NULL |+----+-------------+------------+--------+---------------+-------------------+---------+-------------------------+------+-------------+ ref：相比 eq_ref，不使用唯一索引，而是使用普通索引或者唯一性索引的部分前缀，索引要和某个值相比较，可能会找到多个符合条件的行。 12345678910111213141516-- 1. 简单 select 查询，name是普通索引（非唯一索引）mysql&gt; explain select * from film where name = "film1";+----+-------------+-------+------+---------------+----------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+----------+---------+-------+------+--------------------------+| 1 | SIMPLE | film | ref | idx_name | idx_name | 33 | const | 1 | Using where; Using index |+----+-------------+-------+------+---------------+----------+---------+-------+------+--------------------------+-- 2.关联表查询，idx_film_actor_id是film_id和actor_id的联合索引，这里使用到了film_actor的左边前缀film_id部分。mysql&gt; explain select * from film left join film_actor on film.id = film_actor.film_id;+----+-------------+------------+-------+-------------------+-------------------+---------+--------------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+-------------------+-------------------+---------+--------------+------+-------------+| 1 | SIMPLE | film | index | NULL | idx_name | 33 | NULL | 3 | Using index || 1 | SIMPLE | film_actor | ref | idx_film_actor_id | idx_film_actor_id | 4 | test.film.id | 1 | Using index |+----+-------------+------------+-------+-------------------+-------------------+---------+--------------+------+-------------+ ref_or_null：类似ref，但是可以搜索值为NULL的行。 123456mysql&gt; explain select * from film where name = "film1" or name is null;+----+-------------+-------+-------------+---------------+----------+---------+-------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------------+---------------+----------+---------+-------+------+--------------------------+| 1 | SIMPLE | film | ref_or_null | idx_name | idx_name | 33 | const | 2 | Using where; Using index |+----+-------------+-------+-------------+---------------+----------+---------+-------+------+--------------------------+ index_merge：表示使用了索引合并的优化方法。例如下表：id是主键，tenant_id是普通索引。or 的时候没有用 primary key，而是使用了 primary key(id) 和 tenant_id 索引。 123456mysql&gt; explain select * from role where id = 11011 or tenant_id = 8888;+----+-------------+-------+-------------+-----------------------+-----------------------+---------+------+------+-------------------------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------------+-----------------------+-----------------------+---------+------+------+-------------------------------------------------+| 1 | SIMPLE | role | index_merge | PRIMARY,idx_tenant_id | PRIMARY,idx_tenant_id | 4,4 | NULL | 134 | Using union(PRIMARY,idx_tenant_id); Using where |+----+-------------+-------+-------------+-----------------------+-----------------------+---------+------+------+-------------------------------------------------+ range：范围扫描通常出现在 in(), between, &gt;, &lt;, &gt;= 等操作中。使用一个索引来检索给定范围的行。 123456mysql&gt; explain select * from actor where id &gt; 1;+----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | actor | range | PRIMARY | PRIMARY | 4 | NULL | 2 | Using where |+----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ index：和ALL一样，不同就是mysql只需扫描索引树，这通常比ALL快一些。 123456mysql&gt; explain select count(*) from film;+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| 1 | SIMPLE | film | index | NULL | idx_name | 33 | NULL | 3 | Using index |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+ ALL：即全表扫描，意味着mysql需要从头到尾去查找所需要的行。通常情况下这需要增加索引来进行优化了 123456mysql&gt; explain select * from actor;+----+-------------+-------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | actor | ALL | NULL | NULL | NULL | NULL | 2 | NULL |+----+-------------+-------+------+---------------+------+---------+------+------+-------+ 5. possible_keys列这一列显示查询可能使用哪些索引来查找。 explain 时可能出现 possible_keys 有列，而 key 显示 NULL 的情况，这种情况是因为表中数据不多，mysql认为索引对此查询帮助不大，选择了全表查询。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查 where 子句看是否可以创造一个适当的索引来提高查询性能，然后用 explain 查看效果。 6. key列这一列显示mysql实际采用哪个索引来优化对该表的访问。 如果没有使用索引，则该列是 NULL。如果想强制mysql使用或忽视possible_keys列中的索引，在查询中使用 force index、ignore index。 7. key_len列这一列显示了mysql在索引里使用的字节数，通过这个值可以算出具体使用了索引中的哪些列。 举例来说，film_actor的联合索引 idx_film_actor_id 由 film_id 和 actor_id 两个int列组成，并且每个int是4字节。通过结果中的key_len=4可推断出查询使用了第一个列：film_id列来执行索引查找。 123456mysql&gt; explain select * from film_actor where film_id = 2;+----+-------------+------------+------+-------------------+-------------------+---------+-------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+-------------------+-------------------+---------+-------+------+-------------+| 1 | SIMPLE | film_actor | ref | idx_film_actor_id | idx_film_actor_id | 4 | const | 1 | Using index |+----+-------------+------------+------+-------------------+-------------------+---------+-------+------+-------------+ key_len计算规则如下： 字符串 char(n)：n字节长度 varchar(n)：2字节存储字符串长度，如果是utf-8，则长度 3n + 2 数值类型 tinyint：1字节 smallint：2字节 int：4字节 bigint：8字节 时间类型 date：3字节 timestamp：4字节 datetime：8字节 如果字段允许为 NULL，需要1字节记录是否为 NULL 索引最大长度是768字节，当字符串过长时，mysql会做一个类似左前缀索引的处理，将前半部分的字符提取出来做索引。 8. ref列这一列显示了在key列记录的索引中，表查找值所用到的列或常量，常见的有：const（常量），func，NULL，字段名（例：film.id）。 9. rows列这一列是mysql估计要读取并检测的行数，注意这个不是结果集里的行数。 10. Extra列这一列展示的是额外信息。常见的重要值如下： distinct: 一旦mysql找到了与行相联合匹配的行，就不再搜索了。 1234567mysql&gt; explain select distinct name from film left join film_actor on film.id = film_actor.film_id;+----+-------------+------------+-------+-------------------+-------------------+---------+--------------+------+------------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+-------+-------------------+-------------------+---------+--------------+------+------------------------------+| 1 | SIMPLE | film | index | idx_name | idx_name | 33 | NULL | 3 | Using index; Using temporary || 1 | SIMPLE | film_actor | ref | idx_film_actor_id | idx_film_actor_id | 4 | test.film.id | 1 | Using index; Distinct |+----+-------------+------------+-------+-------------------+-------------------+---------+--------------+------+------------------------------+ Using index：这发生在对表的请求列都是同一索引的部分的时候，返回的列数据只使用了索引中的信息，而没有再去访问表中的行记录。是性能高的表现。 123456mysql&gt; explain select id from film order by id;+----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+| 1 | SIMPLE | film | index | NULL | PRIMARY | 4 | NULL | 3 | Using index |+----+-------------+-------+-------+---------------+---------+---------+------+------+-------------+ Using where：mysql服务器将在存储引擎检索行后再进行过滤。就是先读取整行数据，再按 where 条件进行检查，符合就留下，不符合就丢弃。 123456mysql&gt; explain select * from film where id &gt; 1;+----+-------------+-------+-------+---------------+----------+---------+------+------+--------------------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+----------+---------+------+------+--------------------------+| 1 | SIMPLE | film | index | PRIMARY | idx_name | 33 | NULL | 3 | Using where; Using index |+----+-------------+-------+-------+---------------+----------+---------+------+------+--------------------------+ Using temporary：mysql需要创建一张临时表来处理查询。出现这种情况一般是要进行优化的，首先是想到用索引来优化。 123456789101112131415-- 1. actor.name没有索引，此时创建了张临时表来distinctmysql&gt; explain select distinct name from actor;+----+-------------+-------+------+---------------+------+---------+------+------+-----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------+| 1 | SIMPLE | actor | ALL | NULL | NULL | NULL | NULL | 2 | Using temporary |+----+-------------+-------+------+---------------+------+---------+------+------+-----------------+-- 2. film.name建立了idx_name索引，此时查询时extra是using index,没有用临时表mysql&gt; explain select distinct name from film;+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| 1 | SIMPLE | film | index | idx_name | idx_name | 33 | NULL | 3 | Using index |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+ Using filesort：mysql 会对结果使用一个外部索引排序，而不是按索引次序从表里读取行。此时mysql会根据联接类型浏览所有符合条件的记录，并保存排序关键字和行指针，然后排序关键字并按顺序检索行信息。这种情况下一般也是要考虑使用索引来优化的。 123456789101112131415-- 1. actor.name未创建索引，会浏览actor整个表，保存排序关键字name和对应的id，然后排序name并检索行记录mysql&gt; explain select * from actor order by name;+----+-------------+-------+------+---------------+------+---------+------+------+----------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+------+---------------+------+---------+------+------+----------------+| 1 | SIMPLE | actor | ALL | NULL | NULL | NULL | NULL | 2 | Using filesort |+----+-------------+-------+------+---------------+------+---------+------+------+----------------+-- 2. film.name建立了idx_name索引，此时查询时extra是using indexmysql&gt; explain select * from film order by name;+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+| 1 | SIMPLE | film | index | NULL | idx_name | 33 | NULL | 3 | Using index |+----+-------------+-------+-------+---------------+----------+---------+------+------+-------------+ 参考 《高性能MySQL》: 附录D mysql官方文档-explain]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL使用CASE WHEN语法判断字段为NULL的用法]]></title>
    <url>%2F2019%2F04%2F19%2FMySQL%2FMySQL%E4%BD%BF%E7%94%A8CASE%20WHEN%E8%AF%AD%E6%B3%95%E5%88%A4%E6%96%AD%E5%AD%97%E6%AE%B5%E4%B8%BANULL%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"><![CDATA[在写sql语句时，遇到比较复杂的sql可能经常会用到CASE WHEN判断，CASE WHEN的基本语法在此不再赘述，网上有许多相关教程。 数据准备：123456789101112131415DROP TABLE IF EXISTS `t_user`;CREATE TABLE `t_user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(10) NOT NULL, `sex` smallint(1) NOT NULL, `email` varchar(40) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8;INSERT INTO t_user(name, sex, email) VALUES ('zong', '0', 'zong@163.com');INSERT INTO t_user(name, sex, email) VALUES ('liu', '1', null);INSERT INTO t_user(name, sex, email) VALUES ('ma', '1', null);INSERT INTO t_user(name, sex, email) VALUES ('wang', '0', 'wang@163.com');INSERT INTO t_user(name, sex, email) VALUES ('zhao', '0', null);INSERT INTO t_user(name, sex, email) VALUES ('li', '1', null); 简单使用：12SELECT name, CASE sex WHEN 0 THEN '男' WHEN 1 THEN '女' ELSE '' END FROM t_user;SELECT name, CASE sex WHEN 0 THEN '男' WHEN 1 THEN '女' ELSE '' END sex FROM t_user; 判断email是否为null：1234SELECT name, CASE WHEN email IS NULL THEN '' ELSE email END emailFROM t_user;]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL查询结果1变为true]]></title>
    <url>%2F2019%2F04%2F19%2FMySQL%2FMySQL%E6%9F%A5%E8%AF%A2%E7%BB%93%E6%9E%9C1%E5%8F%98%E4%B8%BAtrue%2F</url>
    <content type="text"><![CDATA[使用sql查询出某个字段的值为1，然后转换成Map&lt;String, Object&gt;，此时该字段值变为true，而不是原来的1。原因：数据库中该字段定义的类型为tinyint解决办法： 修改该字段的定义类型：使用非tinyint类型 修改sql语句，使用CONCAT函数处理该字段：CONCAT(fieldName, ‘’)。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MySQL排序让空值NULL排在数字后边]]></title>
    <url>%2F2019%2F04%2F19%2FMySQL%2Fmysql%E6%8E%92%E5%BA%8F%E8%AE%A9%E7%A9%BA%E5%80%BCNULL%E6%8E%92%E5%9C%A8%E6%95%B0%E5%AD%97%E5%90%8E%E8%BE%B9%2F</url>
    <content type="text"><![CDATA[从现实项目需求出发； 有一张城市表，里面有北京、上海、广州、河北、天津、河南6座城市。1234567891011mysql&gt; select * from bjy_order;+----+------+| id | city |+----+------+| 1 | 北京 || 2 | 上海 || 3 | 广州 || 4 | 河北 || 5 | 天津 || 6 | 河南 |+----+------+ 要求是让上海排第一个，天津排第二个； 最简单粗暴的方法就是添加一个order_number字段，用来标识顺序的；然后通过order by order_number asc排序。1234567891011mysql&gt; select * from bjy_order order by order_number asc;+----+------+--------------+| id | city | order_number |+----+------+--------------+| 2 | 上海 | 1 || 5 | 天津 | 2 || 1 | 北京 | 3 || 3 | 广州 | 4 || 4 | 河北 | 5 || 6 | 河南 | 6 |+----+------+--------------+ 这么做确实能满足需求。但是如果表里面有中国全部的32个省呢？再如果来个全国的县市表几百个数据呢？而我们只是想让某几个值排最前面就好了。就如人们大部分人只知道世界第一高峰是珠穆朗玛峰而不去关注第二第三一样。我们应该首先想到的就是只给需要排在前面的加上排序数字，其他为NULL。1234567891011mysql&gt; select * from bjy_order;+----+------+--------------+| id | city | order_number |+----+------+--------------+| 1 | 北京 | NULL || 2 | 上海 | 1 || 3 | 广州 | NULL || 4 | 河北 | NULL || 5 | 天津 | 2 || 6 | 河南 | NULL |+----+------+--------------+ 然后我们order by一下；1234567891011mysql&gt; select * from bjy_order order by order_number asc;+----+------+--------------+| id | city | order_number |+----+------+--------------+| 1 | 北京 | NULL || 3 | 广州 | NULL || 4 | 河北 | NULL || 6 | 河南 | NULL || 2 | 上海 | 1 || 5 | 天津 | 2 |+----+------+--------------+ 然而即将成功的时候让人沮丧的事情发生了，那些为NULL的排在在最前面。OK，下面有请今天的主角出场来解决这个问题。我们来利用is null把sql给稍微改造一下即可。1234567891011mysql&gt; select * from bjy_order order by order_number is null,order_number asc;+----+------+--------------+| id | city | order_number |+----+------+--------------+| 2 | 上海 | 1 || 5 | 天津 | 2 || 1 | 北京 | NULL || 3 | 广州 | NULL || 4 | 河北 | NULL || 6 | 河南 | NULL |+----+------+--------------+ 到此完美实现需求。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[常用SQL]]></title>
    <url>%2F2019%2F04%2F19%2FMySQL%2F%E5%B8%B8%E7%94%A8sql%2F</url>
    <content type="text"><![CDATA[统计n年之前的数据 year salary 2000 1000 2001 2000 2002 3000 2003 4000 year salary 2000 1000 2001 3000 2002 6000 2003 10000 1234SELECT year, (SELECT SUM(t2.salary) from t_year t2 where t1.`year` &gt;= t2.`year`) salary FROM `t_year` t1; 库存123456DROP TABLE IF EXISTS t_product;CREATE TABLE `t_product` ( `p_no` int(11) NOT NULL COMMENT '编号', `p_io` smallint(1) NOT NULL COMMENT '1：入仓；2：出仓', `amount` int(11) NOT NULL COMMENT '数量') ENGINE=InnoDB DEFAULT CHARSET=utf8; 1select p_no,IFNULL(sum(case when p_io=1 then amount end)-sum(case when p_io=2 then amount end),sum(case when p_io=1 then amount end)) from t_p group by p_no 12345SELECT a.p_no, IFNULL(a.c - b.c,a.c) from (select p_no, SUM(amount) c from t_p p where p.p_io = 1 GROUP BY p.p_no) aLEFT JOIN(select p_no, SUM(amount) c from t_p p where p.p_io = 2 GROUP BY p.p_no) bon a.p_no = b.p_no 123456SELECT SUM(CASE p_io WHEN 1 THEN quantity ELSE 0 END) - SUM(CASE p_io WHEN 2 THEN quantity ELSE 0 END)FROM t_productGROUP BY p_no; 删除重复数据]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库隔离级别]]></title>
    <url>%2F2019%2F04%2F19%2FMySQL%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%2F</url>
    <content type="text"><![CDATA[读未提交读提交重复读序列化 http://blog.csdn.net/fg2006/article/details/6937413http://www.cnblogs.com/xwdreamer/archive/2011/01/18/2297042.html 事务有四个属性：ACID（原子性、一致性、隔离性、持久性） 脏读不可重复读幻读 事务隔离级别：未提交读 （脏读）已提交读 （不可重复读） 默认可重复读 （幻读）序列化 http://www.cnblogs.com/dongguacai/p/7114885.htmlhttps://www.cnblogs.com/vigarbuaa/archive/2012/12/18/2824256.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据库SQL优化总结]]></title>
    <url>%2F2019%2F04%2F19%2FMySQL%2F%E6%95%B0%E6%8D%AE%E5%BA%93SQL%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num is null 最好不要给数据库留NULL，尽可能的使用NOT NULL填充数据库。备注、描述、评论之类的可以设置为NULL，其他的，最好不要使用NULL。不要以为NULL不需要空间，比如：char(100)型，在字段建立时，空间就固定了，不管是否插入值（NULL也包含在内），都是占用100个字符的空间的，如果是varchar这样的变长字段，null不占用空间。可以在num上设置默认值0，确保表中num列没有null值，然后这样查询：1select id from t where num = 0 应尽量避免在where子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 应尽量避免在where子句中使用or来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num=10 or Name = 'admin' 可以这样查询：123select id from t where num = 10union allselect id from t where Name = 'admin' in和not in也要慎用，否则会导致全表扫描，如：1select id from t where num in(1,2,3) 对于连续的数值，能用between就不要用in了：1select id from t where num between 1 and 3 很多时候用exists代替in是一个好的选择：1select num from a where num in (select num from b) 用下面的语句替换：1select num from a where exists (select 1 from b where num=a.num) 下面的查询也将导致全表扫描：1select id from t where name like ‘%abc%’ 若要提高效率，可以考虑全文检索。 如果在where子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描：1select id from t where num = @num 可以改为强制查询使用索引：1select id from t with(index(索引名)) where num = @num 应尽量避免在where子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如：1select id from t where num/2 = 100 应改为:1select id from t where num = 100*2 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如：12select id from t where substring(name,1,3) = 'abc' -–name以abc开头的idselect id from t where datediff(day,createdate,’2005-11-30′) = 0 -–'2005-11-30' --生成的id 应改为:12select id from t where name like 'abc%'select id from t where createdate &gt;= '2005-11-30' and createdate &lt; '2005-12-1' 不要在where子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 不要写一些没有意义的查询，如需要生成一个空表结构： 1select col1,col2 into #t from t where 1=0 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样：1create table #t(…) Update语句，如果只更改1、2个字段，不要Update全部字段，否则频繁调用会引起明显的性能消耗，同时带来大量日志。 对于多张大数据量（这里几百条就算大了）的表JOIN，要先分页再JOIN，否则逻辑读会很高，性能很差。 select count(*) from table这样不带任何条件的count会引起全表扫描，并且没有任何业务意义，是一定要杜绝的。 索引并不是越多越好，索引固然可以提高相应的select的效率，但同时也降低了insert及update的效率，因为insert或update时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 尽可能的使用varchar/nvarchar代替char/nchar，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 任何地方都不要使用select * from t，用具体的字段列表代替“*”，不要返回用不到的任何字段。 尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 避免频繁创建和删除临时表，以减少系统表资源的消耗。临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件， 最好使用导出表。 在新建临时表时，如果一次性插入数据量很大，那么可以使用select into 代替create table，避免造成大量log，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table，然后drop table，这样可以避免系统表的较长时间锁定。 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使用。对小型数据集使用FAST_FORWARD游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 在所有的存储过程和触发器的开始处设置SET NOCOUNT ON，在结束时设置 SET NOCOUNT OFF。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC消息。 尽量避免大事务操作，提高系统并发能力。 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 实际案例分析：拆分大的DELETE或INSERT语句，批量提交SQL语句 如果你需要在一个在线的网站上去执行一个大的DELETE或INSERT查询，你需要非常小心，要避免你的操作让你的整个网站停止相应。因为这两个操作是会锁表的，表一锁住了，别的操作都进不来了。Apache会有很多的子进程或线程。所以，其工作起来相当有效率，而我们的服务器也不希望有太多的子进程，线程和数据库链接，这是极大的占服务器资源的事情，尤其是内存。如果你把你的表锁上一段时间，比如30秒钟，那么对于一个有很高访问量的站点来说，这30秒所积累的访问进程/线程，数据库链接，打开的文件数，可能不仅仅会让你的WEB服务崩溃，还可能会让你的整台服务器马上挂了。所以，如果你有一个大的处理，你一定把其拆分，使用LIMIT oracle(rownum),sqlserver(top)条件是一个好的方法。下面是一个mysql示例： 12345678910111213141516while(1)&#123; //每次只做1000条 mysql_query(“delete from logs where log_date &lt;= ’2012-11-01’ limit 1000”); if(mysql_affected_rows() == 0)&#123; //删除完成，退出！ break； &#125;//每次暂停一段时间，释放表让其他进程/线程访问。usleep(50000)&#125; http://www.cnblogs.com/yunfeifei/p/3850440.html]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Dead-Letter-Exchanges]]></title>
    <url>%2F2019%2F04%2F19%2FRabbitMQ%2FDead-Letter-Exchanges%2F</url>
    <content type="text"></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RabbitMQ安装]]></title>
    <url>%2F2019%2F04%2F19%2FRabbitMQ%2FRabbitMQ%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[由于RabbitMQ是用]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[最全面透彻的RabbitMQ指南]]></title>
    <url>%2F2019%2F04%2F19%2FRabbitMQ%2F%E6%9C%80%E5%85%A8%E9%9D%A2%E9%80%8F%E5%BD%BB%E7%9A%84RabbitMQ%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[本文大纲 RabbitMQ 历史 RabbitMQ 应用场景 RabbitMQ 系统架构 RabbitMQ 基本概念 RabbitMQ 细节阐明 历史-从开始到现在RabbitMQ是一个Erlang开发的AMQP（Advanced Message Queuing Protocol ）的开源实现。AMQP 的出现其实也是应了广大人民群众的需求，虽然在同步消息通讯的世界里有很多公开标准（如 Cobar）的 IIOP ，或者是 SOAP 等），但是在异步消息处理中却不是这样，只有大企业有一些商业实现（如微软的 MSMQ ，IBM 的 WebSphere MQ 等），因此，在 2006 年的 6 月，Cisco 、Red Hat、iMatix 等联合制定了 AMQP 的公开标准。 RabbitMQ由RabbitMQ Technologies Ltd开发并且提供商业支持的。该公司在2010年4月被SpringSource（VMware的一个部门）收购。在2013年5月被并入Pivotal。其实VMware，Pivotal和EMC本质上是一家的。不同的是，VMware是独立上市子公司，而Pivotal是整合了EMC的某些资源，现在并没有上市。 RabbitMQ官网：http://www.rabbitmq.com 一、应用场景言归正传。RabbitMQ，或者说AMQP解决了什么问题，或者说它的应用场景是什么？ 对于一个大型的软件系统来说，它会有很多的组件或者说模块，又或者说子系统。那这些模块又如何通信？这和传统的IPC有很大的区别。传统的IPC很多都是在单一系统上的，模块耦合性很大，不适合扩展（Scalability）。如果使用Socket，那么不同的模块的确可以部署到不同的机器上，但是还是有很多问题需要解决。比如： 信息的发送者和接收者如何维持这个连接，如果一方的连接中断，这期间的数据是以什么方式丢失？ 如何降低发送者和接收者的耦合度？ 如何让Priority高的接收者先接到数据？ 如何做到Load Balance？有效均衡接收者的负载？ 如何有效的将数据发送到相关的接收者？也就是说将接收者subscribe 不同的数据，如何做有效的filter。 如何做到可扩展，甚至将这个通信模块发到cluster上？ 如何保证接收者接收到了完整，正确的数据？ AMQP协议解决了以上的问题，而RabbitMQ实现了AMQP。 二、系统架构 RabbitMQ Server也叫Broker Server，它不是运送食物的卡车，而是一种传输服务。原话是RabbitMQ isn’t a food truck, it’s a delivery service. 它的角色就是维护一条从Producer到Consumer的路线，保证数据能够按照指定的方式进行传输。虽然这个保证也不是100%的保证，但是对于普通的应用来说这已经足够了。当然对于商业系统来说，可以再做一层数据一致性的guard，就可以彻底保证系统的一致性了。 Client P也叫Producer，数据的发送方。Create messages and publish (send) them to a Broker Server (RabbitMQ)。一个Message有两个部分：payload（有效载荷）和label（标签）。payload顾名思义就是传输的数据。label是exchange的名字或者说是一个tag，它描述了payload，而且RabbitMQ也是通过这个label来决定把这个Message发给哪个Consumer。AMQP仅仅描述了label，而RabbitMQ决定了如何使用这个label的规则。 Client C也叫Consumer，数据的接收方。Consumers attach to a Broker Server (RabbitMQ) and subscribe to a queue。把queue比作是一个有名字的邮箱。当有Message到达某个邮箱后，RabbitMQ把它发送给它的某个订阅者即Consumer。当然可能会把同一个Message发送给很多的Consumer。在这个Message中，只有payload，label已经被删掉了。对于Consumer来说，它是不知道谁发送的这个信息的，就是协议本身不支持。当然了，如果Producer发送的payload包含了Producer的信息就另当别论了。 对于一个数据从Producer到Consumer的正确传递，还有三个概念需要明确：exchanges, queues and bindings。 Exchanges are where producers publish their messages. Queues are where the messages end up and are received by consumers. Bindings are how the messages get routed from the exchange to particular queues. 还有几个概念是上述图中没有标明的，那就是Connection（连接）和Channel（通道，频道）。 Connection就是一个TCP的连接。Producer和Consumer都是通过TCP连接到RabbitMQ Server的。以后我们可以看到，程序的起始处就是建立这个TCP连接。 Channel虚拟连接。它建立在上述的TCP连接中。数据流动都是在Channel中进行的。也就是说，一般情况是程序起始建立TCP连接，第二步就是建立这个Channel。 那么，为什么使用Channel，而不是直接使用TCP连接？ 对于OS来说，建立和关闭TCP连接是有代价的，频繁的建立关闭TCP连接对于系统的性能有很大的影响，而且TCP的连接数也有限制，这也限制了系统处理高并发的能力。但是，在TCP连接中建立Channel是没有上述代价的。对于Producer或者Consumer来说，可以并发的使用多个Channel进行Publish或者Receive。有实验表明，1s的数据可以Publish10K的数据包。当然对于不同的硬件环境，不同的数据包大小这个数据肯定不一样，但是我只想说明，对于普通的Consumer或者Producer来说，这已经足够了。如果不够用，你考虑的应该是如何细化SPLIT你的设计。 相关定义： Broker： 简单来说就是消息队列服务器实体 Exchange： 消息交换机，它指定消息按什么规则，路由到哪个队列 Queue： 消息队列载体，每个消息都会被投入到一个或多个队列 Binding： 绑定，它的作用就是把exchange和queue按照路由规则绑定起来 Routing Key： 路由关键字，exchange根据这个关键字进行消息投递 VHost： 虚拟主机，一个broker里可以开设多个vhost，用作不同用户的权限分离。 Producer： 消息生产者，就是投递消息的程序 Consumer： 消息消费者，就是接受消息的程序 Channel： 消息通道，在客户端的每个连接里，可建立多个channel，每个channel代表一个会话任务 由Exchange、Queue、RoutingKey三个才能决定一个从Exchange到Queue的唯一的线路。 三、基本概念Connection Factory、Connection、Channel都是RabbitMQ对外提供的API中最基本的对象。Connection是RabbitMQ的socket链接，它封装了socket协议相关部分逻辑。Connection Factory则是Connection的制造工厂。 Channel是我们与RabbitMQ打交道的最重要的一个接口，我们大部分的业务操作是在Channel这个接口中完成的，包括定义Queue、定义Exchange、绑定Queue与Exchange、发布消息等。 QueueQueue（队列）是RabbitMQ的内部对象，用于存储消息，如下图表示。 RabbitMQ中的消息都只能存储在Queue中，生产者（下图中的P）生产消息并最终投递到Queue中，消费者（下图中的C）可以从Queue中获取消息并消费。 多个消费者可以订阅同一个Queue，这时Queue中的消息会被平均分摊给多个消费者进行处理，而不是每个消费者都收到所有的消息并处理。 Message acknowledgment在实际应用中，可能会发生消费者收到Queue中的消息，但没有处理完成就宕机（或出现其他意外）的情况，这种情况下就可能会导致消息丢失。为了避免这种情况发生，我们可以要求消费者在消费完消息后发送一个回执给RabbitMQ，RabbitMQ收到消息回执（Message acknowledgment）后才将该消息从Queue中移除。 如果RabbitMQ没有收到回执并检测到消费者的RabbitMQ连接断开，则RabbitMQ会将该消息发送给其他消费者（如果存在多个消费者）进行处理。这里不存在timeout，一个消费者处理消息时间再长也不会导致该消息被发送给其他消费者，除非它的RabbitMQ连接断开。 这里会产生另外一个问题，如果我们的开发人员在处理完业务逻辑后，忘记发送回执给RabbitMQ，这将会导致严重的bug——Queue中堆积的消息会越来越多。消费者重启后会重复消费这些消息并重复执行业务逻辑。 另外publish message 是没有ACK的。 Message durability如果我们希望即使在RabbitMQ服务重启的情况下，也不会丢失消息，我们可以将Queue与Message都设置为可持久化的（durable），这样可以保证绝大部分情况下我们的RabbitMQ消息不会丢失。但依然解决不了小概率丢失事件的发生（比如RabbitMQ服务器已经接收到生产者的消息，但还没来得及持久化该消息时RabbitMQ服务器就断电了），如果我们需要对这种小概率事件也要管理起来，那么我们要用到事务。由于这里仅为RabbitMQ的简单介绍，所以这里将不讲解RabbitMQ相关的事务。 Prefetch count前面我们讲到如果有多个消费者同时订阅同一个Queue中的消息，Queue中的消息会被平摊给多个消费者。这时如果每个消息的处理时间不同，就有可能会导致某些消费者一直在忙，而另外一些消费者很快就处理完手头工作并一直空闲的情况。我们可以通过设置Prefetch count来限制Queue每次发送给每个消费者的消息数，比如我们设置prefetchCount=1，则Queue每次给每个消费者发送一条消息；消费者处理完这条消息后Queue会再给该消费者发送一条消息。 Exchange在上一节我们看到生产者将消息投递到Queue中，实际上这在RabbitMQ中这种事情永远都不会发生。实际的情况是，生产者将消息发送到Exchange（交换器，下图中的X），由Exchange将消息路由到一个或多个Queue中（或者丢弃）。 Exchange是按照什么逻辑将消息路由到Queue的？这个将在Binding一节中介绍。 RabbitMQ中的Exchange有四种类型，不同的类型有着不同的路由策略，这将在Exchange Types一节介绍。 Routing Key生产者在将消息发送给Exchange的时候，一般会指定一个Routing Key，来指定这个消息的路由规则，而这个Routing Key需要与Exchange Type及Binding key联合使用才能最终生效。 在Exchange Type与Binding key固定的情况下（在正常使用时一般这些内容都是固定配置好的），我们的生产者就可以在发送消息给Exchange时，通过指定Routing Key来决定消息流向哪里。 RabbitMQ为Routing Key设定的长度限制为255 bytes。 BindingRabbitMQ中通过Binding将Exchange与Queue关联起来，这样RabbitMQ就知道如何正确地将消息路由到指定的Queue了。 Binding key在绑定（Binding）Exchange与Queue的同时，一般会指定一个Binding key。消费者将消息发送给Exchange时，一般会指定一个Routing Key。当Binding key与Routing Key相匹配时，消息将会被路由到对应的Queue中。这个将在Exchange Types章节会列举实际的例子加以说明。 在绑定多个Queue到同一个Exchange的时候，这些Binding允许使用相同的Binding key。 Binding key并不是在所有情况下都生效，它依赖于Exchange Type，比如fanout类型的Exchange就会无视Binding key，而是将消息路由到所有绑定到该Exchange的Queue。 Exchange TypesRabbitMQ常用的Exchange Type有fanout、direct、topic、headers这四种（AMQP规范里还提到两种Exchange Type，分别为system与自定义，这里不予以描述），下面分别进行介绍。 fanoutfanout类型的Exchange路由规则非常简单，它会把所有发送到该Exchange的消息路由到所有与它绑定的Queue中。 上图中，生产者（P）发送到Exchange（X）的所有消息都会路由到图中的两个Queue，并最终被两个消费者（C1与C2）消费。 directdirect类型的Exchange路由规则也很简单，它会把消息路由到那些Binding key与Routing key完全匹配的Queue中。 以上图的配置为例，我们以routingKey=”error”发送消息到Exchange，则消息会路由到Queue1（amqp.gen-S9b…，这是由RabbitMQ自动生成的Queue名称）和Queue2（amqp.gen-Agl…）；如果我们以Routing Key=”info”或routingKey=”warning”来发送消息，则消息只会路由到Queue2。如果我们以其他Routing Key发送消息，则消息不会路由到这两个Queue中。 topic前面讲到direct类型的Exchange路由规则是完全匹配Binding Key与Routing Key，但这种严格的匹配方式在很多情况下不能满足实际业务需求。topic类型的Exchange在匹配规则上进行了扩展，它与direct类型的Exchage相似，也是将消息路由到Binding Key与Routing Key相匹配的Queue中，但这里的匹配规则有些不同，它约定： Routing Key为一个句点号“.”分隔的字符串（我们将被句点号”. “分隔开的每一段独立的字符串称为一个单词），如”stock.usd.nyse”、”nyse.vmw”、”quick.orange.rabbit”。Binding Key与Routing Key一样也是句点号“. ”分隔的字符串。 Binding Key中可以存在两种特殊字符”*“与”#”，用于做模糊匹配，其中”*”用于匹配一个单词，”#”用于匹配多个单词（可以是零个）。 以上图中的配置为例，routingKey=”quick.orange.rabbit”的消息会同时路由到Q1与Q2，routingKey=”lazy.orange.fox”的消息会路由到Q1，routingKey=”lazy.brown.fox”的消息会路由到Q2，routingKey=”lazy.pink.rabbit”的消息会路由到Q2（只会投递给Q2一次，虽然这个routingKey与Q2的两个bindingKey都匹配）；routingKey=”quick.brown.fox”、routingKey=”orange”、routingKey=”quick.orange.male.rabbit”的消息将会被丢弃，因为它们没有匹配任何bindingKey。 headersheaders类型的Exchange不依赖于Routing Key与Binding Key的匹配规则来路由消息，而是根据发送的消息内容中的headers属性进行匹配。 在绑定Queue与Exchange时指定一组键值对；当消息发送到Exchange时，RabbitMQ会取到该消息的headers（也是一个键值对的形式），对比其中的键值对是否完全匹配Queue与Exchange绑定时指定的键值对。如果完全匹配则消息会路由到该Queue，否则不会路由到该Queue。 该类型的Exchange没有用到过（不过也应该很有用武之地），所以不做介绍。 RPCMQ本身是基于异步的消息处理，前面的示例中所有的生产者（P）将消息发送到RabbitMQ后不会知道消费者（C）处理成功或者失败（甚至连有没有消费者来处理这条消息都不知道）。 但实际的应用场景中，我们很可能需要一些同步处理，需要同步等待服务端将我的消息处理完成后再进行下一步处理。这相当于RPC（Remote Procedure Call，远程过程调用）。在RabbitMQ中也支持RPC。 RabbitMQ中实现RPC的机制是： 客户端发送请求（消息）时，在消息的属性（Message Properties，在AMQP协议中定义了14种properties，这些属性会随着消息一起发送）中设置两个值replyTo（一个Queue名称，用于告诉服务器处理完成后将通知我的消息发送到这个Queue中）和correlationId（此次请求的标识号，服务器处理完成后需要将此属性返还，客户端将根据这个id了解哪条请求被成功执行了或执行失败）。服务器端收到消息处理完后，将生成一条应答消息到replyTo指定的Queue，同时带上correlationId属性。客户端之前已订阅replyTo指定的Queue，从中收到服务器的应答消息后，根据其中的correlationId属性分析哪条请求被执行了，根据执行结果进行后续业务处理。 四、细节阐明使用ACK确认Message的正确传递默认情况下，如果Message 已经被某个Consumer正确的接收到了，那么该Message就会被从Queue中移除。当然也可以让同一个Message发送到很多的Consumer。 如果一个Queue没被任何的Consumer Subscribe（订阅），当有数据到达时，这个数据会被cache，不会被丢弃。当有Consumer时，这个数据会被立即发送到这个Consumer。这个数据被Consumer正确收到时，这个数据就被从Queue中删除。 那么什么是正确收到呢？通过ACK。每个Message都要被acknowledged（确认，ACK）。我们可以显示的在程序中去ACK，也可以自动的ACK。如果有数据没有被ACK，那么RabbitMQ Server会把这个信息发送到下一个Consumer。 如果这个APP有bug，忘记了ACK，那么RabbitMQ Server不会再发送数据给它，因为Server认为这个Consumer处理能力有限。而且ACK的机制可以起到限流的作用（Benefitto throttling）：在Consumer处理完成数据后发送ACK，甚至在额外的延时后发送ACK，将有效的balance Consumer的load。 当然对于实际的例子，比如我们可能会对某些数据进行merge，比如merge 4s内的数据，然后sleep 4s后再获取数据。特别是在监听系统的state，我们不希望所有的state实时的传递上去，而是希望有一定的延时。这样可以减少某些IO，而且终端用户也不会感觉到。 Reject a message有两种方式，第一种的Reject可以让RabbitMQ Server将该Message 发送到下一个Consumer。第二种是从Queue中立即删除该Message。 Creating a queueConsumer和Procuder都可以通过 queue.declare 创建queue。对于某个Channel来说，Consumer不能declare一个queue，却订阅其他的queue。当然也可以创建私有的queue。这样只有APP本身才可以使用这个queue。queue也可以自动删除，被标为auto-delete的queue在最后一个Consumer unsubscribe后就会被自动删除。那么如果是创建一个已经存在的queue呢？那么不会有任何的影响。需要注意的是没有任何的影响，也就是说第二次创建如果参数和第一次不一样，那么该操作虽然成功，但是queue的属性并不会被修改。 那么谁应该负责创建这个queue呢？是Consumer，还是Producer？ 如果queue不存在，当然Consumer不会得到任何的Message。那么Producer Publish的Message会被丢弃。所以，还是为了数据不丢失，Consumer和Producer都try to create the queue！反正不管怎么样，这个接口都不会出问题。 queue对load balance的处理是完美的。对于多个Consumer来说，RabbitMQ 使用循环的方式（round-robin）的方式均衡的发送给不同的Consumer。 Exchanges从架构图可以看出，Procuder Publish的Message进入了Exchange。接着通过”routing keys”， RabbitMQ会找到应该把这个Message放到哪个queue里。queue也是通过这个routing keys来做的绑定。 有三种类型的Exchanges：direct, fanout, topic。 每个实现了不同的路由算法（routing algorithm）。 Direct exchange：如果 routing key 匹配，那么Message就会被传递到相应的queue中。其实在queue创建时，它会自动的以queue的名字作为routing key来绑定那个exchange。 Fanout exchange：会向响应的queue广播。 Topic exchange：对key进行模式匹配，比如ab可以传递到所有ab的queue。 Virtual hosts每个virtual host本质上都是一个RabbitMQ Server，拥有它自己的queue，exchagne，和bings rule等等。这保证了你可以在多个不同的Application中使用RabbitMQ。]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis哨兵模式安装与配置]]></title>
    <url>%2F2019%2F04%2F19%2FRedis%2FRedis%E5%93%A8%E5%85%B5%E6%A8%A1%E5%BC%8F%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[在介绍哨兵模式之前首先介绍下Redis主从复制。 Redis 主从复制 可将 主节点 数据同步给 从节点，从节点此时有两个作用： 一旦 主节点宕机，从节点 作为 主节点 的 备份 可以随时顶上来。 扩展 主节点 的 读能力，分担主节点读压力。 主从复制 同时存在以下几个问题： 一旦 主节点宕机，从节点 晋升成 主节点，同时需要修改 应用方 的 主节点地址，还需要命令所有 从节点 去 复制 新的主节点，整个过程需要 人工干预。 主节点 的 写能力 受到 单机的限制。 主节点 的 存储能力 受到 单机的限制。 原生复制 的弊端在早期的版本中也会比较突出，比如：Redis 复制中断 后，从节点 会发起 psync。此时如果 同步不成功，则会进行 全量同步，主库 执行 全量备份 的同时，可能会造成毫秒或秒级的 卡顿。 Redis Sentinel 是 Redis 高可用 的实现方案。Sentinel 是一个管理多个 Redis 实例的工具，它可以实现对 Redis 的 监控、通知、自动故障转移。 1. Redis Sentinel规划一个一主多从的Redis系统中，可以使用多个哨兵进行监控任务以保证系统足够稳健。此时，不仅哨兵会同时监控主数据库和从数据库，哨兵之间也会相互监控。在这里，建议大家哨兵至少部署3个，并且使用奇数个哨兵。本文使用3台服务器作为sentinal哨兵节点，另外3台服务器作为主从数据节点。 IP 端口号 角色 192.168.88.111 6379 Redis Master 192.168.88.112 6379 Redis Slave 192.168.88.113 6379 Redis Slave 192.168.88.114 26379 Sentinel 192.168.88.115 26379 Sentinel 192.168.88.116 26379 Sentinel 2. Redis安装与配置本文使用的Redis版本为redis-4.0.11。 2.1 数据节点和哨兵节点通用安装源码安装包redis-4.0.11.tar.gz放在/opt目录下。 12345678910111213141516# 解压cd /opttar -zxvf redis-4.0.11.tar.gzcd redis-4.0.11# 安装makemake install# 创建相关文件夹mkdir /usr/local/redis# 拷贝启动命令文件cd /opt/redis-4.0.11/srccp redis-cli /usr/local/rediscp redis-server /usr/local/redis 2.2 主从数据节点配置拷贝配置文件到相关目录：12cd /opt/redis-4.0.11cp redis.conf /usr/local/redis 编辑配置文件：12cd /usr/local/redisvim redis.conf 常用配置如下：1234567891011121314151617181920# 注释以下内容开启远程访问# bind 127.0.0.1# Redis使用后台模式daemonize yes# 关闭保护模式protected-mode no# 修改pidfile指向路径pidfile /usr/local/redis/redis.pid#日志文件logfile &quot; /usr/local/redis/log/redis.log&quot;#数据库文件名dbfilename dump.rdb#数据库文存放目录dir /usr/local/redis/data 如果是从节点服务器，则redis.conf配置文件中还需要添加如下内容：12# 192.168.88.111为主节点ip，6379为端口slaveof 192.168.88.111 6379 启动服务：12cd /usr/local/redisredis-server redis.conf 启动客户端：12redis-cliredis-cli -p 6379 查看节点状态：1info replication 如果需要远程连接redis服务器，需要修改防火墙配置：123456789# centos 6vim /etc/sysconfig/iptables# 添加如下配置-A INPUT -m state --state NEW -m tcp -p tcp --dport 6379 -j ACCEPTservice iptables restart# centos 7firewall-cmd --permanent --add-port=6379/tcpfirewall-cmd --reload 没有使用哨兵模式，主redis服务器挂掉之后，从redis服务器只能读不能写。 2.3 sentinel哨兵节点配置拷贝sentinel配置文件到相关目录：12cd /opt/redis-4.0.11cp sentinel.conf /usr/local/redis 编辑配置文件：123456# 关闭保护模式protected-mode no# sentinel monitor [master-group-name] [ip] [port] [quorum]# 该行的意思是：监控的master的名字叫做mymaster （可以自定义），地址为192.168.88.111:6379，行尾最后的一个2代表在sentinel集群中，多少个sentinel认为master死了，才能真正认为该master不可用了。sentinel monitor mymaster 192.168.88.111 6379 2 启动sentinel：12cd /usr/local/redisredis-server sentinel.conf --sentinel 防火墙设置：12345678vim /etc/sysconfig/iptables# 添加如下配置-A INPUT -m state --state NEW -m tcp -p tcp --dport 26379 -j ACCEPTservice iptables restart# centos 7firewall-cmd --permanent --add-port=26379/tcpfirewall-cmd --reload 3. 总结Redis哨兵为Redis提供了高可用性。实际上这意味着你可以使用哨兵模式创建一个可以不用人为干预而应对各种故障的Redis部署。 哨兵模式还提供了其他的附加功能，如监控，通知，为客户端提供配置。 下面是在宏观层面上哨兵模式的功能列表：监控：哨兵不断的检查master和slave是否正常的运行。通知：当监控的某台Redis实例发生问题时，可以通过API通知系统管理员和其他的应用程序。自动故障转移：如果一个master不正常运行了，哨兵可以启动一个故障转移进程，将一个slave升级成为master，其他的slave被重新配置使用新的master，并且应用程序使用Redis服务端通知的新地址。配置提供者：哨兵作为Redis客户端发现的权威来源：客户端连接到哨兵请求当前可靠的master的地址。如果发生故障，哨兵将报告新地址。 哨兵的分布式特性Redis哨兵是一个分布式系统：哨兵自身被设计成和多个哨兵进程一起合作运行。有多个哨兵进程合作的好处有：当多个哨兵对一个master不再可用达成一致时执行故障检测。这会降低错误判断的概率。即使在不是所有的哨兵都工作时哨兵也会工作，使系统健壮的抵抗故障。毕竟在故障系统里单点故障没有什么意义。 Redis的哨兵、Redis实例(master和slave)、和客户端是一个有特种功能的大型分布式系统。 当然，Redis Sentinel 仅仅解决了 高可用 的问题，对于 主节点 单点写入和单节点无法扩容等问题，还需要引入 Redis Cluster 集群模式 予以解决。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[RocketMQ生产环境下的配置和使用]]></title>
    <url>%2F2019%2F04%2F19%2FRocketMQ%2FRocketMQ%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%9A%84%E9%85%8D%E7%BD%AE%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文的目的是带领读者快速将RocketMQ应用到生产环境中，因此不会探究原理和细节。本文会先介绍RocketMQ的各个角色，然后介绍如何搭建一个高可用的分布式消息队列集群，以及RocketMQ的Consumer和Producer的使用方法与常用命令。 1. RocketMQ各部分角色介绍 RocketMQ由四部分组成，先来直观地了解一下这些角色以及各自的功能。分布式消息队列是用来高效地传输消息的，它的功能和现实生活中的邮局收发信件很类似，我们类比地说一下相应的模块。现实生活中的邮政系统要正常运行，离不开下面这四个角色，一是发信者，二是收信者，三是负责暂存、传输的邮局，四是负责协调各个地方邮局的管理机构。对应到RocketMQ中，这四个角色就是Producer、Consumer、Broker和NameServer。 启动RocketMQ的顺序是先启动NameServer，再启动Broker，这时候消息队列已经可以提供服务了，想发送消息就使用Producer来发送，想接收消息就使用Consumer来接收。很多应用程序既要发送，又要接收，可以启动多个Producer和Consumer来发送多种消息，同时接收多种消息。 为了消除单点故障，增加可靠性或增大吞吐量，可以在多台机器上部署多个NameServer和Broker，为每个Broker部署一个或多个Slave。 了解了四种角色以后，再介绍一下Topic和Message Queue这两个名词。一个分布式消息队列中间件部署好以后，可以给很多个业务提供服务，同一个业务也有不同类型的消息要投递，这些不同类型的消息以不同的Topic名称来区分。所以发送和接收消息前，先创建Topic，针对某个Topic发送和接收消息。有了Topic以后，还需要解决性能问题。如果一个Topic要发送和接收的数据量非常大，需要能支持增加并行处理的机器来提高处理速度，这时候一个Topic可以根据需求设置一个或多个Message Queue，Message Queue类似分区或Partition。Topic有了多个Message Queue后，消息可以并行地向各个Message Queue发送，消费者也可以并行地从多个Message Queue读取消息并消费。 2. 多机集群配置和部署 本节将说明如何只用两台物理机，搭建出双主、双从、无单点故障的高可用RocketMQ集群。假设这两台物理机的IP分别是192.168.100.131和192.168.100.132。 2.1 启动多个NameServer和Broker 首先在这两台机器上分别启动NameServer（nohup sh bin/mqnamesrv &amp;），这样我们就得到了一个无单点的NameServer服务，服务地址是192.168.100.131:9876;192.168.100.132:9876。 然后启动Broker，每台机器上都要分别启动一个Master角色的Broker和一个Slave角色的Broker，并互为主备。可以基于RocketMQ自带的示例配置文件写自己的配置文件（示例配置文件在conf/2m-2s-sync目录下）。 1）192.168.100.131机器上Master Broker的配置文件： 12345678910namesrvAddr=192.168.100.131:9876;192.168.100.132:9876brokerClusterName=DefaultClusterbrokerName=broker-abrokerId=0deleteWhen=04fileReservedTime=48brokerRole=SYNC_MASTERflushDiskType=ASYNC_FLUSHlistenPort=10911storePathRootDir=/home/rocketmq/store-a 2）192.168.100.132机器上Master Broker的配置文件： 12345678910namesrvAddr=192.168.100.131:9876;192.168.100.132:9876brokerClusterName=DefaultClusterbrokerName=broker-bbrokerId=0deleteWhen=04fileReservedTime=48brokerRole=SYNC_MASTERflushDiskType=ASYNC_FLUSHlistenPort=10911storePathRootDir=/home/rocketmq/store-b 3）192.168.100.131机器上SlaveBroker的配置文件： 12345678910namesrvAddr=192.168.100.131:9876;192.168.100.132:9876brokerClusterName=DefaultClusterbrokerName=broker-abrokerId=1deleteWhen=04fileReservedTime=48brokerRole=SLAVEflushDiskType=ASYNC_FLUSHlistenPort=11011storePathRootDir=/home/rocketmq/store-a 4）192.168.100.132机器上Slave Broker的配置文件： 12345678910namesrvAddr=192.168.100.131:9876;192.168.100.132:9876brokerClusterName=DefaultClusterbrokerName=broker-bbrokerId=1deleteWhen=04fileReservedTime=48brokerRole=SLAVEflushDiskType=ASYNC_FLUSHlistenPort=11011storePathRootDir=/home/rocketmq/store-b 然后分别使用如下命令启动四个Broker： 1nohup sh ./bin/mqbroker -c config_file &amp; 这样一个高可用的RocketMQ集群就搭建好了，还可以在一台机器上启动rocketmq-console，比如在192.168.100.131上启动RocketMQ-console，然后在浏览器中输入地址192.168.100.131:8080，这样就可以可视化地查看集群状态了。 2.2 配置参数介绍 本节将逐个介绍Broker配置文件中用到的参数含义： 1）namesrvAddr=192.168.100.131:9876;192.168.100.132:9876 NamerServer的地址，可以是多个。 2）brokerClusterName=DefaultCluster Cluster的地址，如果集群机器数比较多，可以分成多个Cluster，每个Cluster供一个业务群使用。 3）brokerName=broker-a Broker的名称，Master和Slave通过使用相同的Broker名称来表明相互关系，以说明某个Slave是哪个Master的Slave。 4）brokerId=0 一个Master Borker可以有多个Slave，0表示Master，大于0表示不同Slave的ID。 5）fileReservedTime=48 在磁盘上保存消息的时长，单位是小时，自动删除超时的消息。 6）deleteWhen=04 与fileReservedTime参数呼应，表明在几点做消息删除动作，默认值04表示凌晨4点。 7）brokerRole=SYNC_MASTER brokerRole有3种：SYNC_MASTER、ASYNC_MASTER、SLAVE。关键词SYNC和ASYNC表示Master和Slave之间同步消息的机制，SYNC的意思是当Slave和Master消息同步完成后，再返回发送成功的状态。 8）flushDiskType=ASYNC_FLUSH flushDiskType表示刷盘策略，分为SYNC_FLUSH和ASYNC_FLUSH两种，分别代表同步刷盘和异步刷盘。同步刷盘情况下，消息真正写入磁盘后再返回成功状态；异步刷盘情况下，消息写入page_cache后就返回成功状态。 9）listenPort=10911 Broker监听的端口号，如果一台机器上启动了多个Broker，则要设置不同的端口号，避免冲突。 10）storePathRootDir=/home/rocketmq/store-a 存储消息以及一些配置信息的根目录。 这些配置参数，在Broker启动的时候生效，如果启动后有更改，要重启Broker。现在使用云服务或多网卡的机器比较普遍，Broker自动探测获得的ip地址可能不符合要求，通过brokerIP1=47.98.41.234这样的配置参数，可以设置Broker机器对外暴露的ip地址。 2.3 发送/接收消息示例 可以用自己熟悉的开发工具创建一个Java项目，加入RocketMQ Client包的依赖，用下面代码发送消息，这个示例代码是以Sync方式发送消息的。 Producer示例程序 12345678910111213141516171819202122public class SyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer("please_rename_unique_group_name"); // Specify name server addresses. producer.setNamesrvAddr("localhost:9876"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message("TopicTest" /* Topic */, "TagA" /* Tag */, ("Hello RocketMQ " + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. SendResult sendResult = producer.send(msg); System.out.printf("%s%n", sendResult); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 主要流程是：创建一个DefaultMQProducer对象，设置好GroupName和NameServer地址后启动，然后把待发送的消息拼装成Message对象，使用Producer来发送。接下来看看如何接收消息，也就是使用DefaultMQPushConsumer类实现的消费者程序，代码如下。 Consumer示例程序 12345678910111213141516171819202122232425262728public class Consumer &#123; public static void main(String[] args) throws InterruptedException, MQClientException &#123; // Instantiate with specified consumer group name. DefaultMQPushConsumer consumer = new DefaultMQPushConsumer("please_rename_unique_group_name"); // Specify name server addresses. consumer.setNamesrvAddr("localhost:9876"); // Subscribe one more more topics to consume. consumer.subscribe("TopicTest", "*"); // Register callback to execute on arrival of messages fetched from brokers. consumer.registerMessageListener(new MessageListenerConcurrently() &#123; @Override public ConsumeConcurrentlyStatus consumeMessage(List&lt;MessageExt&gt; msgs, ConsumeConcurrentlyContext context) &#123; System.out.printf("%s Receive New Messages: %s %n", Thread.currentThread().getName(), msgs); return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; &#125; &#125;); //Launch the consumer instance. consumer.start(); System.out.printf("Consumer Started.%n"); &#125;&#125; Consumer或Producer都必须设置GroupName、NameServer地址以及端口号。然后指明要操作的Topic名称，最后进入发送和接收逻辑。 2.4 常用管理命令 MQAdmin是RocketMQ自带的命令行管理工具，在bin目录下，运行mqadmin即可执行。使用mqadmin命令，可以进行创建、修改Topic，更新Broker的配置信息，查询特定消息等各种操作。 具体命令参考：https://rocketmq.apache.org/docs/cli-admin-tool/ 2.5 通过图形界面管理集群 对于RocketMQ新手，可以启动运维服务，从页面上直观看到消息队列集群的状态。有一定经验以后，可以使用命令行更快捷，其功能更全面。 运维服务程序是个SpringBoot项目，需要从GitHub上的apache/rocketmq-externals里下载源码（https://github.com/apache/rocketmq-externals/tree/master/rocketmq-console）。 进入下载源码的目录，运行如下命令即可启动： 1mvn spring-boot:run 也可以编译成jar包，通过java -jar来执行。 服务启动后，在浏览器里访问server_ip_address:8080（server_ip_address是启动rocketmq-console的机器IP）地址就可看到集群的状态。 2.6 本章小结 在生产环境中使用RocketMQ集群需要比QuickStart部分了解更多的内容，本文在机器角色、集群配置和部署，以及集群管理方面都做了介绍，用户可以基于这些内容搭建起一个生成环境的RocketMQ消息队列集群，在数据量不大的非关键场景，可以快速上线。]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[快速上手RocketMQ]]></title>
    <url>%2F2019%2F04%2F19%2FRocketMQ%2F%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8BRocketMQ%2F</url>
    <content type="text"><![CDATA[本文介绍如何安装配置单机版的RocketMQ，以及简单地收发消息。读者也可以参考RocketMQ官网的说明文档。 1. RocketMQ的下载、安装和配置RocketMQ的Binary版是一些编译好的jar和辅助的shell脚本，可以直接从官网找到下载链接（http://rocketmq.apache.org/dowloading/releases/），也可以下载源码自己编译。系统要求：64bit的Linux、Unix或Mac。Java版本大于等于JDK1.8。如果需要从GitHub上下载源码和编译的话，需要安装Maven 3.2.x和Git。RocketMQ当前的最新版本是4.2.0，下面以Binary版本为例说明如何快速使用：123&gt; unzip rocketmq-all-4.2.0-bin-release.zip -d ./rocketmq-all-4.2.0-bin&gt; ls&gt; cd rocketmq-all-4.2.0-bin/ 里面含有以下内容：1LICENSE NOTICE README.md benchmark/ bin/ conf/ lib/ LICENSE、NOTICE和README.md包括一些版权声明和功能说明信息；benchmark里包括运行benchmark程序的shell脚本；bin文件夹里含有各种使用RocketMQ的shell脚本（Linux平台）和cmd脚本（Windows平台），比如常用的启动NameServer的脚本mqnamesrv，启动Broker的脚本mqbroker，集群管理脚本mqadmin等；conf文件夹里有一些示例配置文件，包括三种方式的broker配置文件、logback日志配置文件等，用户在写配置文件的时候，一般基于这些示例配置文件，加上自己特殊的需求即可；lib文件夹里包括RocketMQ各个模块编译成的jar包，以及RocketMQ依赖的一些jar包，比如Netty、commons-lang、FastJSON等。 2. 启动消息队列服务启动单机的消息队列服务比较简单，不需要写配置文件，只需要依次启动本机的NameServer和Broker即可。启动NameServer：123&gt; nohup sh bin/mqnamesrv &amp;&gt; tail -f ~/logs/rocketmqlogs/namesrv.logThe Name Server boot success... 启动Broker：123&gt; nohup sh bin/mqbroker -n localhost:9876 &amp;&gt; tail -f ~/logs/rocketmqlogs/broker.logThe broker[%s, 192.168.0.233:10911] boot success... 这种方式启动Broker，Topic并不会自动创建。如果需要自动创建，使用命令nohup sh bin/mqbroker -n localhost:9876 autoCreateTopicEnable=true &amp;，添加了autoCreateTopicEnable=true。当然也可以使用mqadmin或者rocketmq-console添加Topic。 3. 用命令行发送和接收消息为了快速展示发送和接收消息，本节展示的是用命令行发送和接收消息，实际上就是运行写好的demo程序，后续我们可以参考这些demo来写自己的发送和接收程序。运行示例程序，发送和接收消息：12345&gt; export NAMESRV_ADDR=localhost:9876&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ProducerSendResult [sendStatus=SEND_OK, msgId= ...&gt; sh bin/tools.sh org.apache.rocketmq.example.quickstart.ConsumerConsumeMessageThread_%d Receive New Messages: [MessageExt... 4. 关闭消息队列消息队列被启动后，如果不主动关闭，则会一直在后台运行，占用系统资源。我们有专门用来关闭NameServer和Broker的命令。关闭NameServer和Broker：1234567&gt; sh bin/mqshutdown brokerThe mqbroker(36695) is running...Send shutdown request to mqbroker(36695) OK&gt; sh bin/mqshutdown namesrvThe mqnamesrv(36664) is running...Send shutdown request to mqnamesrv(36664) OK 恭喜，现在你已经能够使用RocketMQ发送并接收消息了，使用消息队列的基本功能就是这么简单。]]></content>
      <categories>
        <category>RocketMQ</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[缓存穿透、缓存并发、缓存失效之思路变迁]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%85%B6%E4%BB%96%2F%E7%BC%93%E5%AD%98%E7%A9%BF%E9%80%8F%E3%80%81%E7%BC%93%E5%AD%98%E5%B9%B6%E5%8F%91%E3%80%81%E7%BC%93%E5%AD%98%E5%A4%B1%E6%95%88%E4%B9%8B%E6%80%9D%E8%B7%AF%E5%8F%98%E8%BF%81%2F</url>
    <content type="text"><![CDATA[我们在用缓存的时候，不管是Redis或者Memcached，基本上都会遇到以下三个问题： 缓存穿透 缓存并发 缓存失效 一、缓存穿透 注：上面三个图会有什么问题呢？ 我们在项目中使用缓存通常都是先检查缓存中是否存在，如果存在直接返回缓存内容，如果不存在就直接查询数据库然后再缓存查询结果返回。这个时候如果我们查询的某一个数据在缓存中一直不存在，就会造成每一次请求都查询DB，这样缓存就失去了意义，在流量大时，可能DB就挂掉了。 那这种问题有什么好办法解决呢？ 要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。有一个比较巧妙的作法是，可以将这个不存在的key预先设定一个值。比如，“key”，“&amp;&amp;”。在返回这个&amp;&amp;值的时候，我们的应用就可以认为这是不存在的key，那我们的应用就可以决定是继续等待访问，还是放弃掉这次操作。如果继续等待访问，过一个时间轮询点后，再次请求这个key，如果取到的值不再是&amp;&amp;，则可以认为这时候key有值了，从而避免了透传到数据库，从而把大量的类似请求挡在了缓存之中。 二、缓存并发有时候如果网站并发访问高，一个缓存如果失效，可能出现多个进程同时查询DB，同时设置缓存的情况，如果并发确实很大，这也可能造成DB压力过大，还有缓存频繁更新的问题。 我现在的想法是对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询。 这种情况和刚才说的预先设定值问题有些类似，只不过利用锁的方式，会造成部分请求等待。 三、缓存失效引起这个问题的主要原因还是高并发的时候，平时我们设定一个缓存的过期时间时，可能有一些会设置1分钟，5分钟这些，并发很高时可能会出在某一个时间同时生成了很多的缓存，并且过期时间都一样，这个时候就可能引发一旦过期时间到后，这些缓存同时失效，请求全部转发到DB，DB可能会压力过重。 那如何解决这些问题呢？其中的一个简单方案就是将缓存失效时间分散开，比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 我们讨论的第二个问题时针对同一个缓存，第三个问题时针对很多缓存。 总结来看： 缓存穿透：查询一个必然不存在的数据。比如文章表，查询一个不存在的id，每次都会访问DB，如果有人恶意破坏，很可能直接对DB造成影响。 缓存失效：如果缓存集中在一段时间内失效，DB的压力凸显。这个没有完美解决办法，但可以分析用户行为，尽量让失效时间点均匀分布。当发生大量的缓存穿透，例如对某个失效的缓存的大并发访问就造成了缓存雪崩。 四、问题汇总问题1如何解决DB和缓存一致性问题？答：当修改了数据库后，有没有及时修改缓存。这种问题，以前有过实践，修改数据库成功，而修改缓存失败的情况，最主要就是缓存服务器挂了。而因为网络问题引起的没有及时更新，可以通过重试机制来解决。而缓存服务器挂了，请求首先自然也就无法到达，从而直接访问到数据库。那么我们在修改数据库后，无法修改缓存，这时候可以将这条数据放到数据库中，同时启动一个异步任务定时去检测缓存服务器是否连接成功，一旦连接成功则从数据库中按顺序取出修改数据，依次进行缓存最新值的修改。 问题2问下缓存穿透那块！例如，一个用户查询文章，通过ID查询，按照之前说的，是将缓存的KEY预先设置一个值，如果通过ID查询，发现是预先设定的一个值，比如说是“&amp;&amp;”，那之后的继续等待访问是什么意思，这个ID什么时候会真正被附上用户所需要的值呢？答：我刚说的主要是咱们常用的后面配置，前台获取的场景。前台无法获取相应的key，则等待，或者放弃。当在后台配置界面上配置了相关key和value之后，那么以前的key &amp;&amp;也自然会被替换掉。你说的那种情况，自然也应该会有一个进程会在某一个时刻，在缓存中设置这个ID，再有新的请求到达的时候，就会获取到最新的ID和value。 问题3其实用redis的话，那天看到一个不错的例子，双key，有一个当时生成的一个附属key来标识数据修改到期时间，然后快到的时候去重新加载数据，如果觉得key多可以把结束时间放到主key中，附属key起到锁的功能。答：这种方案，之前我们实践过。这种方案会产生双份数据，而且需要同时控制附属key与key之间的关系，操作上有一定复杂度。 问题4多级缓存是什么概念呢？答：多级缓存就像我今天之前给大家发的文章里面提到了，将ehcache与redis做二级缓存，就像我之前写的文章 http://www.jianshu.com/p/2cd6ad416a5a 提到过的。但同样会存在一致性问题，如果我们需要强一致性的话，缓存与数据库同步是会存在时间差的，所以我们在具体开发的过程中，一定要根据场景来具体分析，二级缓存更多的是解决缓存穿透与程序的健壮性，当集中式缓存出现问题的时候，我们的应用能够继续运行。]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型以及happens before规则]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FJava%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E4%BB%A5%E5%8F%8Ahappens%20before%E8%A7%84%E5%88%99%2F</url>
    <content type="text"><![CDATA[JMM的介绍在多线程中稍微不注意就会出现线程安全问题，那么什么是线程安全问题？我的认识是，在多线程下代码执行的结果与预期正确的结果不一致，该代码就是线程不安全的，否则则是线程安全的。虽然这种回答似乎不能获取什么内容，可以google下。在&lt;&lt;深入理解Java虚拟机&gt;&gt;中看到的定义。 原文如下： 当多个线程访问同一个对象时，如果不用考虑这些线程在运行时环境下的调度和交替运行，也不需要进行额外的同步，或者在调用方进行任何其他的协调操作，调用这个对象的行为都可以获取正确的结果，那这个对象是线程安全的。 关于定义的理解这是一个仁者见仁智者见智的事情。 出现线程安全的问题一般是因为主内存和工作内存数据不一致性和重排序导致的，而解决线程安全的问题最重要的就是理解这两种问题是怎么来的，那么，理解它们的核心在于理解java内存模型（JMM）。 在多线程条件下，多个线程肯定会相互协作完成一件事情，一般来说就会涉及到多个线程间相互通信告知彼此的状态以及当前的执行结果等，另外，为了性能优化，还会涉及到编译器指令重排序和处理器指令重排序。下面会一一来聊聊这些知识。 内存模型抽象结构线程间协作通信可以类比人与人之间的协作的方式，在现实生活中，之前网上有个流行语“你妈喊你回家吃饭了”，就以这个生活场景为例。小明在外面玩耍，小明妈妈在家里做饭，做晚饭后准备叫小明回家吃饭，那么就存在两种方式： 小明妈妈要去上班了十分紧急这个时候手机又没有电了，于是就在桌子上贴了一张纸条“饭做好了，放在…”，小明回家后看到纸条如愿吃到妈妈做的饭菜，那么，如果将小明妈妈和小明作为两个线程，那么这张纸条就是这两个线程间通信的共享变量，通过读写共享变量实现两个线程间协作； 还有一种方式就是，妈妈的手机还有电，妈妈在赶去坐公交的路上给小明打了个电话，这种方式就是通知机制来完成协作。同样，可以引申到线程间通信机制。 通过上面这个例子，应该有些认识。在并发编程中主要需要解决两个问题： 1. 线程之间如何通信； 2. 线程之间如何完成同步（这里的线程指的是并发执行的活动实体）。 通信是指线程之间以何种机制来交换信息，主要有两种：共享内存和消息传递。 这里，可以分别类比上面的两个举例。java内存模型是共享内存的并发模型，线程之间主要通过读-写共享变量来完成隐式通信。如果程序员不能理解Java的共享内存模型在编写并发程序时一定会遇到各种各样关于内存可见性的问题。 1. 哪些是共享变量在java程序中所有实例域，静态域和数组元素都是放在堆内存中（所有线程均可访问到，是可以共享的），而局部变量，方法定义参数和异常处理器参数不会在线程间共享。共享数据会出现线程安全的问题，而非共享数据不会出现线程安全的问题。关于JVM运行时内存区域在后面的文章会讲到。 2. JMM抽象结构模型我们知道CPU的处理速度和主存的读写速度不是一个量级的，为了平衡这种巨大的差距，每个CPU都会有缓存。因此，共享变量会先放在主存中，每个线程都有属于自己的工作内存，并且会把位于主存中的共享变量拷贝到自己的工作内存，之后的读写操作均使用位于工作内存的变量副本，并在某个时刻将工作内存的变量副本写回到主存中去。JMM就从抽象层次定义了这种方式，并且JMM决定了一个线程对共享变量的写入何时对其他线程是可见的。 如图为JMM抽象示意图，线程A和线程B之间要完成通信的话，要经历如下两步： 线程A从主内存中将共享变量读入线程A的工作内存后并进行操作，之后将数据重新写回到主内存中； 线程B从主存中读取最新的共享变量 从横向去看看，线程A和线程B就好像通过共享变量在进行隐式通信。这其中有很有意思的问题，如果线程A更新后数据并没有及时写回到主存，而此时线程B读到的是过期的数据，这就出现了“脏读”现象。 可以通过同步机制（控制不同线程间操作发生的相对顺序）来解决或者通过volatile关键字使得每次volatile变量都能够强制刷新到主存，从而对每个线程都是可见的。 3. 重排序 一个好的内存模型实际上会放松对处理器和编译器规则的束缚，也就是说软件技术和硬件技术都为同一个目标而进行奋斗：在不改变程序执行结果的前提下，尽可能提高并行度。JMM对底层尽量减少约束，使其能够发挥自身优势。因此，在执行程序时，为了提高性能，编译器和处理器常常会对指令进行重排序。一般重排序可以分为如下三种： 编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序； 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序； 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行的。 如图，1属于编译器重排序，而2和3统称为处理器重排序。这些重排序会导致线程安全的问题，一个很经典的例子就是DCL问题，这个在以后的文章中会具体去聊。针对编译器重排序，JMM的编译器重排序规则会禁止一些特定类型的编译器重排序；针对处理器重排序，编译器在生成指令序列的时候会通过插入内存屏障指令来禁止某些特殊的处理器重排序。 那么什么情况下，不能进行重排序了？下面就来说说数据依赖性。有如下代码： 1double pi = 3.14 //Adouble r = 1.0 //Bdouble area = pi * r * r //C 这是一个计算圆面积的代码，由于A,B之间没有任何关系，对最终结果也不会存在关系，它们之间执行顺序可以重排序。因此可以执行顺序可以是A-&gt;B-&gt;C或者B-&gt;A-&gt;C执行最终结果都是3.14，即A和B之间没有数据依赖性。 具体的定义为：如果两个操作访问同一个变量，且这两个操作有一个为写操作，此时这两个操作就存在数据依赖性这里就存在三种情况：1. 读后写；2.写后写；3. 写后读，者三种操作都是存在数据依赖性的，如果重排序会对最终执行结果会存在影响。 编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖性关系的两个操作的执行顺序。 另外，还有一个比较有意思的就是as-if-serial语义。 1as-if-serial as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提供并行度），（单线程）程序的执行结果不能被改变。编译器，runtime和处理器都必须遵守as-if-serial语义。 as-if-serial语义把单线程程序保护了起来，遵守as-if-serial语义的编译器，runtime和处理器共同为编写单线程程序的程序员创建了一个幻觉：单线程程序是按程序的顺序来执行的。 比如上面计算圆面积的代码，在单线程中，会让人感觉代码是一行一行顺序执行上，实际上A,B两行不存在数据依赖性可能会进行重排序，即A，B不是顺序执行的。as-if-serial语义使程序员不必担心单线程中重排序的问题干扰他们，也无需担心内存可见性问题。 4. happens-before规则 上面的内容讲述了重排序原则，一会是编译器重排序一会是处理器重排序，如果让程序员再去了解这些底层的实现以及具体规则，那么程序员的负担就太重了，严重影响了并发编程的效率。 因此，JMM为程序员在上层提供了六条规则，这样我们就可以根据规则去推论跨线程的内存可见性问题，而不用再去理解底层重排序的规则。下面以两个方面来说。 4.1 happens-before定义 happens-before的概念最初由Leslie Lamport在其一篇影响深远的论文（《Time，Clocks and the Ordering of Events in a Distributed System》）中提出，有兴趣的可以google一下。JSR-133使用happens-before的概念来指定两个操作之间的执行顺序。由于这两个操作可以在一个线程之内，也可以是在不同线程之间。 因此，JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证（如果A线程的写操作a与B线程的读操作b之间存在happens-before关系，尽管a操作和b操作在不同的线程中执行，但JMM向程序员保证a操作将对b操作可见）。 具体的定义为： 1）如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。 2）两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。 上面的（1）是JMM对程序员的承诺。从程序员的角度来说，可以这样理解happens-before关系：如果A happens-before B，那么Java内存模型将向程序员保证——A操作的结果将对B可见，且A的执行顺序排在B之前。注意，这只是Java内存模型向程序员做出的保证！ 上面的（2）是JMM对编译器和处理器重排序的约束原则。正如前面所言，JMM其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。JMM这么做的原因是：程序员对于这两个操作是否真的被重排序并不关心，程序员关心的是程序执行时的语义不能被改变（即执行结果不能被改变）。 因此，happens-before关系本质上和as-if-serial语义是一回事。 下面来比较一下as-if-serial和happens-before: as-if-serial VS happens-before as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序来执行的。 as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。 4.2 具体规则 具体的一共有八项规则： 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 程序中断规则：对线程interrupted()方法的调用先行于被中断线程的代码检测到中断时间的发生。 对象finalize规则：一个对象的初始化完成（构造函数执行结束）先行于发生它的finalize()方法的开始。 下面以一个具体的例子来讲下如何使用这些规则进行推论： 依旧以上面计算圆面积的进行描述。利用程序顺序规则（规则1）存在三个happens-before关系：1. A happens-before B；2. B happens-before C;3. A happens-before C。这里的第三个关系是利用传递性进行推论的。 A happens-before B,定义1要求A执行结果对B可见，并且A操作的执行顺序在B操作之前，但与此同时利用定义中的第二条，A,B操作彼此不存在数据依赖性，两个操作的执行顺序对最终结果都不会产生影响，在不改变最终结果的前提下，允许A，B两个操作重排序，即happens-before关系并不代表了最终的执行顺序。 5. 总结 上面已经聊了关于JMM的两个方面： JMM的抽象结构（主内存和线程工作内存）； 重排序以及happens-before规则。 接下来，我们来做一个总结。从两个方面进行考虑。 如果让我们设计JMM应该从哪些方面考虑，也就是说JMM承担哪些功能； happens-before与JMM的关系； 由于JMM，多线程情况下可能会出现哪些问题？ 5.1 JMM的设计 JMM是语言级的内存模型，在我的理解中JMM处于中间层，包含了两个方面：（1）内存模型；（2）重排序以及happens-before规则。同时，为了禁止特定类型的重排序会对编译器和处理器指令序列加以控制。 而上层会有基于JMM的关键字和J.U.C包下的一些具体类用来方便程序员能够迅速高效率的进行并发编程。站在JMM设计者的角度，在设计JMM时需要考虑两个关键因素: 程序员对内存模型的使用 程序员希望内存模型易于理解、易于编程。程序员希望基于一个强内存模型来编写代码。 编译器和处理器对内存模型的实现 编译器和处理器希望内存模型对它们的束缚越少越好，这样它们就可以做尽可能多的优化来提高性能。编译器和处理器希望实现一个弱内存模型。 另外还要一个特别有意思的事情就是关于重排序问题，更简单的说，重排序可以分为两类： 会改变程序执行结果的重排序。 不会改变程序执行结果的重排序。 JMM对这两种不同性质的重排序，采取了不同的策略，如下。 对于会改变程序执行结果的重排序，JMM要求编译器和处理器必须禁止这种重排序。 对于不会改变程序执行结果的重排序，JMM对编译器和处理器不做要求（JMM允许这种重排序） JMM的设计图为： 从图可以看出： JMM向程序员提供的happens-before规则能满足程序员的需求。JMM的happens-before规则不但简单易懂，而且也向程序员提供了足够强的内存可见性保证（有些内存可见性保证其实并不一定真实存在，比如上面的A happens-before B）。 JMM对编译器和处理器的束缚已经尽可能少。从上面的分析可以看出，JMM其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。 例如，如果编译器经过细致的分析后，认定一个锁只会被单个线程访问，那么这个锁可以被消除。 再如，如果编译器经过细致的分析后，认定一个volatile变量只会被单个线程访问，那么编译器可以把这个volatile变量当作一个普通变量来对待。这些优化既不会改变程序的执行结果，又能提高程序的执行效率。 5.2 happens-before与JMM的关系 一个happens-before规则对应于一个或多个编译器和处理器重排序规则。对于Java程序员来说，happens-before规则简单易懂，它避免Java程序员为了理解JMM提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现方法 5.3 今后可能需要关注的问题 从上面内存抽象结构来说，可能出在数据“脏读”的现象，这就是数据可见性的问题，另外，重排序在多线程中不注意的话也容易存在一些问题，比如一个很经典的问题就是DCL（双重检验锁），这就是需要禁止重排序，另外，在多线程下原子操作例如i++不加以注意的也容易出现线程安全的问题。 但总的来说，在多线程开发时需要从原子性，有序性，可见性三个方面进行考虑。J.U.C包下的并发工具类和并发容器也是需要花时间去掌握的，这些东西在以后得文章中多会一一进行讨论。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java多线程之内存可见性]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B9%8B%E5%86%85%E5%AD%98%E5%8F%AF%E8%A7%81%E6%80%A7%2F</url>
    <content type="text"><![CDATA[可见性：一个线程对共享变量值的修改，能够及时地被其他线程看到。 共享变量：如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是这几个线程的共享变量。 Java内存模型（JMM） 所有的变量都存储在主内存中 每个线程都有自己独立的工作内存，里面保存该线程使用到的变量的副本（主内存中该变量的一份拷贝） 两条规定 线程对共享变量的所有操作都必须在自己的工作内存中进行，不能直接从主内存中读写 不同线程之间无法直接访问其他线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成 要实现共享变量的可见性，必须保证两点： 线程修改后的共享变量值能够及时从工作内存刷新到主内存中 其他线程能够及时把共享变量的最新值从主内存更新到自己的工作内存中 Java语言层面支持的可见性实现方式： synchronized volatile synchronized能够实现： 原子性 可见性 JMM关于synchronized的两条规定： 线程解锁前，必须把共享变量的最新值刷新到主内存中 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值（注意：加锁与解锁需要是同一把锁） 线程解锁前对共享变量的修改在下次加锁时对其他线程可见 线程执行互斥代码的过程： 获得互斥锁 清空工作内存 从主内存拷贝变量的最新副本到工作内存 执行代码 将更改后的共享变量的值刷新到主内存 释放互斥锁]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java多线程编程核心技术]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FJava%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[第1章 Java多线程技能 在Java中有以下3种方法可以终止正在运行的线程：1）使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。2）使用stop方法强行终止线程，但是不推荐使用这个方法，因为stop和suspend及resume一样，都是作废过期的方法，使用它们可能产生不可预料的结果。3）使用interrupt方法中断线程。调用interrupt()方法仅仅是在当前线程中打了一个停止的标记，并不是真的停止线程。 判断线程是否是停止状态1）this.interrupted()：测试当前线程是否已经中断。2）this.isInterrupted()：测试线程能够是否已经中断。interrupted()方法具有清除状态的功能。 yield方法yield()方法的作用是放弃当前的CPU资源，将他让给其他的任务去占用CPU执行时间。但放弃的时间不确定，有可能刚刚放弃，马上又获得CPU时间片。 线程的优先级在Java中，线程的优先级具有继承性，比如A线程启动B线程，则B线程的优先级与A是一样的。 第2章 对象及变量的并发访问2.1 synchronized同步方法 方法内的变量为线程安全，这是方法内部的变量是私有的特性造成的。 实例变量非线程安全。 在两个线程访问同一个对象中的同步方法时一定是线程安全的。 关键字synchronized取得的锁都是对象锁，而不是把一段代码或方法当作锁。 调用用关键字synchronized声明的方法一定是排队运行的。另外需要牢牢记住“共享”这两个字，只有共享资源的读写访问才需要同步化，如果不是共享资源，那么根本就没有同步的必要。 当A线程调用anyObject对象加入synchronized关键字的X方法时，A线程就获得了X方法锁，更准确地讲，是获得了对象的锁，所以其他线程必须等A线程执行完毕才可以调用X方法，但B线程可以随意调用其他的非synchronized同步方法。当A线程调用anyObject对象加入synchronized关键字的X方法时，A线程就获得了X方法所在对象的锁，所以其他线程必须等A线程执行完毕才可以调用X方法，而B线程如果调用了synchronized关键字的非X方法时，必须等A线程将X方法执行完，也就是释放对象锁后才可以调用。这时A线程已经执行了一个完整的任务，也就是说username和password这两个实例变量已经同时被赋值，不存在脏读的基本环境。 synchronized锁重入：关键字synchronized拥有锁重入的功能，也就是在使用synchronized时，当一个线程得到一个对象锁后，再次请求此对象锁时是可以再次得到该对象的锁的。 当一个线程执行的代码出现异常时，其所持有的锁会自动释放。 同步不可以继承。 2.2 synchronized同步语句块用关键字synchronized声明方法在某些情况下是有弊端的，比如A线程调用同步方法执行一个长时间的任务，那么B线程则必须等待比较长时间。在这样的情况下可以使用synchronized同步语句块来解决。 在使用同步synchronized(this)代码块时需要注意的是，当一个线程访问object的一个synchronized(this)同步代码块时，其他线程对同一个object中所有其他synchronized(this)同步代码块的访问将被阻塞，这说明synchronized使用的“对象监视器”是一个。 和synchronized方法一样，synchronized(this)代码块也是锁定当前对象的。 多个线程调用同一个对象中的不同名称的synchronized同步方法或synchronized(this)同步代码块时，调用的效果就是按顺序执行，也就是同步的，阻塞的。 这说明synchronized同步方法或synchronized(this)同步代码块分别有两种作用。 （1）synchronized同步方法1）对其他synchronized同步方法或synchronized(this)同步代码块调用呈阻塞状态。2）同一时间只有一个线程可以执行synchronized同步方法中的代码。（2）synchronized(this)同步代码块1）对其他synchronized同步方法或synchronized(this)同步代码块调用呈阻塞状态。2）同一时间只有一个线程可以执行synchronized同步代码块中的代码。 如果在一个类中有很多个synchronized方法，这时虽然能实现同步，但会受到阻塞，所以影响运行效率；但如果使用同步代码块锁非this对象，则synchronized(非this)代码块中的程序与同步方法是异步的，不与其他锁this同步方法争抢this锁，则可大大提高运行效率。 使用“synchronized(非this对象x)同步代码块”格式进行同步操作时，对象监视器必须是同一个对象。如果不是同一个对象监视器，运行的结果就是异步调用了，就会交叉运行。 1）当多个线程同时执行synchronized(X){}同步代码块时呈同步效果。2）当其他线程执行x对象中synchronized同步方法时呈同步效果。3）当其他线程执行x对象方法里面的synchronized(this)代码块时也呈现同步效果。但需要注意：如果其他线程调用不加synchronized关键字的方法时，还是异步调用。 关键字volatile的主要作用是使变量在多个线程间可见。 关键字volatile的作用是强制从公共堆栈中取得变量的值，而不是从线程私有数据栈中取得变量的值。 synchronized和volatile比较： 关键字volatile是线程同步的轻量级实现，所以volatile性能肯定比synchronized要好，并且volatile只能修饰于变量，而synchronized可以修饰方法，以及代码块。 多线程访问volatile不会发生阻塞，而synchronized会出现阻塞。 volatile能保证数据的可见性，但不能保证原子性；而synchronized可以保证原子性，也可以间接保证可见性，因为它会将私有内存和公共内存中的数据做同步。 关键字volatile解决的是变量在多个线程之间的可见性；而synchronized关键字解决的是多个线程之间访问资源的同步性。 第3章 线程间通信方法wait()的作用是使当前执行代码的线程进行等待，wait()方法是Object类的方法，该方法用来将当前线程置入“预执行队列”中，并且在wait()所在的代码行处停止执行，直到接到通知或被中断为止。在调用wait()之前，线程必须获得该对象的对象级别锁，即只能在同步方法或同步块中调用wait()方法。在执行wait()方法后，当前线程释放锁。在从wait()返回前，线程与其他线程竞争重新获得锁。如果调用wait()时没有持有适当的锁，则抛出IllegalMonitorStateException，它是RuntimeException的一个子类，因此，不需要try-catch语句进行捕获异常。方法notify()也要在同步方法或同步块中调用，即在调用前，线程也必须获得该对象的对象级别锁。如果调用notify()时灭有持有适当的锁，也会抛出IllegalMonitorStateException。该方法用来通知那些可能等待该对象的对象锁的其他线程，对其发出通知notify，并使它等待获取该对象的对象锁。需要说明的是，在执行notify()方法后，当前线程不会马上释放该对象锁，呈wait状态的线程也并不能马上获取该对象锁，要等到执行notify()方法的线程将程序执行完，也就是退出synchronized代码块后，当前线程才会释放锁，而呈wait状态所在的线程才可以获取该对象锁。当第一个获得了该对象锁的wait线程运行完毕以后，它会释放掉该对象锁，此时如果该对象没有再次使用notify语句，则即便该对象已经空闲，其他wait状态等待的线程由于没有得到该对象的通知，还会继续阻塞在wait状态，直到这个对象发出一个notify或nofityAll。用一句话来总结一下wait和notify：wait使线程停止运行，而notify使停止的线程继续运行。 关键字synchronized可以将任何一个Object对象作为同步对象来看待，而Java为每个Object都实现了wait()和notify()方法，它们必须用在被synchronized同步的Object的临界区内。通过调用wait()方法可以使处于临界区内的线程进入等待状态，同时释放被同步对象的锁。而notify操作可以唤醒一个因调用了wait操作而处于阻塞状态中的线程，使其进入就绪状态。被重新唤醒的线程会试图重新获得临界区的控制权，也就是锁，并继续执行临界区内wait之后的代码。如果发出notify操作时没有处于阻塞状态中的线程，那么该命令会被忽略。wait()方法可以使调用该方法的线程释放共享资源的锁，然后从运行状态退出，进入等待队列，直到被再次唤醒。notify()方法可以随机唤醒等待队列中等待同一共享资源的“一个”线程，并使该线程退出等待队列，进入可运行状态，也就是notify()方法仅通知“一个”线程。notifyAll()方法可以使所有正在等待队列中等待同一共享资源的“全部”线程从等待状态退出，进入可运行状态。此时，优先级最高的那个线程最先执行，但也有可能是随机执行，因为这要取决于JVM虚拟机的实现。 当方法wait()被执行后，锁被自动释放，但执行完notify()方法，锁却不自动释放。 带一个参数的wait(long)方法的功能是等待某一时间内是否有线程对锁进行唤醒，如果超过这个时间则自动唤醒。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java并发编程的艺术]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FJava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[第4章 Java并发编程基础4.1.4 线程的状态Java线程的状态|状态名称|说明||–|–||NEW|初始状态，线程被构建，但是还没有调用start()方法||RUNNABLE|运行状态，Java线程将操作系统的就绪和运行两种状态笼统地称作“运行中”||BLOCKED|阻塞状态，表示线程阻塞于锁||WAITING|等待状态，表示线程进入等待状态，进入该状态表示当前线程需要等待其他线程做出一些特定动作（通知或中断）||TIME_WAITING|超时等待状态，该状态不同于WAITING，它是可以在指定的时间自行返回的||TERMINATED|终止状态，表示当前线程已经执行完毕|线程创建之后，调用start()方法开始运行。当线程执行wait()方法之后，线程进入等待状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态，而超时等待状态相当于在等待状态的基础上增加了超时限制，也就是超时时间到达时将会返回到运行状态。当线程调用同步方法时，在没有获取到锁的情况下，线程将会进入到阻塞状态。线程在执行Runnable的run()方法之后将会进入到终止状态。 4.1.5 Daemon线程Daemon属性需要在启动线程之前设置，不能在启动线程之后设置。 在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SimpleDateFormat非线程安全]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FSimpleDateFormat%E9%9D%9E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%2F</url>
    <content type="text"><![CDATA[类SimpleDateFormat主要负责日期的转换与格式化，但在多线程的环境中，使用此类容易造成数据转换及处理的不准确，因为SimpleDateFormat类并不是线程安全的。 出现异常本示例将实现实用类SimpleDateFormat在多线程环境下处理日期但得出的结果却是错误的情况，这也是在多线程环境开发中容易遇到的问题。 类MyThread.java代码如下：1234567891011121314151617181920212223242526272829303132import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class MyThread extends Thread &#123; private SimpleDateFormat sdf; private String dateString; public MyThread(SimpleDateFormat sdf, String dateString) &#123; super(); this.sdf = sdf; this.dateString = dateString; &#125; @Override public void run() &#123; try &#123; Date dateRef = sdf.parse(dateString); String newDateString = sdf.format(dateRef).toString(); if (!newDateString.equals(dateString)) &#123; System.out.println("ThreadName=" + this.getName() + "报错了 日期字符串：" + dateString + " 转换成的日期为：" + newDateString); &#125; &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行类Test.java代码如下：123456789101112131415161718192021222324import java.text.SimpleDateFormat;import extthread.MyThread;public class Test &#123; public static void main(String[] args) &#123; SimpleDateFormat sdf = new SimpleDateFormat("yyyy-MM-dd"); String[] dateStringArray = new String[] &#123; "2000-01-01", "2000-01-02", "2000-01-03", "2000-01-04", "2000-01-05", "2000-01-06", "2000-01-07", "2000-01-08", "2000-01-09", "2000-01-10" &#125;; MyThread[] threadArray = new MyThread[10]; for (int i = 0; i &lt; 10; i++) &#123; threadArray[i] = new MyThread(sdf, dateStringArray[i]); &#125; for (int i = 0; i &lt; 10; i++) &#123; threadArray[i].start(); &#125; &#125;&#125; 程序运行后的结果如下：1234ThreadName=Thread-4报错了 日期字符串：2000-01-05 转换成的日期为：2000-02-24ThreadName=Thread-5报错了 日期字符串：2000-01-06 转换成的日期为：0005-02-24ThreadName=Thread-8报错了 日期字符串：2000-01-09 转换成的日期为：0001-01-10ThreadName=Thread-9报错了 日期字符串：2000-01-10 转换成的日期为：0001-01-10 从控制台中打印的结果来看，使用单例的SimpleDateFormat类在多线程的环境中处理日期，极易出现日期转换错误的情况。 解决异常方法1类MyThread.java代码如下：1234567891011121314151617181920212223242526272829303132333435import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import tools.DateTools;public class MyThread extends Thread &#123; private SimpleDateFormat sdf; private String dateString; public MyThread(SimpleDateFormat sdf, String dateString) &#123; super(); this.sdf = sdf; this.dateString = dateString; &#125; @Override public void run() &#123; try &#123; Date dateRef = DateTools.parse("yyyy-MM-dd", dateString); String newDateString = DateTools.format("yyyy-MM-dd", dateRef) .toString(); if (!newDateString.equals(dateString)) &#123; System.out.println("ThreadName=" + this.getName() + "报错了 日期字符串：" + dateString + " 转换成的日期为：" + newDateString); &#125; &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 类DateTools.java代码如下：123456789101112public class DateTools &#123; public static Date parse(String formatPattern, String dateString) throws ParseException &#123; return new SimpleDateFormat(formatPattern).parse(dateString); &#125; public static String format(String formatPattern, Date date) &#123; return new SimpleDateFormat(formatPattern).format(date).toString(); &#125;&#125; 运行类Test.java代码与前面一节是一样的。控制台中没有输入任何异常。解决处理错误的原理其实就是创建了多个SimpleDateFormat类的实例。 解决异常方法2ThreadLocal类能使线程绑定到指定的对象。使用该类也可以解决多线程环境下SimpleDateFormat类处理错误的情况。 类MyThread.java代码如下：1234567891011121314151617181920212223242526272829303132333435import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;import tools.DateTools;public class MyThread extends Thread &#123; private SimpleDateFormat sdf; private String dateString; public MyThread(SimpleDateFormat sdf, String dateString) &#123; super(); this.sdf = sdf; this.dateString = dateString; &#125; @Override public void run() &#123; try &#123; Date dateRef = DateTools.getSimpleDateFormat("yyyy-MM-dd").parse(dateString); String newDateString = DateTools.getSimpleDateFormat("yyyy-MM-dd") .format(dateRef).toString(); if (!newDateString.equals(dateString)) &#123; System.out.println("ThreadName=" + this.getName() + "报错了 日期字符串：" + dateString + " 转换成的日期为：" + newDateString); &#125; &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 类DateTools.java代码如下：1234567891011121314151617import java.text.SimpleDateFormat;public class DateTools &#123; private static ThreadLocal&lt;SimpleDateFormat&gt; tl = new ThreadLocal&lt;SimpleDateFormat&gt;(); public static SimpleDateFormat getSimpleDateFormat(String datePattern) &#123; SimpleDateFormat sdf = null; sdf = tl.get(); if (sdf == null) &#123; sdf = new SimpleDateFormat(datePattern); tl.set(sdf); &#125; return sdf; &#125;&#125; 运行类Test.java代码与前面小节是一样的。控制台没有信息被输出，看来运行结果是正确的。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Thread Runnable Callable]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FThread%20Runnable%20Callable%2F</url>
    <content type="text"><![CDATA[Runnable 接口Thread 类 Thread有start()方法，Runnable没有。 开发多线程以实现Runnable接口为主。 实现Runnable接口相比继承Thread类有如下好处： 避免继承的局限，一个类可以继承多个接口。 适合于资源的共享。 接口Callable与线程功能密不可分，但和Runnable的主要区别为： Callable接口的call方法可以有返回值，而Runnable接口的run()方法没有返回值。 Callable接口的call方法可以声明抛出异常，而Runnable接口的run()方法不可以声明抛出异常。执行完Callable接口中的任务后，返回值是通过Future接口进行获得的。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FThreadLocal%2F</url>
    <content type="text"><![CDATA[ThreadLocal使用场合主要解决多线程中数据因并发产生不一致的问题。 ThreadLocal类中一共有4个方法： T get() protected T initialValue() void remove() void set(T value) ThreadLocal设置值有两种方案： Override其initialValue方法 通过set设置 对于多线程资源共享的问题，同步机制采用了“以时间换空间”的方式，比如定义一个static变量，同步访问，而ThreadLocal采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 ThreadLocal建议：ThreadLocal类变量因为本身定位为要被多个线程来访问，它通常被定义为static变量。能够通过值传递的参数，不要通过ThreadLocal存储，以免造成ThreadLocal的滥用。在线程池的情况下，在ThreadLocal业务周期处理完成时，最好显式的调用remove()方法，清空“线程局部变量”中的值。在正常情况下使用ThreadLocal不会造成OOM，弱引用的只是ThreadLocal，保存值依然是强引用，如果ThreadLocal依然被其他对象应用，线程局部变量将无法回收。 ThreadLocal类的作用是为每个线程都创建一个变量副本，每个线程都可以修改自己所拥有的变量副本，而不会影响其他线程的副本。其实这也是解决线程安全的问题的一种方法。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Timer]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2FTimer%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526272829public class Run4 &#123; static int i = 0; static public class MyTask extends TimerTask &#123; @Override public void run() &#123; System.out.println("正常执行了" + i); &#125; &#125; public static void main(String[] args) &#123; while (true) &#123; try &#123; i++; Timer timer = new Timer(); MyTask task = new MyTask(); SimpleDateFormat sdf = new SimpleDateFormat( "yyyy-MM-dd HH:mm:ss"); String dateString = "2014-10-12 09:08:00"; Date dateRef = sdf.parse(dateString); timer.schedule(task, dateRef); timer.cancel(); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 这是因为Timer类中的cancel()方法有时并没有争抢到queue锁，所以TimerTask类中的任务继续正常执行。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[synchronized]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2Fsynchronized%2F</url>
    <content type="text"><![CDATA[Java语言的关键字，当它用来修饰一个方法或者一个代码块的时候，能够保证在同一时刻最多只有一个线程执行该段代码。 当两个并发线程访问同一个对象object中的这个synchronized(this)同步代码块时，一个时间内只能有一个线程得到执行。另一个线程必须等待当前线程执行完这个代码块以后才能执行该代码块。 然而，当一个线程访问object的一个synchronized(this)同步代码块时，另一个线程仍然可以访问该object中的非synchronized(this)同步代码块。 尤其关键的是，当一个线程访问object的一个synchronized(this)同步代码块时，其他线程对object中所有其它synchronized(this)同步代码块的访问将被阻塞。 第三个例子同样适用其它同步代码块。也就是说，当一个线程访问object的一个synchronized(this)同步代码块时，它就获得了这个object的对象锁。结果，其它线程对该object对象所有同步代码部分的访问都被暂时阻塞。 以上规则对其它对象锁同样适用。 synchronized关键字，它包括两种用法：synchronized方法和synchronized块。 同步代码块和同步方法有小小的不同：从尺寸上讲，同步代码块比同步方法小。你可以把同步代码块看成是没上锁房间里的一块用带锁的屏风隔开的空间。同步代码块还可以人为的指定获得某个其它对象的key。就像是指定用哪一把钥匙才能开这个屏风的锁，你可以用本房的钥匙；你也可以指定用另一个房子的钥匙才能开，这样的话，你要跑到另一栋房子那儿把那个钥匙拿来，并用那个房子的钥匙来打开这个房子的带锁的屏风。记住你获得的那另一栋房子的钥匙，并不影响其他人进入那栋房子没有锁的房间。 如果一个类中定义了一个synchronized的静态方法A，也定义了一个synchronized的实例方法B，那么这个类的同一对象Obj在多线程中分别访问A和B两个方法时，不会构成同步，因为它们的锁都不一样。A方法的锁是Obj这个对象，而B的锁是Obj所属的那个Class。 http://www.cnblogs.com/GnagWang/archive/2011/02/27/1966606.html synchronized与static synchronized 的区别：http://www.cnblogs.com/shipengzhi/articles/2223100.html synchronized是Java中的关键字，是一种同步锁。它修饰的对象有以下几种： 修饰一个代码块，被修饰的代码块称为同步语句块，其作用的范围是大括号{}括起来的代码，作用的对象是调用这个代码块的对象； 修饰一个方法，被修饰的方法称为同步方法，其作用的范围是整个方法，作用的对象是调用这个方法的对象； 修改一个静态的方法，其作用的范围是整个静态方法，作用的对象是这个类的所有对象； 修改一个类，其作用的范围是synchronized后面括号括起来的部分，作用的对象是这个类的所有对象。 在用synchronized修饰方法时要注意以下几点： synchronized关键字不能继承。 在定义接口方法时不能使用synchronized关键字。 构造方法不能使用synchronized关键字，但可以使用synchronized代码块来进行同步。 A. 无论synchronized关键字加在方法上还是对象上，如果它作用的对象是非静态的，则它取得的锁是对象；如果synchronized作用的对象是一个静态方法或一个类，则它取得的锁是对类，该类所有的对象同一把锁。B. 每个对象只有一个锁（lock）与之相关联，谁拿到这个锁谁就可以运行它所控制的那段代码。C. 实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[volatile]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2Fvolatile%2F</url>
    <content type="text"><![CDATA[对于可见性，Java提供了volatile关键字来保证可见性。 当一个共享变量被volatile修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。 而普通的共享变量不能保证可见性，因为普通共享变量被修改之后，什么时候被写入主存是不确定的，当其他线程去读取时，此时内存中可能还是原来的旧值，因此无法保证可见性。 另外，通过synchronized和Lock也能够保证可见性，synchronized和Lock能保证同一时刻只有一个线程获取锁然后执行同步代码，并且在释放锁之前会将对变量的修改刷新到主存当中。因此可以保证可见性。 一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义： 1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。2）禁止进行指令重排序。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[yield vs join]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2Fyield%20vs%20join%2F</url>
    <content type="text"><![CDATA[join方法join的作用是使所属的线程对象x正常执行run()方法中的任务，而使当前线程z进行无限期的阻塞，等待线程x销毁后再继续执行线程z后面的代码。方法join具有使线程排队运行的作用，有些类似同步的运行效果。join与synchronized的区别是：join在内部使用wait()方法进行等待，而synchronized关键字使用的是“对象监视器”原理做为同步。 yieldyield()方法的作用是放弃当前的CPU资源，将它让给其他的任务去占用CPU执行时间。]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[单例模式与多线程]]></title>
    <url>%2F2019%2F04%2F19%2F%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文的知识点非常重要，通过单例模式与多线程技术相结合，在这个过程中能发现很多以前未考虑过的情况，一些不良的程序设计方法如果应用在商业项目中，将会遇到非常大的麻烦。本文的案例也将充分说明，线程与某些技术相结合时要考虑的事情有很多。在学习本文时只需要考虑一件事情，那就是：如何使单例模式遇到多线程是安全的、正确的。 在标准的23个设计模式中，单例设计模式在应用中是比较常见的。但在常规的该模式教学资料介绍中，多数并没有结合多线程技术作为参考，这就造成在使用多线程技术的单例模式时会出现一些意想不到的情况，这样的代码如果在生产环境中出现异常，有可能造成灾难性的后果。本文将介绍单例模式结合多线程技术在使用时的相关知识。 1. 立即加载/“饿汉模式”什么是立即加载？立即加载就是使用类的时候已经将对象创建完毕，常见的实现办法就是直接new实例化。而立即加载从中文的语境来看，有“着急”、“急迫”的含义，所以也称为“饿汉模式”。立即加载/“饿汉模式”是在调用方法前，实例已经被创建了，来看一下实现代码。 创建测试用的项目，创建类MyObject.java代码如下：1234567891011121314151617public class MyObject &#123; // 立即加载方式==饿汉模式 private static MyObject myObject = new MyObject(); private MyObject() &#123; &#125; public static MyObject getInstance() &#123; // 此代码版本为立即加载 // 此版本代码的缺点是不能有其它实例变量 // 因为getInstance()方法没有同步 // 所以有可能出现非线程安全问题 return myObject; &#125;&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：12345678910111213141516import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的结果如下：123103154895710315489571031548957 控制台打印的hashCode是同一个只，说明对象是同一个，也就实现了立即加载型单例设计模式。 2. 延迟加载/“懒汉模式”什么是延迟加载？延迟加载就是在调用get()方法时实例才被创建，常见的实现办法就是在get()方法中进行new实例化。而延迟加载从中文的语境来看，是“缓慢”、“不急迫”的含义，所以也称为“懒汉模式”。 2.1 延迟加载/“懒汉模式”解析延迟加载/“懒汉模式”是在调用方法时实例才被创建。一起来看一下实现代码。 创建类MyObject.java代码如下：1234567891011121314151617public class MyObject &#123; private static MyObject myObject; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; // 延迟加载 if (myObject != null) &#123; &#125; else &#123; myObject = new MyObject(); &#125; return myObject; &#125;&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：12345678910import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); t1.start(); &#125;&#125; 程序运行后的效果如下：11031548957 此实验虽然取得一个对象的实例，但如果是在多线程的环境中，就会出现去除多个实例的情况，与单例模式的初衷是相背离的。 2.2 延迟加载/“懒汉模式”的缺点前面两个实验虽然使用“立即加载”和“延迟加载”实现了单例设计模式，但在多线程的环境中，前面“延迟加载”示例中的代码完全就是错误的，根本补鞥呢实现保持单例的状态。来看一下如何在多线程环境中结合“错误的单例模式”创建出“多例”。创建类MyObject.java代码如下：12345678910111213141516171819202122public class MyObject &#123; private static MyObject myObject; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; try &#123; if (myObject != null) &#123; &#125; else &#123; // 模拟在创建对象之前做一些准备性的工作 Thread.sleep(3000); myObject = new MyObject(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return myObject; &#125;&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：12345678910111213141516import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的效果如下：1231866686879033414021770731361 控制台打印出了3中hashCode，说明创建了3个对象，并不是单例的，这就是“错误的单例模式”。如何解决呢？先看一下解决方案。 2.3 延迟加载/“懒汉模式”的解决方案2.3.1 声明synchronized关键字既然多个线程可以同时进入getInstance()方法，那么只需要对getInstance()方法声明synchronized关键字即可。 创建类MyObject.java代码如下：123456789101112131415161718192021222324public class MyObject &#123; private static MyObject myObject; private MyObject() &#123; &#125; // 设置同步方法效率太低了 // 整个方法被上锁 synchronized public static MyObject getInstance() &#123; try &#123; if (myObject != null) &#123; &#125; else &#123; // 模拟在创建对象之前做一些准备性的工作 Thread.sleep(3000); myObject = new MyObject(); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return myObject; &#125;&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：12345678910111213141516import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的结果如下：123531436928531436928531436928 此方法加入同步synchronized关键字得到相同实例的对象，但此种方法的运行效率非常低下，是同步运行的，下一个线程想要取得对象，则必须等上一个线程释放锁之后，才可以继续执行。 2.3.2 尝试同步代码块同步方法是对方法的整体进行持锁，这对运行效率来讲是不利的。改成同步代码块能解决吗？ 创建类MyObject.java代码如下：12345678910111213141516171819202122232425262728public class MyObject &#123; private static MyObject myObject; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; try &#123; // 此种写法等同于： // synchronized public static MyObject getInstance() // 的写法，效率一样很低，全部代码被上锁 synchronized (MyObject.class) &#123; if (myObject != null) &#123; &#125; else &#123; // 模拟在创建对象之前做一些准备性的工作 Thread.sleep(3000); myObject = new MyObject(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return myObject; &#125;&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：12345678910111213141516171819import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); // 此版本代码虽然是正确的 // 但public static MyObject getInstance()方法 // 中的全部代码都是同步的了，这样做有损效率 &#125;&#125; 程序运行后的结果如下：123903341402903341402903341402 此方法加入同步synchronized语句块得到相同实例的对象，但此种方法的运行效率也是非常低的，和synchronized同步方法一样是同步运行的。继续更改代码尝试解决这个缺点。 2.3.3 针对某些重要的代码进行单独的同步同步代码块可以针对某些重要的代码进行单独的同步，而其他的代码则不需要同步。这样在运行时，效率完全可以得到大幅提升。 创建MyObject.java代码如下：123456789101112131415161718192021222324252627public class MyObject &#123; private static MyObject myObject; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; try &#123; if (myObject != null) &#123; &#125; else &#123; // 模拟在创建对象之前做一些准备性的工作 Thread.sleep(3000); // 使用synchronized (MyObject.class) // 虽然部分代码被上锁 // 但还是有非线程安全问题 synchronized (MyObject.class) &#123; myObject = new MyObject(); &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return myObject; &#125;&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：12345678910111213141516import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的结果如下：12390334140217707313611684444739 此方法使同步synchronized语句块，只对实例化对象的关键代码进行同步，从语句的结构上来讲，运行的效率的确得到了提升。但如果是遇到多线程的情况下无法解决得到同一个实例对象的结果。到底如何解决“懒汉模式”遇到多线程的情况呢？ 2.3.4 使用DCL双检查锁机制在最后的步骤中，使用的是DCL双检查锁机制来实现多线程环境中的延迟加载单例设计模式。 创建类MyObject.java代码如下:12345678910111213141516171819202122232425262728293031public class MyObject &#123; private volatile static MyObject myObject; private MyObject() &#123; &#125; // 使用双检测机制来解决问题 // 即保证了不需要同步代码的异步 // 又保证了单例的效果 public static MyObject getInstance() &#123; try &#123; if (myObject != null) &#123; &#125; else &#123; // 模拟在创建对象之前做一些准备性的工作 Thread.sleep(3000); synchronized (MyObject.class) &#123; if (myObject == null) &#123; myObject = new MyObject(); &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return myObject; &#125; // 此版本的代码称为： // 双重检查Double-Check Locking&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：123456789101112131415161718import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的结果如下：123186668687186668687186668687 使用双重检查锁功能，成功地解决了“懒汉模式”遇到多线程的问题。DCL也是大多数多线程结合单例模式使用的解决方案。 3. 使用静态内置类实现单例模式DCL可以解决多线程单例模式的非线程安全问题。当然，使用其他的办法也能达到同样的效果。 创建类MyObject.java代码如下：123456789101112131415public class MyObject &#123; // 内部类方式 private static class MyObjectHandler &#123; private static MyObject myObject = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.myObject; &#125;&#125; 创建线程类MyThread.java代码如下：12345678910import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; System.out.println(MyObject.getInstance().hashCode()); &#125;&#125; 创建运行类Run.java代码如下：12345678910111213141516import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的结果如下：123531436928531436928531436928 4. 序列化与反序列化的单例模式实现静态内置类可以达到线程安全问题，但如果遇到序列化对象时，使用默认的方式运行得到的结果还是多例的。 创建MyObject.java代码如下：12345678910111213141516171819202122232425import java.io.ObjectStreamException;import java.io.Serializable;public class MyObject implements Serializable &#123; private static final long serialVersionUID = 888L; // 内部类方式 private static class MyObjectHandler &#123; private static final MyObject myObject = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.myObject; &#125; protected Object readResolve() throws ObjectStreamException &#123; System.out.println("调用了readResolve方法！"); return MyObjectHandler.myObject; &#125;&#125; 创建业务类SaveAndRead.java代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.FileOutputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import test.MyObject;public class SaveAndRead &#123; public static void main(String[] args) &#123; try &#123; MyObject myObject = MyObject.getInstance(); FileOutputStream fosRef = new FileOutputStream(new File( "myObjectFile.txt")); ObjectOutputStream oosRef = new ObjectOutputStream(fosRef); oosRef.writeObject(myObject); oosRef.close(); fosRef.close(); System.out.println(myObject.hashCode()); &#125; catch (FileNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; try &#123; FileInputStream fisRef = new FileInputStream(new File( "myObjectFile.txt")); ObjectInputStream iosRef = new ObjectInputStream(fisRef); MyObject myObject = (MyObject) iosRef.readObject(); iosRef.close(); fisRef.close(); System.out.println(myObject.hashCode()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 程序运行后的效果如下：12183601924081628611 解决办法就是在反序列化中使用readResolve()方法。去掉如下代码的注释：1234protected Object readResolve() throws ObjectStreamException &#123; System.out.println("调用了readResolve方法！"); return MyObjectHandler.myObject;&#125; 程序运行后的结果如下：1231836019240调用了readResolve方法！1836019240 5. 使用static代码块实现单例模式静态代码块中的代码在使用类的时候就已经执行了，所以可以应用静态代码块的这个特性来实现单例设计模式。 创建MyObject.java代码如下：12345678910111213141516public class MyObject &#123; private static MyObject instance = null; private MyObject() &#123; &#125; static &#123; instance = new MyObject(); &#125; public static MyObject getInstance() &#123; return instance; &#125;&#125; 创建线程类MyThread.java代码如下：1234567891011import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getInstance().hashCode()); &#125; &#125;&#125; 创建运行类Run.java代码如下：12345678910111213141516import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的结果如下：123456789101112131415186668687186668687186668687186668687186668687186668687186668687186668687186668687186668687186668687186668687186668687186668687186668687 6. 使用enum枚举数据类型实现单例模式枚举enum和静态代码块的特性相似，在使用枚举类时，构造方法会被自动调用，也可以应用其这个特性实现单例设计模式。 创建类MyObject.java代码如下：1234567891011121314151617181920212223242526272829import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;public enum MyObject &#123; connectionFactory; private Connection connection; private MyObject() &#123; try &#123; System.out.println("调用了MyObject的构造"); String url = "jdbc:mysql://localhost:3306/test"; String username = "root"; String password = "root"; String driverName = "com.mysql.jdbc.Driver"; Class.forName(driverName); connection = DriverManager.getConnection(url, username, password); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; public Connection getConnection() &#123; return connection; &#125;&#125; 创建线程类MyThread.java代码如下：123456789101112import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.connectionFactory.getConnection() .hashCode()); &#125; &#125;&#125; 创建运行类Run.java代码如下：123456789101112131415import extthread.MyThread;public class Run &#123; public static void main(String[] args) &#123; MyThread t1 = new MyThread(); MyThread t2 = new MyThread(); MyThread t3 = new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 程序运行后的结果如下：12345678910111213141516调用了MyObject的构造482793510482793510482793510482793510482793510482793510482793510482793510482793510482793510482793510482793510482793510482793510482793510 7. 完善使用enum枚举实现单例模式前面一节将枚举类进行暴露，违反了“职责单一原则”，在本节中进行完善。 更改类MyObject.java代码如下：1234567891011121314151617181920212223242526272829303132333435363738import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;public class MyObject &#123; public enum MyEnumSingleton &#123; connectionFactory; private Connection connection; private MyEnumSingleton() &#123; try &#123; System.out.println("创建MyObject对象"); String url = "jdbc:mysql://localhost:3306/test"; String username = "root"; String password = "root"; String driverName = "com.mysql.jdbc.Driver"; Class.forName(driverName); connection = DriverManager.getConnection(url, username, password); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; public Connection getConnection() &#123; return connection; &#125; &#125; public static Connection getConnection() &#123; return MyEnumSingleton.connectionFactory.getConnection(); &#125;&#125; 更改MyThread.java类代码如下：1234567891011import test.MyObject;public class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getConnection().hashCode()); &#125; &#125;&#125; 程序运行的结果如下：12345678910111213141516创建MyObject对象200788226920078822692007882269200788226920078822692007882269200788226920078822692007882269200788226920078822692007882269200788226920078822692007882269]]></content>
      <categories>
        <category>多线程</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[单例模式]]></title>
    <url>%2F2019%2F04%2F19%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例模式应该是设计模式中应用最多的一种设计模式。其看似简单，但是要真正写一个好的单例模式又没那么简单。 最常见的单例模式应该是饿汉式和懒汉式。 饿汉式1234567class Single &#123; private static final Single s = new Single(); private Single() &#123;&#125; public static Single getInstance()&#123; return s; &#125;&#125; 懒汉式12345678910class Single &#123; private static Single s = null; private Single() &#123;&#125; public static Single getInstance()&#123; if(s == null)&#123; //安全问题 s = new Single(); &#125; return s; &#125;&#125; 懒汉式最大的问题就是非线程安全。当有多个线程访问时，如果线程A和线程B先后判断s==null，然后线程A创建Single的一个实例，之后线程B再次创建一个Single的实例，就会导致创建两个Single的实例，变成非单例的。 12345678910class Single &#123; private static Single s = null; private Single() &#123;&#125; public static synchronized Single getInstance()&#123; //每次都判断锁，比较低效 if(s == null)&#123; s = new Single(); &#125; return s; &#125;&#125; 此程序每次都要判断锁，效率比较低。 1234567891011121314class Single &#123; private static Single s = null; private Single() &#123;&#125; public static Single getInstance()&#123; if(s == null) &#123; synchronized(Single.class) &#123; //减少了判断锁的次数 if(s == null)&#123; s = new Single(); &#125; &#125; &#125; return s; &#125;&#125; 此程序减少了判断锁的次数，效率比上个高。]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HashMap和ConcurrentHashMap原理解析]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FHashMap%E5%92%8CConcurrentHashMap%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[前言Map 这样 Key Value 的数据结构在开发中是非常经典的结构，常用于在内存中存放数据。 本篇主要想讨论 ConcurrentHashMap 这样一个并发容器，在正式开始之前我觉得有必要谈谈 HashMap，没有它就不会有后面的 ConcurrentHashMap。 HashMap众所周知 HashMap 底层是基于 数组 + 链表 实现的，不过在 jdk1.7 和 1.8 中具体实现稍有不同。 Base 1.7 先来看看 1.7 中的实现： 图中标记的是 HashMap 中比较核心的几个成员变量，它们的含义分别是： 初始化桶大小，因为底层是数组，所以这是数组默认的大小。 桶最大值。 默认的负载因子（0.75）。 table 真正存放数据的数组。 Map 存放数量的大小。 桶大小，可在初始化时显式指定。 负载因子，可在初始化时显式指定。 重点解释下负载因子： 由于给定的 HashMap 的容量大小是固定的，比如默认初始化： 1234567891011121314151617public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);&#125;public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; init();&#125; 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 因此通常建议能提前预估 HashMap 的大小，尽量的减少扩容带来的性能损耗。 根据代码可以看到其实真正存放数据的是1transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE; 这个数组，那么它又是如何定义的呢？ Entry 是 HashMap 中的一个内部类，从它的成员变量很容易看出： key 就是写入时的键。 value 就是值。 开始的时候就提到 HashMap 是由数组和链表组成，所以这个 next 就是用于实现链表结构。 hash 存放的是当前 key 的 hashcode。 知晓了基本结构，那来看看其中重要的put、get方法： put 方法123456789101112131415161718192021public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); return null;&#125; 判断当前数组是否需要初始化。 如果 key 为空，则 put 一个空值进去。 根据 key 计算出 hashcode。 根据计算出的 hashcode 定位出所在桶。 如果桶是一个链表则需要遍历判断里面的 hashcode、key 是否和传入 key 相等，如果相等则进行覆盖，并返回原来的值。 如果桶是空的，说明当前位置没有数据存入；新增一个 Entry 对象写入当前位置。 1234567891011121314void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 当调用 addEntry 写入 Entry 时需要判断是否需要扩容。 如果需要就进行两倍扩充，并将当前的 key 重新 hash 并定位。 而在 createEntry 中会将当前位置的桶传入到新建的桶中，如果当前桶有值就会在该位置形成链表。 get 方法再来看看 get 函数： 12345678910111213141516171819202122public V get(Object key) &#123; if (key == null) return getForNullKey(); Entry&lt;K,V&gt; entry = getEntry(key); return null == entry ? null : entry.getValue();&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; if (size == 0) &#123; return null; &#125; int hash = (key == null) ? 0 : hash(key); for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; return null;&#125; 首先也是根据 key 计算出 hashcode，然后定位到具体的桶中。 判断该位置是否为链表。 不是链表就根据 key、key 的 hashcode 是否相等来返回值。 为链表则需要遍历直到 key 及 hashcode 相等时候就返回值。 啥都没取到就直接返回 null 。 Base 1.8不知道 1.7 的实现大家看出需要优化的点没有？ 其实一个很明显的地方就是： 当 Hash 冲突严重时，在桶上形成的链表会变的越来越长，这样在查询时的效率就会越来越低；时间复杂度为 O(N)。 因此 1.8 中重点优化了这个查询效率。 1.8 HashMap 结构图： 先来看看几个核心的成员变量： 12345678910111213141516171819202122static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * The maximum capacity, used if a higher value is implicitly specified * by either of the constructors with arguments. * MUST be a power of two &lt;= 1&lt;&lt;30. */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * The load factor used when none specified in constructor. */static final float DEFAULT_LOAD_FACTOR = 0.75f;static final int TREEIFY_THRESHOLD = 8;transient Node&lt;K,V&gt;[] table;/** * Holds cached entrySet(). Note that AbstractMap fields are used * for keySet() and values(). */transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet;/** * The number of key-value mappings contained in this map. */transient int size; 和 1.7 大体上都差不多，还是有几个重要的区别： TREEIFY_THRESHOLD 用于判断是否需要将链表转换为红黑树的阈值。 HashEntry 修改为 Node。 Node 的核心组成其实也是和 1.7 中的 HashEntry 一样，存放的都是 key value hashcode next 等数据。 再来看看核心方法。 put 方法 看似要比 1.7 的复杂，我们一步步拆解： 判断当前桶是否为空，空的就需要初始化（resize 中会判断是否进行初始化）。 根据当前 key 的 hashcode 定位到具体的桶中并判断是否为空，为空表明没有 Hash 冲突就直接在当前位置创建一个新桶即可。 如果当前桶有值（Hash 冲突），那么就要比较当前桶中的 key、key 的 hashcode 与写入的 key 是否相等，相等就赋值给 e，在第 8 步的时候会统一进行赋值及返回。 如果当前桶为红黑树，那就要按照红黑树的方式写入数据。 如果是个链表，就需要将当前的 key、value 封装成一个新节点写入到当前桶的后面（形成链表）。 接着判断当前链表的大小是否大于预设的阈值，大于时就要转换为红黑树。 如果在遍历过程中找到 key 相同时直接退出遍历。 如果 e != null 就相当于存在相同的 key，那就需要将值覆盖。 最后判断是否需要进行扩容。 get 方法123456789101112131415161718192021222324public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; get 方法看起来就要简单许多了。 首先将 key hash 之后取得所定位的桶。 如果桶为空则直接返回 null 。 否则判断桶的第一个位置(有可能是链表、红黑树)的 key 是否为查询的 key，是就直接返回 value。 如果第一个不匹配，则判断它的下一个是红黑树还是链表。 红黑树就按照树的查找方式返回值。 不然就按照链表的方式遍历匹配返回值。 从这两个核心方法（get/put）可以看出 1.8 中对大链表做了优化，修改为红黑树之后查询效率直接提高到了 O(logn)。 但是 HashMap 原有的问题也都存在，比如在并发场景下使用时容易出现死循环。 123456789final HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();for (int i = 0; i &lt; 1000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; map.put(UUID.randomUUID().toString(), ""); &#125; &#125;).start();&#125; 但是为什么呢？简单分析下。 看过上文的还记得在 HashMap 扩容的时候会调用 resize() 方法，就是这里的并发操作容易在一个桶上形成环形链表；这样当获取一个不存在的 key 时，计算出的 index 正好是环形链表的下标就会出现死循环。 如下图： 遍历方式还有一个值得注意的是 HashMap 的遍历方式，通常有以下几种： 1234567891011Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; entryIterator = map.entrySet().iterator();while (entryIterator.hasNext()) &#123; Map.Entry&lt;String, Integer&gt; next = entryIterator.next(); System.out.println("key=" + next.getKey() + " value=" + next.getValue());&#125;Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext()) &#123; String key = iterator.next(); System.out.println("key=" + key + " value=" + map.get(key));&#125; 强烈建议使用第一种 EntrySet 进行遍历。 第一种可以把 key value 同时取出，第二种还得需要通过 key 取一次 value，效率较低。 简单总结下 HashMap：无论是 1.7 还是 1.8 其实都能看出 JDK 没有对它做任何的同步操作，所以并发会出问题，甚至 1.7 中出现死循环导致系统不可用（1.8 已经修复死循环问题）。 因此 JDK 推出了专项专用的 ConcurrentHashMap ，该类位于 java.util.concurrent 包下，专门用于解决并发问题。 坚持看到这里的朋友算是已经把 ConcurrentHashMap 的基础已经打牢了，下面正式开始分析。 ConcurrentHashMapConcurrentHashMap 同样也分为 1.7 、1.8 版，两者在实现上略有不同。 Base 1.7先来看看 1.7 的实现，下面是他的结构图： 如图所示，是由 Segment 数组、HashEntry 组成，和 HashMap 一样，仍然是数组加链表。 它的核心成员变量： 123456/** * Segment 数组，存放数据时首先需要定位到具体的 Segment 中。 */final Segment&lt;K,V&gt;[] segments;transient Set&lt;K&gt; keySet;transient Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet; Segment 是 ConcurrentHashMap 的一个内部类，主要的组成如下： 1234567891011static final class Segment&lt;K, V&gt; extends ReentrantLock implements Serializable &#123; private static final long serialVersionUID = 2249069246763182397L; // 和 HashMap 中的 HashEntry 作用一样，真正存放数据的桶 transient volatile HashEntry&lt;K, V&gt;[] table; transient int count; transient int modCount; transient int threshold; final float loadFactor;&#125; 看看其中 HashEntry 的组成： 和 HashMap 非常类似，唯一的区别就是其中的核心数据如 value ，以及链表都是 volatile 修饰的，保证了获取时的可见性。 原理上来说：ConcurrentHashMap 采用了分段锁技术，其中 Segment 继承于 ReentrantLock。不会像 HashTable 那样不管是 put 还是 get 操作都需要做同步处理，理论上 ConcurrentHashMap 支持 CurrencyLevel (Segment 数组数量)的线程并发。每当一个线程占用锁访问一个 Segment 时，不会影响到其他的 Segment。 下面也来看看核心的 put get 方法。 put 方法1234567891011public V put(K key, V value) &#123; Segment&lt;K, V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; if ((s = (Segment&lt;K, V&gt;) UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); return s.put(key, hash, value, false);&#125; 首先是通过 key 定位到 Segment，之后在对应的 Segment 中进行具体的 put。 123456789101112131415161718192021222324252627282930313233343536373839404142final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; HashEntry&lt;K, V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try &#123; HashEntry&lt;K, V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K, V&gt; first = entryAt(tab, index); for (HashEntry&lt;K, V&gt; e = first; ; ) &#123; if (e != null) &#123; K k; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123; if (node != null) node.setNext(first); else node = new HashEntry&lt;K, V&gt;(hash, key, value, first); int c = count + 1; if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue;&#125; 虽然 HashEntry 中的 value 是用 volatile 关键词修饰的，但是并不能保证并发的原子性，所以 put 操作时仍然需要加锁处理。 首先第一步的时候会尝试获取锁，如果获取失败肯定就有其他线程存在竞争，则利用 scanAndLockForPut() 自旋获取锁。 尝试自旋获取锁。 如果重试的次数达到了 MAX_SCAN_RETRIES 则改为阻塞锁获取，保证能获取成功。 再结合图看看 put 的流程。 将当前 Segment 中的 table 通过 key 的 hashcode 定位到 HashEntry。 遍历该 HashEntry，如果不为空则判断传入的 key 和当前遍历的 key 是否相等，相等则覆盖旧的 value。 不为空则需要新建一个 HashEntry 并加入到 Segment 中，同时会先判断是否需要扩容。 最后会解除在 1 中所获取当前 Segment 的锁。 get 方法1234567891011121314151617public V get(Object key) &#123; Segment&lt;K, V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K, V&gt;[] tab; int h = hash(key); long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K, V&gt;) UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; for (HashEntry&lt;K, V&gt; e = (HashEntry&lt;K, V&gt;) UNSAFE.getObjectVolatile (tab, ((long) (((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; get 逻辑比较简单： 只需要将 Key 通过 Hash 之后定位到具体的 Segment ，再通过遍历定位到具体的元素上。 由于 HashEntry 中的 value 属性是用 volatile 关键词修饰的，保证了内存可见性，所以每次获取时都是最新值。 ConcurrentHashMap 的 get 方法是非常高效的，因为整个过程都不需要加锁。 Base 1.81.7 已经解决了并发问题，并且能支持 N 个 Segment 这么多次数的并发，但依然存在 HashMap 在 1.7 版本中的问题。 那就是查询遍历链表效率太低。 因此 1.8 做了一些数据结构上的调整。 首先来看下底层的组成结构： 看起来是不是和 1.8 HashMap 结构类似？ 其中抛弃了原有的 Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。 也将 1.7 中存放数据的 HashEntry 改为 Node，但作用都是相同的。 其中的 val next 都用了 volatile 修饰，保证了可见性。 put 方法重点来看看 put 函数： 根据 key 计算出 hashcode 。 判断是否需要进行初始化。 f 即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。 如果当前位置的 hashcode == MOVED，则需要进行扩容。 如果都不满足，则利用 synchronized 锁写入数据。 如果数量大于 TREEIFY_THRESHOLD 则要转换为红黑树。 get 方法 根据计算出来的 hashcode 寻址，如果就在桶上那么直接返回值。 如果是红黑树那就按照树的方式获取值。 就不满足那就按照链表的方式遍历获取值。 1.8 在 1.7 的数据结构上做了大的改动，采用红黑树之后可以保证查询效率（O(logn)），甚至取消了 ReentrantLock 改为了 synchronized，这样可以看出在新版的 JDK 中对 synchronized 优化是很到位的。 总结看完了整个 HashMap 和 ConcurrentHashMap 在 1.7 和 1.8 中不同的实现方式相信大家对他们的理解应该会更加到位。 其实这块也是面试的重点内容，通常的套路是： 谈谈你理解的 HashMap，讲讲其中的 get put 过程。 1.8 做了什么优化？ 是线程安全的嘛？ 不安全会导致哪些问题？ 如何解决？有没有线程安全的并发容器？ ConcurrentHashMap 是如何实现的？ 1.7、1.8 实现有何不同？为什么这么做？ 这一串问题相信大家仔细看完都能怼回面试官。 除了面试会问到之外平时的应用其实也蛮多，像之前谈到的 Guava 中 Cache 的实现就是利用 ConcurrentHashMap 的思想。 同时也能学习 JDK 作者大牛们的优化思路以及并发解决方案。]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HashSet LinkedHashSet TreeSet对比]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FHashSet%20LinkedHashSet%20TreeSet%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[\ HashSet LinkedHashSet TreeSet 内部工作机制 HashSet内部使用HashMap存储元素 LinkedHashSet内部使用LinkedHashMap存储元素 TreeSet内部使用TreeMap存储元素 元素顺序 HashSet不维护元素的顺序 LinkedHashSet维护元素的插入顺序，元素按插入顺序排序 TreeSet根据提供的Comparator排序。如果没有Comparator，元素按自然升序排序 性能 HashSet性能比LinkedHashSet和TreeSet都好 LinkedHashSet的性能介于HashSet和TreeSet之间。不过和HashSet接近，只是稍微慢一点。因为它使用LinkedList来维护元素的插入顺序 TreeSet在三者中性能最差，因为在插入和删除操作之后会对元素排序 插入、删除、查找操作 时间复杂度：O(1) 时间复杂度：O(1) 时间复杂度：O(log(n)) 如何比较元素 HashSet使用equals()和hashCode()比较元素是否重复 LinkedHashSet也使用equals()和hashCode()比较元素是否重复 TreeSet使用compare()或compareTo()方法比较元素是否重复，而不是使用equals()和hashCode() null元素 HashSet允许有一个null元素 LinkedHashSet允许有一个null元素 TreeSet不允许有null元素 内存占用 HashSet需要最少的内存，因为它使用HashMap存储元素 LinkedHashSet需要的内存比HashSet多，因为它需要维护LinkedList而且使用HashMap存储元素 TreeSet需要的内存也比HashSet多，因为它需要维护Comparator对元素排序并使用TreeMap存储元素 何时使用 不需要元素的顺序时使用HashSet 如果需要维护插入顺序则使用LinkedHashSet 如果需要根据某些Comparator排序则使用TreeSet HashSet、LinkedHashSet和TreeSet的相同点： 不允许重复元素 非同步（线程不安全） 继承Cloneable和Serializable类 http://javaconceptoftheday.com/hashset-vs-linkedhashset-vs-treeset-in-java/]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2019%2F04%2F19%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[简单工厂模式 工厂方法模式 抽象工厂模式 代理模式]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HashMap和Hashtable源码学习和面试总结]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FHashMap%E5%92%8CHashtable%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0%E5%92%8C%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[如果说Java的HashMap是数组+链表，那么JDK 8之后就是数组+链表+红黑树组成了HashMap。 在之前谈过，如果hash算法不好，会使得hash表蜕化为顺序查找，即使负载因子和hash算法优化再多，也无法避免出现链表过长的情景（这个概论虽然很低），于是在JDK1.8中，对HashMap做了优化，引入红黑树。具体原理就是当hash表中每个桶附带的链表长度默认超过8时，链表就转换为红黑树结构，提高HashMap的性能，因为红黑树的增删改是O(logn)，而不是O(n)。 红黑树的具体原理和实现以后再总结。 主要看put方法实现123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 封装了一个final方法，里面用到一个常量，具体用处看源码： 1static final int TREEIFY_THRESHOLD = 8; 下面是具体源代码注释： 123456789101112131415161718192021222324252627282930313233343536373839404142final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 首先判断hash表是否是空的，如果空，则resize扩容 n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) // 通过key计算得到hash表下标，如果下标处为null，就新建链表头结点，在方法最后插入即可 tab[i] = newNode(hash, key, value, null); else &#123; // 如果下标处已经存在节点，则进入到这里 Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 先看hash表该处的头结点是否和key一样（hashcode和equals比较），一样就更新 e = p; else if (p instanceof TreeNode) // hash表头结点和key不一样，则判断节点是不是红黑树，是红黑树就按照红黑树处理 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 如果不是红黑树，则按照之前的HashMap原理处理 for (int binCount = 0; ; ++binCount) &#123; // 遍历链表 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st (原jdk注释) 显然当链表长度大于等于7的时候，也就是说大于8的话，就转化为红黑树结构，针对红黑树进行插入（logn复杂度） treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) // 如果超过容量，即扩容 resize(); afterNodeInsertion(evict); return null; &#125; resize是新的扩容方法，之前谈过，扩容原理是使用新的（2倍旧长度）的数组代替，把旧数组的内容放到新数组，需要重新计算hash和hash表的位置，非常耗时，但是自从 JDK 1.8 对HashMap引入了红黑树，它和之前的扩容方法相比有了改进。 扩容方法的改进12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) // 如果长度没有超过最大值，则扩容为2倍的关系 newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 进行新旧元素的转移过程 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order（原注释） 如果不是红黑树的情况这里改进了，没有rehash的过程，如下分别记录链表的头尾 Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 因为有这样一个特点：比如hash表的长度是16，那么15对应二进制是： 0000 0000， 0000 0000， 0000 0000， 0000 1111 = 15 扩容之前有两个key，分别是k1和k2： k1的hash： 0000 0000， 0000 0000， 0000 0000， 0000 1111 = 15 k2的hash： 0000 0000， 0000 0000， 0000 0000， 0001 1111 = 15 hash值和15模得到： k1：0000 0000， 0000 0000， 0000 0000， 0000 1111 = 15 k2：0000 0000， 0000 0000， 0000 0000， 0000 1111 = 15 扩容之后表长对应为32，则31二进制： 0000 0000， 0000 0000， 0000 0000， 0001 1111 = 31 重新hash之后得到： k1：0000 0000， 0000 0000， 0000 0000， 0000 1111 = 15 k2：0000 0000， 0000 0000， 0000 0000， 0001 1111 = 31 = 15 + 16 观察发现：如果扩容后新增的位是0，那么rehash索引不变，否则才会改变，并且变为原来的索引+旧hash表的长度，故我们只需看原hash表长新增的bit是1还是0，如果是0，索引不变，如果是1，索引变成原索引+旧表长，根本不用像JDK 7 那样rehash，省去了重新计算hash值的时间，而且新增的bit是0还是1可以认为是随机的，因此resize的过程，还能均匀的把之前的冲突节点分散。 故JDK 8对HashMap的优化是非常到位的。 如下是之前整理的旧hash的实现机制和原理，并和jdk古老的Hashtable做了比较。 整理jdk 1.8之前的HashMap实现： Java集合概述 HashMap介绍 HashMap源码学习 关于HashMap的几个经典问题 Hashtable介绍和源码学习 HashMap 和 Hashtable比较 先上图 Set和List接口是Collection接口的子接口，分别代表无序集合和有序集合，Queue是Java提供的队列实现。 Map用于保存具有key-value映射关系的数据。 Java 中有四种常见的Map实现——HashMap，TreeMap，Hashtable和LinkedHashMap。 HashMap就是一张hash表，键和值都没有排序。 TreeMap以红黑树结构为基础，键值可以设置按某种顺序排列。 LinkedHashMap保存了插入时的顺序。 Hashtable是同步的(而HashMap是不同步的)。所以如果在线程安全的环境下应该多使用HashMap，而不是Hashtable，因为Hashtable对同步有额外的开销，不过JDK 5之后的版本可以使用conncurrentHashMap代替Hashtable。 本文重点总结HashMap，HashMap是基于哈希表实现的，每一个元素是一个key-value对，其内部通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。 HashMap是非线程安全的，只用于单线程环境下，多线程环境下可以采用concurrent并发包下的concurrentHashMap。 HashMap 实现了Serializable接口，因此它支持序列化。 HashMap还实现了Cloneable接口，故能被克隆。 关于HashMap的用法，这里就不再赘述了，只说原理和一些注意点。 HashMap的存储结构 紫色部分即代表哈希表本身（其实是一个数组），数组的每个元素都是一个单链表的头节点，链表是用来解决hash地址冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中保存。 HashMap有四个构造方法，方法中有两个很重要的参数：初始容量和加载因子这两个参数是影响HashMap性能的重要参数，其中容量表示哈希表中槽的数量（即哈希数组的长度），初始容量是创建哈希表时的容量（默认为16），加载因子是哈希表当前key的数量和容量的比值，当哈希表中的条目数超出了加载因子与当前容量的乘积时，则要对该哈希表提前进行 resize 操作（即扩容）。如果加载因子越大，对空间的利用更充分，但是查找效率会降低（链表长度会越来越长）；如果加载因子太小，那么表中的数据将过于稀疏（很多空间还没用，就开始扩容了），严重浪费。 JDK开发者规定的默认加载因子为0.75，因为这是一个比较理想的值。另外，无论指定初始容量为多少，构造方法都会将实际容量设为不小于指定容量的2的幂次方，且最大值不能超过2的30次方。 重点分析HashMap中用的最多的两个方法put和get的源码12345678910111213141516171819202122232425// 获取key对应的valuepublic V get(Object key) &#123; if (key == null) return getForNullKey(); // 获取key的hash值 int hash = hash(key.hashCode()); // 在“该hash值对应的链表”上查找“键值等于key”的元素 for (Entry&lt;K, V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; // 判断key是否相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; // 没找到则返回null return null;&#125;// 获取“key为null”的元素的值，HashMap将“key为null”的元素存储在table[0]位置，但不一定是该链表的第一个位置！private V getForNullKey() &#123; for (Entry&lt;K, V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null;&#125; 首先，如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中，当然不一定是存放在头结点table[0]中。如果key不为null，则先求的key的hash值，根据hash值找到在table中的索引，在该索引对应的单链表中查找是否有键值对的key与目标key相等，有就返回对应的value，没有则返回null。 12345678910111213141516171819202122232425// 将“key-value”添加到HashMap中public V put(K key, V value) &#123; // 若“key为null”，则将该键值对添加到table[0]中。 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值，然后将其添加到该哈希值对应的链表中。 int hash = hash(key.hashCode()); int i = indexFor(hash, table.length); for (Entry&lt;K, V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 若“该key”对应的键值对已经存在，则用新的value取代旧的value。然后退出！ if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 若“该key”对应的键值对不存在，则将“key-value”添加到table中 modCount++; // 将key-value添加到table[i]处 addEntry(hash, key, value, i); return null;&#125; 如果key为null，则将其添加到table[0]对应的链表中，如果key不为null，则同样先求出key的hash值，根据hash值得出在table中的索引，而后遍历对应的单链表，如果单链表中存在与目标key相等的键值对，则将新的value覆盖旧的value，且将旧的value返回，如果找不到与目标key相等的键值对，或者该单链表为空，则将该键值对插入到单链表的头结点位置（每次新插入的节点都是放在头结点的位置），该操作是有addEntry方法实现的，它的源码如下： 1234567891011// 新增Entry。将“key-value”插入指定位置，bucketIndex是位置索引。void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 保存“bucketIndex”位置的值到“e”中 Entry&lt;K, V&gt; e = table[bucketIndex]; // 设置“bucketIndex”位置的元素为“新Entry”， // 设置“e”为“新Entry的下一个节点” table[bucketIndex] = new Entry&lt;K, V&gt;(hash, key, value, e); // 若HashMap的实际大小 不小于 “阈值”，则调整HashMap的大小 if (size++ &gt;= threshold) resize(2 * table.length);&#125; 注意这里倒数第三行的构造方法，将key-value键值对赋给table[bucketIndex]，并将其next指向元素e，这便将key-value放到了头结点中，并将之前的头结点接在了它的后面。该方法也说明，每次put键值对的时候，总是将新的该键值对放在table[bucketIndex]处（即头结点处）。两外注意最后两行代码，每次加入键值对时，都要判断当前已用的槽的数目是否大于等于阀值（容量*加载因子），如果大于等于，则进行扩容，将容量扩为原来容量的2倍。 重点来分析下求hash值和索引值的方法，这两个方法便是HashMap设计的最为核心的部分，二者结合能保证哈希表中的元素尽可能均匀地散列。由hash值找到对应索引的方法如下123static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 因为容量初始还是设定都会转化为2的幂次。故可以使用高效的位与运算替代模运算。下面会解释原因。 计算hash值的方法如下1234static int hash(int h) &#123; h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; JDK 的 HashMap 使用了一个 hash 方法对hash值使用位的操作，使hash值的计算效率很高。为什么这样做？主要是因为如果直接使用hashcode值，那么这是一个int值（8个16进制数，共32位），int值的范围正负21亿多，但是hash表没有那么长，一般比如初始16，自然散列地址需要对hash表长度取模运算，得到的余数才是地址下标。假设某个key的hashcode是0AAA0000，hash数组长默认16，如果不经过hash函数处理，该键值对会被存放在hash数组中下标为0处，因为0AAA0000 &amp; (16-1) = 0。过了一会儿又存储另外一个键值对，其key的hashcode是0BBB0000，得到数组下标依然是0，这就说明这是个实现得很差的hash算法，因为hashcode的1位全集中在前16位了，导致算出来的数组下标一直是0。于是明明key相差很大的键值对，却存放在了同一个链表里，导致以后查询起来比较慢（蜕化为了顺序查找）。故JDK的设计者使用hash函数的若干次的移位、异或操作，把hashcode的“1位”变得“松散”，非常巧妙。 下面是几个常见的面试题说下HashMap的 扩容机制？前面说了，hashmap的构造器里指明了两个对于理解HashMap比较重要的两个参数 int initialCapacity，float loadFactor，这两个参数会影响HashMap效率，HashMap底层采用的散列数组实现，利用initialCapacity这个参数我们可以设置这个数组的大小，也就是散列桶的数量，但是如果需要Map的数据过多，在不断的add之后，这些桶可能都会被占满，这是有两种策略，一种是不改变Capacity，因为即使桶占满了，我们还是可以利用每个桶附带的链表增加元素。但是这有个缺点，此时HaspMap就退化成为了LinkedList，使get和put方法的时间开销上升，这是就要采用另一种方法：增加Hash桶的数量，这样get和put的时间开销又回退到近于常数复杂度上。Hashmap就是采用的该方法。 关于扩容。看HashMap的扩容方法，resize方法，它的源码如下：12345678910111213141516// 重新调整HashMap的大小，newCapacity是调整后的单位void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 新建一个HashMap，将“旧HashMap”的全部元素添加到“新HashMap”中， // 然后，将“新HashMap”赋值给“旧HashMap”。 Entry[] newTable = new Entry[newCapacity]; transfer(newTable); table = newTable; threshold = (int) (newCapacity * loadFactor);&#125; 很明显，是从新建了一个HashMap的底层数组，长度为原来的两倍，而后调用transfer方法，将旧HashMap的全部元素添加到新的HashMap中（要重新计算元素在新的数组中的索引位置）。 transfer方法的源码如下： 123456789101112131415161718// 将HashMap中的全部元素都添加到newTable中void transfer(Entry[] newTable) &#123; Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; Entry&lt;K, V&gt; e = src[j]; if (e != null) &#123; src[j] = null; do &#123; Entry&lt;K, V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; while (e != null); &#125; &#125;&#125; 很明显，扩容是一个相当耗时的操作，因为它需要重新计算这些元素在新的数组中的位置并进行复制处理。因此，我们在用HashMap时，最好能提前预估下HashMap中元素的个数，这样有助于提高HashMap的性能。 HashMap什么时候需要增加容量呢？因为效率问题，JDK采用预处理法，这时前面说的loadFactor就派上了用场，当size &gt; initialCapacity * loadFactor，HashMap内部resize方法就被调用，使得重新扩充hash桶的数量，在目前的实现中，是增加一倍，这样就保证当你真正想put新的元素时效率不会明显下降。所以一般情况下HashMap并不存在键值放满的情况。当然并不排除极端情况，比如设置的JVM内存用完了，或者这个HashMap的Capacity已经达到了MAXIMUM_CAPACITY（目前的实现是2^30）。 initialCapacity和loadFactor参数设什么样的值好呢？initialCapacity的默认值是16，有些人可能会想如果内存足够，是不是可以将initialCapacity设大一些，即使用不了这么大，就可避免扩容导致的效率的下降，反正无论initialCapacity大小，我们使用的get和put方法都是常数复杂度的。这么说没什么不对，但是可能会忽略一点，实际的程序可能不仅仅使用get和put方法，也有可能使用迭代器，如initialCapacity容量较大，那么会使迭代器效率降低。所以理想的情况还是在使用HashMap前估计一下数据量。 加载因子默认值是0.75，是JDK权衡时间和空间效率之后得到的一个相对优良的数值。如果这个值过大，虽然空间利用率是高了，但是对于HashMap中的一些方法的效率就下降了，包括get和put方法，会导致每个hash桶所附加的链表增长，影响存取效率。如果比较小，除了导致空间利用率较低外没有什么坏处，只要有的是内存，毕竟现在大多数人把时间看的比空间重要。但是实际中还是很少有人会将这个值设置的低于0.5。 HashMap的key和value都能为null么？如果k能为null，那么它是怎么样查找值的？如果key为null，则直接从哈希表的第一个位置table[0]对应的链表上查找。记住，key为null的键值对永远都放在以table[0]为头结点的链表中。 HashMap中put值的时候如果发生了冲突，是怎么处理的？JDK使用了链地址法，hash表的每个元素又分别链接着一个单链表，元素为头结点，如果不同的key映射到了相同的下标，那么就使用头插法，插入到该元素对应的链表。 HashMap的key是如何散列到hash表的？相比较Hashtable有什么改进？我们一般对哈希表的散列很自然地会想到用hash值对length取模（即除留余数法），Hashtable就是这样实现的，这种方法基本能保证元素在哈希表中散列的比较均匀，但取模会用到除法运算，效率很低，且Hashtable直接使用了hashcode值，没有重新计算。 HashMap中则通过 h&amp;(length-1) 的方法来代替取模，其中h是key的hash值，同样实现了均匀的散列，但效率要高很多，这也是HashMap对Hashtable的一个改进。 接下来，我们分析下为什么哈希表的容量一定要是2的整数次幂。 首先，length为2的整数次幂的话，h&amp;(length-1) 在数学上就相当于对length取模，这样便保证了散列的均匀，同时也提升了效率； 其次，length为2的整数次幂的话，则一定为偶数，那么 length-1 一定为奇数，奇数的二进制的最后一位是1，这样便保证了 h&amp;(length-1) 的最后一位可能为0，也可能为1（这取决于h的值），即与后的结果可能为偶数，也可能为奇数，这样便可以保证散列的均匀，而如果length为奇数的话，很明显 length-1 为偶数，它的最后一位是0，这样 h&amp;(length-1) 的最后一位肯定为0，即只能为偶数，这样导致了任何hash值都只会被散列到数组的偶数下标位置上，浪费了一半的空间，因此length取2的整数次幂，是为了使不同hash值发生碰撞的概率较小，这样就能使元素在哈希表中均匀地散列。 作为对比，再讨论一下HashtableHashtable同样是基于哈希表实现的，其实类似HashMap，只不过有些区别，Hashtable同样每个元素是一个key-value对，其内部也是通过单链表解决冲突问题，容量不足（超过了阀值）时，同样会自动增长。 Hashtable比较古老， 是JDK1.0就引入的类，而HashMap 是 1.2 引进的 Map 的一个实现。 Hashtable是线程安全的，能用于多线程环境中。Hashtable同样也实现了Serializable接口，支持序列化，也实现了Cloneable接口，能被克隆。 Hashtable继承于Dictionary类，实现了Map接口。Dictionary是声明了操作”键值对”函数接口的抽象类。 有一点注意，Hashtable除了线程安全之外（其实是直接在方法上增加了synchronized关键字，比较古老，落后，低效的同步方式），还有就是它的key、value都不为null。另外Hashtable 也有 初始容量 和 加载因子。 123public Hashtable() &#123; this(11, 0.75f);&#125; 默认加载因子也是 0.75，Hashtable在不指定容量的情况下的默认容量为11，而HashMap为16，Hashtable不要求底层数组的容量一定要为2的整数次幂，而HashMap则要求一定为2的整数次幂。因为Hashtable是直接使用除留余数法定位地址。且Hashtable计算hash值，直接用key的hashCode()。 还要注意：前面说了Hashtable中key和value都不允许为null，而HashMap中key和value都允许为null（key只能有一个为null，而value则可以有多个为null）。但如在Hashtable中有类似put(null,null)的操作，编译同样可以通过，因为key和value都是Object类型，但运行时会抛出NullPointerException异常，这是JDK的规范规定的。 最后针对扩容：Hashtable扩容时，将容量变为原来的2倍加1，而HashMap扩容时，将容量变为原来的2倍。 下面是几个常见的笔试，面试题Hashtable和HashMap的区别有哪些？HashMap和Hashtable都实现了Map接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性，同步(synchronization)，以及速度。 理解HashMap是Hashtable的轻量级实现（非线程安全的实现，Hashtable是非轻量级，线程安全的），都实现Map接口，主要区别在于： 由于HashMap非线程安全，在只有一个线程访问的情况下，效率要高于Hashtable。 HashMap允许将null作为一个entry的key或者value，而Hashtable不允许。 HashMap把Hashtable的contains方法去掉了，改成containsValue和containsKey。因为contains方法容易让人引起误解。 Hashtable继承自陈旧的Dictionary类，而HashMap是Java1.2引进的Map 的一个实现。 Hashtable和HashMap扩容的方法不一样，Hashtable中hash数组默认大小11，扩容方式是 old*2+1。HashMap中hash数组的默认大小是16，而且一定是2的指数，增加为原来的2倍，没有加1。 两者通过hash值散列到hash表的算法不一样，HashTbale是古老的除留余数法，直接使用hashcode，而后者是强制容量为2的幂，重新根据hashcode计算hash值，在使用hash 位与 （hash表长度 – 1），也等价取模，但更加高效，取得的位置更加分散，偶数，奇数保证了都会分散到。前者就不能保证。 另一个区别是HashMap的迭代器(Iterator)是fail-fast迭代器，而Hashtable的enumerator迭代器不是fail-fast的。所以当有其它线程改变了HashMap的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的remove()方法移除元素则不会抛出ConcurrentModificationException异常。但这并不是一个一定发生的行为，要看JVM。这条同样也是Enumeration和Iterator的区别。 fail-fast和iterator迭代器相关。如果某个集合对象创建了Iterator或者ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出ConcurrentModificationException异常。但其它线程可以通过set()方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用set()方法，将会抛出IllegalArgumentException异常。 结构上的更改指的是删除或者插入一个元素，这样会影响到map的结构。 该条说白了就是在使用迭代器的过程中有其他线程在结构上修改了map，那么将抛出ConcurrentModificationException，这就是所谓fail-fast策略。 为什么HashMap是线程不安全的，实际会如何体现？第一，如果多个线程同时使用put方法添加元素 假设正好存在两个put的key发生了碰撞(hash值一样)，那么根据HashMap的实现，这两个key会添加到数组的同一个位置，这样最终就会发生其中一个线程的put的数据被覆盖。 第二，如果多个线程同时检测到元素个数超过数组大小*loadFactor 这样会发生多个线程同时对hash数组进行扩容，都在重新计算元素位置以及复制数据，但是最终只有一个线程扩容后的数组会赋给table，也就是说其他线程的都会丢失，并且各自线程put的数据也丢失。且会引起死循环的错误。 具体细节上的原因，可以参考：不正当使用HashMap导致cpu 100%的问题追究 能否让HashMap实现线程安全，如何做？1、直接使用Hashtable，但是当一个线程访问Hashtable的同步方法时，其他线程如果也要访问同步方法，会被阻塞住。举个例子，当一个线程使用put方法时，另一个线程不但不可以使用put方法，连get方法都不可以，效率很低，现在基本不会选择它了。 2、HashMap可以通过下面的语句进行同步： Collections.synchronizeMap(hashMap); 3、直接使用JDK 5 之后的 ConcurrentHashMap，如果使用Java 5或以上的话，请使用ConcurrentHashMap。 Collections.synchronizeMap(hashMap);又是如何保证了HashMap线程安全？直接分析源码吧 View Code 从源码中看出 synchronizedMap()方法返回一个SynchronizedMap类的对象，而在SynchronizedMap类中使用了synchronized来保证对Map的操作是线程安全的，故效率其实也不高。 为什么Hashtable的默认大小和HashMap不一样？前面分析了，Hashtable 的扩容方法是乘2再+1，不是简单的乘2，故hashtable保证了容量永远是奇数，结合之前分析HashMap的重算hash值的逻辑，就明白了，因为在数据分布在等差数据集合(如偶数)上时，如果公差与桶容量有公约数 n，则至少有(n-1)/n 数量的桶是利用不到的，故之前的HashMap会在取模（使用位与运算代替）哈希前先做一次哈希运算，调整hash值。这里Hashtable比较古老，直接使用了除留余数法，那么就需要设置容量起码不是偶数（除（近似）质数求余的分散效果好）。而JDK开发者选了11。 JDK 8对HashMap有了什么改进？说说你对红黑树的理解？参考更新的jdk 8对HashMap的的改进部分整理，并且还能引申出高级数据结构——红黑树，这又能引出很多问题……学无止境啊！ 临时小结：感觉针对Java的HashMap和Hashtable面试，或者理解，到这里就可以了，具体就是多写代码实践。]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HashSet添加null报空指针异常]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FHashSet%E6%B7%BB%E5%8A%A0null%E6%8A%A5%E7%A9%BA%E6%8C%87%E9%92%88%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[HashSet添加null报空指针异常。12345678910111213141516171819202122232425public class TestSet &#123; public static void main(String[] args) &#123; Set&lt;Integer&gt; hashSet = new HashSet&lt;Integer&gt;(); hashSet.add(2); hashSet.add(5); hashSet.add(1); hashSet.add(null); // will throw null pointer hashSet.add(999); hashSet.add(10); hashSet.add(10); hashSet.add(11); hashSet.add(9); hashSet.add(10); hashSet.add(000); hashSet.add(999); hashSet.add(0); Iterator&lt;Integer&gt; it = hashSet.iterator(); while(it.hasNext())&#123; int i = it.next(); System.out.print(i+" "); &#125; &#125;&#125; Java集合类不能存储基本数据类型（如果要存储基本数据类型可以使用第三方API，如Torve），所以当执行如下代码：12hashSet.add(2);hashSet.add(5); 实际上执行的是：12hashSet.add(new Integer(2));hashSet.add(new Integer(5)); 向HashSet中添加null值并不是产生空指针异常的原因，HashSet中是可以添加null值的。NPE是因为在遍历set时需要把值拆箱为基本数据类型：1234while(it.hasNext())&#123; int i = it.next(); System.out.print(i+" ");&#125; 如果值为null，JVM试图把它拆箱为基本数据类型就会导致NPE。装箱相当于执行Integer.valueOf(100)。拆箱相当于执行i.intValue()。此时相当于null调用intValue()方法，所以报NPE。可以把代码修改为：1234while(it.hasNext())&#123; final Integer i = it.next(); System.out.print(i+" ");&#125; http://stackoverflow.com/questions/14774721/why-set-interface-does-not-allow-null-elements]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[List Set Map之间的不同]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FList%20Set%20Map%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%8D%E5%90%8C%2F</url>
    <content type="text"><![CDATA[它们都继承自Collection类。 10点不同： 序号 属性 java.util.List java.util.Set java.util.Map 1 重复元素 List允许存储重复元素。 Set不允许存储重复元素。 Map以键值对形式存储数据，key不允许重复，value可以重复。 2 插入顺序 List以插入顺序存储元素。 大部分Set实现类不维护插入顺序。HashSet不维护插入顺序。LinkedHashSet维护插入顺序。TreeSet以自然顺序排序。 大部分Map实现类不维护插入顺序。HashMap不维护插入顺序。LinkedHashMap维护key的插入顺序。TreeMap以key的自然顺序排序。 3 null keys List允许存储多个null值。 大部分Set实现类允许存储一个null值。TreeSet和ConcurrentSkipListSet 不允许存储null值。 Map实现类：HashMap允许一个null键和多个null值。LinkedHashMap允许一个null键和多个null值。TreeMap不允许null键，允许多个null值。Hashtable不允许null键和null值。ConcurrentHashMap不允许null键和null值。ConcurrentSkipListMap不允许null键和null值。 4 获取指定索引的元素 List实现类提供了get方法获取指定索引的元素。get方法直接通过指定索引获取元素，因此时间复杂度为O(1)。 Set实现类不提供此类方法。 Map实现类不提供此类方法。 5 子类 ArrayListLinkedListVectorCopyOnWriteArrayList HashSetCopyOnWriteArraySet LinkedHashSetTreeSetConcurrentSkipListSetEnumSet HashMapHashtable ConcurrentHashMapLinkedHashMapTreeMapConcurrentSkipListMap IdentityHashMapWeakHashMapEnumMap 6 listIterator listIterator方法遍历元素并返回ListIterator对象。listIterator相对iterator方法提供了额外的方法：hasPrevious(), previous(), nextIndex(), previousIndex(), add(E element), set(E element)。 Set没有提供类似listIterator的方法，只是简单返回Iterator。 Map提供了三种iterator：map.keySet().iterator()遍历key并返回Iterator对象。map.values().iterator()遍历value并返回Iterator对象。map.entrySet().iterator()遍历key和value并返回Map.Entry对象。 7 结构和调整大小 List是可调整大小的数组。 Set使用Map实现。因此Set的结构和调整大小与Map相同。 Map使用哈希技术存储键值对。 8 基于结构/随机访问的索引 ArrayList使用基于索引的数组实现，因此提供了随机访问。LinkedList不是基于索引的结构。 Set不是基于索引的结构。 Map不是基于索引的结构。 9 非同步的子类 ArrayListLinkedList HashSetLinkedHashSetTreeSetEnumSet HashMapLinkedHashMapTreeMapIdentityHashMapWeakHashMapEnumMap 10 同步的子类 VectorCopyOnWriteArrayList CopyOnWriteArraySet ConcurrentSkipListSet HashtableConcurrentHashMapConcurrentSkipListMap http://www.javamadesoeasy.com/2016/02/difference-between-list-set-and-map-in.html]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Map遍历]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FMap%E9%81%8D%E5%8E%86%2F</url>
    <content type="text"><![CDATA[遍历Map常用的方式有四种。 方式一：这是最常见的并且在大多数情况下也是最可取的遍历方式。在键值都需要时使用。1234Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();for (Map.Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; System.out.println(&quot;Key = &quot; + entry.getKey() + &quot;, Value = &quot; + entry.getValue());&#125; 方式二：在for-each循环中遍历keys或values。如果只需要map中的键或者值，你可以通过keySet或values来实现遍历，而不是用entrySet。 123456789Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();// 遍历map中的键 for (Integer key : map.keySet()) &#123; System.out.println(&quot;Key = &quot; + key);&#125;// 遍历map中的值 for (Integer value : map.values()) &#123; System.out.println(&quot;Value = &quot; + value);&#125; 该方法比entrySet遍历在性能上稍好（快了10%），而且代码更加干净。 方式三：使用Iterator遍历使用泛型123456Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; entries = map.entrySet().iterator();while (entries.hasNext()) &#123; Map.Entry&lt;Integer, Integer&gt; entry = entries.next(); System.out.println(&quot;Key = &quot; + entry.getKey() + &quot;, Value = &quot; + entry.getValue());&#125; 不使用泛型12345678Map map = new HashMap();Iterator entries = map.entrySet().iterator();while (entries.hasNext()) &#123; Map.Entry entry = (Map.Entry) entries.next(); Integer key = (Integer) entry.getKey(); Integer value = (Integer) entry.getValue(); System.out.println(&quot;Key = &quot; + key + &quot;, Value = &quot; + value);&#125; 你也可以在keySet和values上应用同样的方法。 该种方式看起来冗余却有其优点所在。首先，在老版本java中这是惟一遍历map的方式。另一个好处是，你可以在遍历时调用iterator.remove()来删除entries，另两个方法则不能。根据javadoc的说明，如果在for-each遍历中尝试使用此方法，结果是不可预测的。 从性能方面看，该方法类同于for-each遍历（即方法二）的性能。 方法四：通过键找值遍历（效率低）12345Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;();for (Integer key : map.keySet()) &#123; Integer value = map.get(key); System.out.println(&quot;Key = &quot; + key + &quot;, Value = &quot; + value);&#125; 作为方法一的替代，这个代码看上去更加干净；但实际上它相当慢且无效率。因为从键取值是耗时的操作（与方法一相比，在不同的Map实现中该方法慢了20%~200%）。如果你安装了FindBugs，它会做出检查并警告你关于哪些是低效率的遍历。所以尽量避免使用。 总结：如果仅需要键(keys)或值(values)使用方法二。如果你使用的语言版本低于java 5，或是打算在遍历时删除entries，必须使用方法三。否则使用方法一(键值都要)。]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[List的asList、toArray、subList方法]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FList%E7%9A%84asList%E3%80%81toArray%E3%80%81subList%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Arrays.asListArrays.asList()把数组转换成集合时，不能使用其修改集合相关的方法，它的 add/remove/clear 方法会抛出 UnsupportedOperationException 异常。 因为asList() 的返回对象是一个 Arrays 内部类，并没有实现集合的修改方法。 toArray12345List&lt;String&gt; list = new ArrayList&lt;&gt;(2);list.add(&quot;guan&quot;);list.add(&quot;bao&quot;);String[] array = (String[]) list.toArray(); 直接使用 toArray() 无参方法返回值只能是 Object[]类，若强转其它类型数组将会抛异常。解决方案：使用 &lt;T&gt; T[] toArray(T[] a); 有参数这个方法，代码如下：12String[] array = new String[list.size()];array = list.toArray(array); subList123456789101112131415161718192021222324252627282930313233343536373839404142public class SubList &#123; @Test public void subList() &#123; List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(&quot;hello&quot;); // 当原始集合大小没有那么大时，毫无疑问抛异常。 //java.lang.IndexOutOfBoundsException //list.subList(0, 4); // 得到一个新的集合，往新集合中增加一条数据。 List&lt;String&gt; newList = list.subList(0, 1); newList.add(&quot;zong&quot;); // 遍历原始集合，竟然 size=2 了，而且往新集合中增加的数据存在于原始集合。 System.out.println(&quot;list size : &quot; + list.size()); for (String str : list) &#123; System.out.println(str); &#125; // 移除新集合中一条数据，遍历新集合。 newList.remove(&quot;zong&quot;); for (String str : newList) &#123; System.out.println(str); &#125; System.out.println(&quot;-------------------------&quot;); // 原始集合增加一条数据并遍历。 list.add(&quot;zong&quot;); for (String str : list) &#123; System.out.println(str); &#125; // 遍历新集合，抛出 ConcurrentModificationException 异常。 // java.util.ConcurrentModificationException for (String str : newList) &#123; System.out.println(str); &#125; &#125;&#125; 返回的新集合是靠原来的集合支持的，修改都会影响到彼此对方。在 subList 场景中，高度注意对原集合元素个数的修改，会导致子列表的遍历、增加、删除均产生异常。]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[List使用remove()方法删除元素]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FList%E4%BD%BF%E7%94%A8remove()%E6%96%B9%E6%B3%95%E5%88%A0%E9%99%A4%E5%85%83%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class Remove &#123; public static void main(String[] args) &#123; String str1 = new String("abc"); String str2 = new String("abc"); String str3 = new String("abc"); String str4 = new String("abc"); List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add(str1); list.add(str2); list.add(str3); list.add(str4); // 不能全部删除符合条件的数据 // 原因：List每remove掉一个元素以后，后面的元素都会向前移动，此时如果执行i=i+1，则刚刚移过来的元素没有被读取 System.out.println("list.size() = " + list.size()); for (int i = 0; i &lt; list.size(); i++) &#123; String s = list.get(i); if ("abc".equals(s)) &#123; list.remove(i); &#125; &#125; System.out.println("after remove : list.size() = " + list.size()); // 使用foreach删除报错：Exception in thread "main" java.util.ConcurrentModificationException for (String s : list) &#123; list.remove(s); &#125; // 三种解决办法 // 1. 倒序遍历 System.out.println("list.size() = " + list.size()); for (int i = list.size() - 1; i &gt;= 0; i--) &#123; String s = list.get(i); if ("abc".equals(s)) &#123; list.remove(i); &#125; &#125; System.out.println("after remove : list.size() = " + list.size()); // 2. 每移除一个元素以后把i移回来 System.out.println("list.size() = " + list.size()); for (int i = 0; i &lt; list.size(); i++) &#123; String s = list.get(i); if ("abc".equals(s)) &#123; list.remove(i); i--; &#125; &#125; System.out.println("after remove : list.size() = " + list.size()); // 3. 使用iterator.remove()方法移除 System.out.println("list.size() = " + list.size()); Iterator&lt;String&gt; it = list.iterator(); while (it.hasNext()) &#123; String s = it.next(); if ("abc".equals(s)) &#123; it.remove(); &#125; &#125; System.out.println("after remove : list.size() = " + list.size()); &#125;&#125;]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Set List Map]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FSet%20List%20Map%2F</url>
    <content type="text"><![CDATA[List有序（插入顺序）、重复 ArrayListLinkedListSet无序、不重复 EnumSetHashSetLinkedHashSetTreeSetMap键值对，key不能重复，value可以重复 ConcurrentHashMapEnumMapHashMapHashtableLinkedHashMapTreeMapWeakHashMaphttp://www.javamadesoeasy.com/2016/02/difference-between-list-set-and-map-in.html]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Set]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2FSet%2F</url>
    <content type="text"><![CDATA[HashSet 内部使用 HashMap 。它将元素存储为键和值。（HashSet 把存储的值作为 key）]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[foreach遍历list删除元素一定会报错？]]></title>
    <url>%2F2019%2F04%2F19%2F%E9%9B%86%E5%90%88%2Fforeach%E9%81%8D%E5%8E%86list%E5%88%A0%E9%99%A4%E5%85%83%E7%B4%A0%E4%B8%80%E5%AE%9A%E4%BC%9A%E6%8A%A5%E9%94%99%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[foreach遍历list集合删除某些元素一定会报错吗？先上一段代码：12345678910111213List list = new ArrayList();list.add(&quot;1&quot;);list.add(&quot;2&quot;);list.add(&quot;3&quot;);list.add(&quot;4&quot;);list.add(&quot;5&quot;);for (String item : list) &#123; if (item.equals(&quot;3&quot;)) &#123; System.out.println(item); list.remove(item); &#125;&#125;System.out.println(list.size()); 控制台报错：java.util.ConcurrentModificationException。这是怎么回事，然后去看了看这个异常，才发现自己果然还是太年轻啊。我们都知道增加for循环即foreach循环其实就是根据list对象创建一个iterator迭代对象，用这个迭代对象来遍历list，相当于list对象中元素的遍历托管给了iterator，如果要对list进行增删操作，都必须经过iterator。 每次foreach循环时都有以下两个操作： iterator.hasNext(); //判读是否有下个元素 item = iterator.next(); //下个元素是什么，并把它赋给item。 首先，我们来看看这个异常信息是什么。123456789101112131415161718192021public boolean hasNext() &#123; return cursor != size;&#125;@SuppressWarnings(&quot;unchecked&quot;)public E next() &#123; checkForComodification(); // 此处报错 int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i];&#125;final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125; 可以看到是进入checkForComodification()方法的时候报错了，也就是说modCount != expectedModCount。具体的原因是：以foreach方式遍历元素的时候，会生成iterator，然后使用iterator遍历。在生成iterator的时候，会保存一个expectedModCount参数，这个是生成iterator的时候List中修改元素的次数。如果你在遍历过程中删除元素，List中modCount就会变化，如果这个modCount和exceptedModCount不一致，就会抛出异常，这个是为了安全考虑。 看看list的remove源码：12345678910111213141516public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false;&#125; 看，并没有对expectedModCount进行任何修改，导致expectedModCount和modCount不一致，抛出异常。 但是，遍历list删除元素使用Iterator则不会报错，如下：123456Iterator it = list.iterator();while (it.hasNext()) &#123; if (it.next().equals(&quot;3&quot;)) &#123; it.remove(); &#125;&#125; 看看Iterator的remove()方法的源码，是对expectedModCount重新做了赋值处理的，如下：1234567891011121314public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; // 处理expectedModCount &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125;&#125; 这样的话保持expectedModCount = modCount相等，就不会报出错了。 是不是foreach所有的list删除操作都会报出这个错呢？ 其实不一定。如果删除的元素是倒数第二个数的话，其实是不会报错的。为什么呢，来一起看看。之前说了foreach循环会走两个方法hasNext() 和next()。如果不想报错的话，只要不进next()方法就好啦，看看hasNext()的方法。123public boolean hasNext() &#123; return cursor != size;&#125; 那么就要求hasNext()的方法返回false了，即cursor == size。其中cursor是Itr类（Iterator子类）中的一个字段，用来保存当前iterator的位置信息，从0开始。cursor本身就是游标的意思，在数据库的操作中用的比较多。只要curosr不等于size就认为存在元素。由于Itr是ArrayList的内部类，因此直接调用了ArrayList的size字段，所以这个字段的值是动态变化的，既然是动态变化的可能就会有问题出现了。我们以上面的代码为例，当到倒数第二个数据也就是“4”的时候，cursor是4，然后调用删除操作，此时size由5变成了4，当再调用hasNext判断的时候，cursor==size，就会调用后面的操作直接退出循环了。我们可以在上面的代码添加一行代码查看效果：123456for (String item : list) &#123; System.out.println(item); if (item.equals(&quot;4&quot;)) &#123; list.remove(item); &#125;&#125; 输出是：12341234 这样的话就可以看到执行到hasNext()方法就退出了，也就不会走后面的异常了。由此可以得出，用foreach删除list元素的时候只有倒数第二个元素删除不会报错，其他都会报错，所以删除list元素时一定要用Iterator。 参考文献：https://blog.csdn.net/bimuyulaila/article/details/52088124]]></content>
      <categories>
        <category>集合</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker-Compose的安装与使用]]></title>
    <url>%2F2019%2F04%2F18%2FDocker%2FDocker-Compose%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Docker Compose简介Docker Compose是一种用于通过使用单个命令创建和启动Docker应用程序的工具。我们可以使用它来配置应用程序的服务。 它是开发，测试和升级环境的利器。 它提供以下命令来管理应用程序的整个生命周期： 启动，停止和重建服务 查看运行服务的状态 测试运行服务的日志输出 在服务商运行一次性命令 要实现Docker Compose，需要包括以下步骤： 将应用程序环境变量放在Docker文件中以公开访问。 在docker-compose.yml文件中提供和配置服务名称，以便它们可以在隔离的环境中一起运行。 运行docker-compose up将启动并运行整个应用程序。 Docker Compose安装使用以下命令安装Docker Compose：1sudo curl -L &quot;https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose 上面链接下载速度较慢，可以使用国内的镜像，如DaoCloud：1curl -L https://get.daocloud.io/docker/compose/releases/download/1.23.2/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose 给docker-compose添加执行的权限：1sudo chmod +x /usr/local/bin/docker-compose 使用docker-compose version查看是否安装成功。 Docker Compose使用创建一个docker-compose.yml配置文件：12345678version: &apos;3&apos;services: tomcat: restart: always image: tomcat container_name: tomcat ports: - 8080:8080 参数说明： version：指定脚本语法解释器版本 services：要启动的服务列表 webapp：服务名称，可以随便起名，不重复即可 restart：启动方式，这里的always表示总是启动，即使服务器重启了也会立即启动服务 image：镜像的名称，默认从Docker Hub下载 container_name：容器名称，可以随便起名，不重复即可 ports：端口映射列表，左边为宿主机端口，右边为容器端口 运行：1234567docker-compose up# 后台运行docker-compose up -d# 删除docker-compose down Docker Compose命令前台运行1docker-compose up 后台运行1docker-compose up -d 启动1docker-compose start 停止1docker-compose stop 停止并移除容器1docker-compose down 查看日志1docker-compose logs -f tomcat]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[docker入门]]></title>
    <url>%2F2019%2F04%2F18%2FDocker%2Fdocker%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1. Docker for Linux本篇文章适用于Linux发行版，如Ubuntu。 for Windows and Mac OS X 通过这篇文章你将学会： 安装Docker 在容器中运行镜像 在Docker Hub中浏览镜像 创建自己的镜像并在容器中运行 创建Docker Hub帐号和镜像仓库 push镜像到Docker Hub 2. 安装Docker2.1 使用sudo权限登录Ubuntu2.2 检查wget是否安装which wget 如果没有安装，需要安装 sudo apt-get update sudo apt-get install wget 2.3 获取最新的Docker包$ wget -qO- https://get.docker.com/ | sh 2.4 检查docker是否正确安装docker run hello-world 3. 理解镜像和容器上一节运行docker run hello-world命令。使用这个命令完成使用Docker的主要任务。命令分三部分： 4. 查找并运行whalesay镜像可以从Docker Hub上查找镜像。 4.1 定位whalesay镜像 打开浏览器进入DockerHub 单击Browse &amp; Search 在搜索框中输入whalesay 单击docker/whalesay镜像每个镜像仓库包含镜像信息。信息中包含如镜像是哪种软件和怎么使用等信息。 4.2 在容器中运行whalesay 光标定位在终端的$之后 输入docker run docker/whalesay cowsay boo 输入docker images查看本地所有镜像 当在容器中运行镜像时，Docker会下载镜像到电脑中。本地的镜像可以节省时间。只有Hub中的镜像源改变后Docker才会下载镜像。当然也可以删除镜像。 5. 构建自己的镜像5.1 创建Dockerfile文件 打开终端 使用mkdir mydockerbuild创建目录 进入mydockerbuild目录创建Dockerfile文件 在Dockerfile文件中输入以下内容 FROM docker/whalesay:latest RUN apt-get -y update &amp;&amp; apt-get install -y fortunes CMD /usr/games/fortune -a | cowsay 保存Dockerfile文件 使用docker build -t docker-whale .构建镜像 6. 创建Docker Hub账号和资源和GitHub账号的注册基本一致，在此不详细介绍。 7. Tag、push、pull镜像本节将介绍如何tag和push自己的镜像到新建的资源库，并测试和pull镜像。 7.1 tag、push镜像 使用docker images列出本地镜像 找到docker-whale镜像的IMAGE ID 使用IMAGE ID和docker tag命令tagdocker-whale镜像$ docker tag 7d9495d03763 maryatdocker/docker-whale:latest 使用docker push命令把镜像push到资源库docker push maryatdocker/docker-whale 7.2 pull镜像 使用docker rmi删除maryatdocker/docker-whale和docker-whale镜像可以使用ID或name删除$ docker rmi -f 7d9495d03763$ docker rmi -f docker-whale 使用docker pull命令从资源库中pull镜像docker pull yourusername/docker-whale 运行镜像docker run maryatdocker/docker-whale 常见问题： 命令没有使用sudo 错误FATA[0000] Post http:///var/run/docker.sock/v1.18/containers/create: dial unix /var/run/docker.sock: no such file or directory. Are you trying to connect to a TLS-enabled daemon without TLS?解决方法：/etc/init.d/docker restart 参考文章：http://docs.docker.com/linux/started/]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker入门实战]]></title>
    <url>%2F2019%2F04%2F18%2FDocker%2FDocker%E5%85%A5%E9%97%A8%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Docker简介产生背景 开发和运维之间因为环境不同而导致的矛盾 集群环境下每台机器部署相同的应用 DevOps（Development and Operations） 简介Docker是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的Linux机器上，也可以实现虚拟化，容器是完全使用沙箱机制，相互之间不会有任何接口。 Docker是世界领先的软件容器平台。开发人员利用Docker可以消除协作编码时“在我的机器上可正常工作”的问题。运维人员利用Docker可以在隔离容器中并行运行和管理应用，获得更好的计算密度。企业利用Docker可以构建敏捷的软件交付管道，以更快的速度、更高的安全性和可靠的信誉为Linux和Windows Server应用发布新功能。 Docker优点简化程序：Docker让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的Linux机器上，便可以实现虚拟化。Docker改变了虚拟化的方式，使开发者可以直接将自己的成果放入Docker中进行管理。方便快捷已经是Docker的最大优势，过去需要用数天乃至数周的任务，在Docker容器的处理下，只需要数秒就能完成。 避免选择恐惧症：如果你有选择恐惧症，还是资深患者。Docker帮你打包你的纠结！比如Docker镜像；Docker镜像中包含了运行环境和配置，所以Docker可以简化部署多种应用实例工作。比如Web应用、后台应用、数据库应用、大数据应用比如Hadoop集群、消息队列等等都可以打包成一个镜像部署。 节省开支：一方面，云计算时代到来，使开发者不必为了追求效果而配置高额的硬件，Docker改变了高性能必然高价格的思维定势。Docker与云的结合，让云空间得到更充分的利用。不仅解决了硬件管理的问题，也改变了虚拟化的方式。 Docker架构Docker使用C/S架构，Client通过接口与Server进程通信实现容器的构建，运行和发布，如图： Host（Docker 宿主机）安装了Docker程序，并运行了Docker daemon的主机。 Docker daemon（Docker 守护进程）：运行在宿主机上，Docker守护进程，用户通过Docker client(Docker命令)与Docker daemon交互。 Images（镜像）：将软件环境打包好的模板，用来创建容器的，一个镜像可以创建多个容器。 Containers（容器）：Docker的运行组件，启动一个镜像就是一个容器，容器与容器之间相互隔离，并且互不影响。 Docker Client（Docker 客户端）Docker命令行工具，用户是用Docker Clients与Docker daemon进行通信并返回结果给用户。也可以使用其他工具通过Docker Api与Docker daemon通信。 Registry(仓库服务注册器)经常会和仓库(Repository)混为一谈，实际上Registry上可以有多个仓库，每个仓库可以看成是一个用户， 一个用户的仓库放了多个镜像。仓库分为了公开仓库(Public Repository)和私有仓库(Private Repository)，最大的公开仓库是官方的Docker Hub，国内也有如阿里云、时速云等，可以给国内用户提供稳定快速的服务。用户也可以在本地网络内创建一个私有仓库。当用户创建了自己的镜像之后就可以使用push命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上pull下来就可以了。 Docker安装Docker提供了两个版本：社区版(CE)和企业版(EE)。 操作系统要求以Centos7为例，且Docker要求操作系统必须为64位，且centos内核版本为3.1及以上。 查看系统内核版本信息：1uname -r —、准备卸载旧版本：123yum remove docker docker-common docker-selinux docker-engineyum remove docker-ce 卸载后将保留/var/lib/docker的内容(镜像、容器、存储卷和网络等)。1rm -rf /var/lib/docker 安装依赖软件包 12345yum install -y yum-utils device-mapper-persistent-data lvm2# 安装前可查看device-mapper-persistent-data 和 lvm2 是否已经安装rpm -qa | grep device-mapper-persistent-datarpm -qa | grep lvm2 设置yum源 1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 更新yum软件包索引 1yum makecache fast 二、安装安装最新版本docker-ce1234567yum install docker-ce -y#安装指定版本docker-ce可使用以下命令查看yum list docker-ce.x86_64 --showduplicates | sort -r# 安装完成之后可以使用命令查看docker version 三、配置镜像加速这里使用阿里云的免费镜像加速服务，也可以使用其他如时速云、网易云等。 注册登录开通阿里云容器镜像服务 查看控制台，找到镜像加速器并复制自己的加速器地址 找到/etc/docker目录下的daemon.json文件，没有则直接vi daemon.json 加入以下配置 1234# 填写自己的加速器地址&#123; &quot;registry-mirrors&quot;: [&quot;https://xxxxxx.mirror.aliyuncs.com&quot;]&#125; 通知systemd重载此配置文件 1systemctl daemon-reload 重启docker服务 1systemctl restart docker Docker常用操作输入docker可以查看Docker的命令用法，输入docker COMMAND --help查看指定命令详细用法。 镜像操作查找镜像：12# 搜索docker hub网站镜像的详细信息docker search 关键词 下载镜像：12# Tag表示版本，有些镜像的版本显示latest，为最新版本docker pull 镜像名:TAG 查看镜像：12# 查看本地所有镜像docker images 删除镜像：1234# 删除指定本地镜像docker rmi -f 镜像ID或者镜像名:TAG# -f 表示强制删除 获取元信息：12# 获取镜像的元信息，详细信息docker inspect 镜像ID或者镜像名:TAG 容器操作运行：12345678docker run --name 容器名 -i -t -p 主机端口:容器端口 -d -v 主机目录:容器目录:ro 镜像TD或镜像名:TAG# --name 指定容器名，可自定义，不指定自动命名# -i 以交互模式运行容器# -t 分配一个伪终端，即命令行，通常组合来使用# -p 指定映射端口，将主机端口映射到容器内的端口# -d 后台运行容器# -v 指定挂载主机目录到容器目录，默认为rw读写模式，ro表示只读 容器列表：12345# docker ps 查看正在运行的容器docker ps -a -q# -a 查看所有容器(运行中、未运行)# -q 只查看容器的ID 启动容器：1docker start 容器ID或容器名 停止容器：1docker stop 容器ID或容器名 删除容器：123docker rm -f 容器ID或容器名# -f 表示强制删除 查看日志：1docker logs 容器ID或容器名 进入正在运行容器：1234567docker exec -it 容器ID或者容器名 /bin/bash# 进入正在运行的容器并且开启交互模式终端# /bin/bash是固有写法，作用是因为docker后台必须运行一个进程，否则容器就会退出，在这里表示启动容器后启动bash。# 也可以用docker exec在运行中的容器执行命令 拷贝文件：123docker cp 主机文件路径 容器ID或容器名:容器路径 # 主机中文件拷贝到容器中docker cp 容器ID或容器名:容器路径 主机文件路径 # 容器中文件拷贝到主机中 获取容器元信息：1docker inspect 容器ID或容器名 创建镜像有时候从Docker镜像仓库中下载的镜像不能满足要求，我们可以基于一个基础镜像构建一个自己的镜像。 两种方式： 更新镜像：使用docker commit命令 构建镜像：使用docker build命令，需要创建Dockerfile文件 更新镜像先使用基础镜像创建一个容器，然后对容器内容进行更改，然后使用docker commit命令提交为一个新的镜像（tomcat为例）。 根据基础镜像，创建容器 1docker run --name mytomcat -p 80:8080 -d tomcat 修改容器内容 123456789docker exec -it mytomcat /bin/bashcd webapps/ROOTrm -f index.jspecho hello world &gt; index. htmlexit 提交为新镜像 1234docker commit -m=&quot;描述消息&quot; -a=&quot;作者&quot; 容器ID或容器名 镜像:TAG# 例：# docker commit -m=&quot;修改了首页&quot; -a=&quot;测试&quot; mytomcat zong/tomcat:v1.0 使用新镜像运行容器 1docker run --name tom -p 8080:8080 -d zong/tomcat:v1.0 使用Dockerfile构建镜像Dockerfile是一个包含创建镜像所有命令的文件，使用docker build命令可以根据Dockerfile的内容创建镜像(以tomcat为例)。 创建一个Dockerfile文件 vi Dockerfile 123456789#注意dockerfile指令须大写FROM tomcatMAINTAINER zongRUN rm -f /usr/local/tomcat/webapps/ROOT/index.jspRUN echo &quot;&lt;h1&gt;hello world2&lt;h1&gt;&quot; &gt; /usr/local/tomcat/webapps/ROOT/index.html 构建新镜像 1234docker build -f Dockerfile -t zong/tomcat:v2.0 .# -f Dockerfile路径，默认是当前目录# -t 指定新镜像的名字以及TAG 使用Dockerfile构建Spring Boot应用镜像—、准备 把你的spring boot项目打包成可执行jar包 把jar包上传到Linux服务器 二、构建 在jar包路径下创建Dockerfile文件vi Dockerfile 1234567891011# 指定基础镜像，本地没有会从dockerHub pull下来FROM java:8# 可执行jar何复制到基础镜像的根目录下ADD test.jar /test.jar# 镜像要暴露旳端口，如要使用端口，在执行docker run命令时使用-p生效EXPOSE 8080# 在镜像运行为容器后执行旳命令ENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/test.jar&quot;] 使用docker build命令构建镜像，基本语法 12345docker build -f Dockerfile -t zong/mypro:v1 .# -f 指定Dockerfile文件的路径# -t 指定镜像名字和TAG# . 指当前目录，这里实际上需要一个上下文路径 三、运行运行自己的Spring Boot镜像1docker run --name pro -p 80:80 -d 镜像名:TAG]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Docker技术入门与实战]]></title>
    <url>%2F2019%2F04%2F18%2FDocker%2FDocker%E6%8A%80%E6%9C%AF%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[第1章 初识Docker 可以简单地将Docker容器理解为一种沙盒（Sandbox）。每个容器内运行一个应用，不同的容器相互隔离，容器之间也可以建立通信机制。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch入门]]></title>
    <url>%2F2019%2F04%2F18%2FElasticsearch%2FElasticSearch%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[ElasticSearch是个开源的基于企业级REST的实时搜索和分析引擎。它的核心搜索功能是建立在Apache Lucene之上，而且支持许多其他特性。 它是由Java语言编写的，支持存储，索引，搜索和分析实时数据。和MongoDB类似，ElasticSearch也是个基于文档的NoSQL数据存储。 ElasticSearch特性： 开源 支持全文本简单、强大的搜索 支持基于REST的API（基于HTTP的JSON） 支持实时搜索分析 根据定义，它是分布式的 支持多租赁特性 支持云和大数据环境 支持跨平台 非规范化的NoSQL数据存储 ElasticSearch的优势： 开源 轻量级的REST API 高可用性，高扩展性 支持缓存数据 无模式 快速搜索性能 支持结构化和非结构化的数据 支持分布式，分片，复制，集群和多节点架构 支持批量操作 建立实时的图表和仪表盘 ElasticSearch的劣势： 不支持MapReduce操作 不能作为主数据存储 不兼容ACID的数据存储 不支持事务和分布式事务 没有内置的认证和授权机制 使用ElasticSearch的公司： Github.com, Quora.com, Stackoverflow.com eBay, DELL, Cisco, Mozilla, Wikimedia Netflix, Symatics, Facebook UK HMRC 例如，Github.com使用ElasticSearch搜索文件，历史等。大部分公司使用ELK管理日志，监控系统。ELK即ElasticSearch、Logstash和Kibana。 1. 安装ElasticSearchElasticSearch使用Java编写，所以需要先安装JDK并设置环境变量。要在本地安装ElasticSearch，按照下列步骤。 1.1 从https://www.elastic.co/downloads/elasticsearch下载ElasticSearch。1.2 Windows 下载并解压Zip文件：elasticsearch-5.2.1.zip 加压后的文件放到F:\elasticsearch-5.2.1 设置环境变量 1PATH = F:\elasticsearch-5.2.1\bin 启动ElasticSearch 1F:/&gt;elasticsearch.bat 在浏览器中输入http://localhost:9200访问ElasticSearch。可以在命令行中使用Ctrl + C停止ElasticSearch服务。 1.3 Ubuntu：使用tar文件安装 下载并解压tar文件 1tar -xvf elasticsearch-5.2.1.tar.gz 启动ElasticSearch 1$ ./elasticsearch 在浏览器中输入http://localhost:9200访问ElasticSearch 1.4 Ubuntu：使用命令行安装 执行以下命令下载ElasticSearch1$ sudo wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.2.1.deb 下载ElasticSearch DEB文件：elasticsearch-5.2.1.deb 执行下面dpkg命令安装ElasticSearch1$ sudo dpkg -i elasticsearch-5.2.1.deb ElasticSearch默认安装在/usr/share/elasticsearch。 启动ElasticSearch 1$ ./elasticsearch 在浏览器中输入http://localhost:9200访问ElasticSearch ElasticSearch默认端口是9200，可以根据需要修改 1.5 ElasticSearch启动后，访问默认URL，可以得到如下响应报文：12345678910111213&#123; &quot;name&quot; : &quot;rBvi0Hs&quot;, &quot;cluster_name&quot; : &quot;elasticsearch&quot;, &quot;cluster_uuid&quot; : &quot;kOQQ_nqfTW-b4vQ00XSvdg&quot;, &quot;version&quot; : &#123; &quot;number&quot; : &quot;5.2.1&quot;, &quot;build_hash&quot; : &quot;db0d481&quot;, &quot;build_date&quot; : &quot;2017-02-09T22:05:32.386Z&quot;, &quot;build_snapshot&quot; : false, &quot;lucene_version&quot; : &quot;6.4.1&quot; &#125;, &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125; 源码地址：https://github.com/elastic/elasticsearch 2. ElasticSearch REST API URL基础ElasticSearch REST API URL应该遵循如下格式。 Server可以是任意的服务器名称或主机名，例如“myserver”。有时使用Node + Port的方式，如“”3. ElasticSearch术语 4. ElasticSearch命令基础5. ElasticSearch CRUD操作6. 索引必须小写]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch]]></title>
    <url>%2F2019%2F04%2F18%2FElasticsearch%2Fes%2F</url>
    <content type="text"><![CDATA[入门Elasticsearch不仅仅是Lucene和全文搜索，我们还能这样去描述它： 分布式的实时文件存储，每个字段都被索引并可被搜索 分布式的实时分析搜索引擎 可以扩展到上百台服务器，处理PB级结构化或非结构化数据 安装安装Marvel./bin/plugin -i elasticsearch/marvel/latest禁用监控echo &#39;marvel.agent.enabled: false&#39; &gt;&gt; ./config/elasticsearch.yml运行./bin/elasticsearch测试curl &#39;http://localhost:9200/?pretty&#39; https://www.elastic.co/guide/cn/index.html http://wiki.jikexueyuan.com/project/elasticsearch-definitive-guide-cn/ https://es.xiaoleilu.com/ http://www.cnblogs.com/wxw16/tag/Elasticsearch/]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper安装]]></title>
    <url>%2F2019%2F04%2F18%2FDubbo%2FZookeeper%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[单机安装1. 安装jdk和zk2. 创建配置文件1cp zoo_sample.cfg zoo.cfg 3. 把zk加入环境变量12vim /etc/profileexport PATH=$PATH:/opt/zookeeper/bin 4. 执行source命令是修改的环境变量生效12source /etc/profileenv 集群搭建1. 修改配置文件zoo.cfg123server.1=192.168.88.129:2888:3888server.2=192.168.88.130:2888:3888server.3=192.168.88.132:2888:3888 2. 创建相关目录数据目录和日志目录，在zoo.cfg中配置默认的只有数据目录dataDir 3. 创建ServerID标识在dataDir目录下配置myid文件这个文件里面有一个数据就是server.1=192.168.88.129:2888:3888中的1。每个服务器对应相应的数字。123echo &quot;1&quot; &gt; /opt/zookeeper/myidecho &quot;2&quot; &gt; /opt/zookeeper/myidecho &quot;3&quot; &gt; /opt/zookeeper/myid 4. 重启服务并查看状态12zkServer.sh restartzkServer.sh status 5. 连接zk集群1zkCli -server 192.168.88.129:2181]]></content>
      <categories>
        <category>Dubbo</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[FastDFS单节点部署]]></title>
    <url>%2F2019%2F04%2F18%2FFastDFS%2FFastDFS%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[FastDFS是由淘宝的余庆先生所开发，是一个轻量级、高性能的开源分布式文件系统，用纯C语言开发，包括文件存储、文件同步、文件访问（上传、下载）、存取负载均衡、在线扩容、相同内容只存储一份等功能，适合有大容量存储需求的应用或系统。做分布式系统开发时，其中要解决的一个问题就是图片、音视频、文件共享的问题，分布式文件系统正好可以解决这个需求。同类的分布式文件系统有谷歌的GFS、HDFS（Hadoop）、TFS（淘宝）等。 源码开放下载地址：https://github.com/happyfish100早期源码开放下载地址：https://sourceforge.net/projects/fastdfs/files/官网论坛：http://bbs.chinaunix.net/forum-240-1.html FastDFS系统架构 FastDFS文件上传流程 1. client询问tracker上传到的storage，不需要附加参数； 2. tracker返回一台可用的storage； 3. client直接和storage通讯完成文件上传。 FastDFS文件下载流程 1. client询问tracker下载文件的storage，参数为文件标识（组名和文件名）； 2. tracker返回一台可用的storage； 3. client直接和storage通讯完成文件下载。 术语FastDFS两个主要的角色：Tracker Server 和 Storage ServerTracker Server：跟踪服务器，主要负责调度storage节点与client通信，在访问上起负载均衡的作用，和记录storage节点的运行状态，是连接client和storage节点的枢纽。Storage Server：存储服务器，保存文件和文件的meta data（元数据）Group：文件组，也可以称为卷。同组内服务器上的文件是完全相同的，做集群时往往一个组会有多台服务器，上传一个文件到同组内的一台机器上后，FastDFS会将该文件即时同步到同组内的其它所有机器上，起到备份的作用。meta data：文件相关属性，键值对（Key Value Pair）方式，如：width=1024, height=768。和阿里云OSS的meta data相似。 FastDFS单节点安装 - 服务器规划跟踪服务器（Tracker Server）：ip01存储服务器（Storage Server）：ip02操作系统：CentOS7用户：root数据存储目录：/fastdfs/tracker 安装包：FastDFS_v5.08.tar.gz：FastDFS源码libfastcommon-master.zip：（从 FastDFS 和 FastDHT 中提取出来的公共 C 函数库）fastdfs-nginx-module-master.zip：storage节点http服务nginx模块nginx-1.10.0.tar.gz：Nginx安装包ngx_cache_purge-2.3.tar.gz：图片缓存清除Nginx模块（集群环境会用到）点击这里下载所有安装包，你也可以从作者github官网去下载。 下载完成后，将压缩包解压到/usr/local/src目录下 一、所有tracker和storage节点都执行如下操作1、安装所需的依赖包1yum install make cmake gcc gcc-c++ 2、安装libfatscommon123456cd /usr/local/src unzip libfastcommon-master.zipcd libfastcommon-master ## 编译、安装./make.sh./make.sh install 3、安装FastDFS12345cd /usr/local/srctar -xzvf FastDFS_v5.08.tar.gzcd FastDFS./make.sh./make.sh install 采用默认安装方式，相应的文件与目录如下：1、 服务脚本：12/etc/init.d/fdfs_storaged /etc/init.d/fdfs_trackerd 2、 配置文件（示例配置文件）：1234ll /etc/fdfs/-rw-r--r-- 1 root root 1461 1月 4 14:34 client.conf.sample -rw-r--r-- 1 root root 7927 1月 4 14:34 storage.conf.sample -rw-r--r-- 1 root root 7200 1月 4 14:34 tracker.conf.sample 3、 命令行工具（/usr/bin目录下）1234567891011121314-rwxr-xr-x 1 root root 260584 1月 4 14:34 fdfs_appender_test -rwxr-xr-x 1 root root 260281 1月 4 14:34 fdfs_appender_test1 -rwxr-xr-x 1 root root 250625 1月 4 14:34 fdfs_append_file -rwxr-xr-x 1 root root 250045 1月 4 14:34 fdfs_crc32 -rwxr-xr-x 1 root root 250708 1月 4 14:34 fdfs_delete_file -rwxr-xr-x 1 root root 251515 1月 4 14:34 fdfs_download_file -rwxr-xr-x 1 root root 251273 1月 4 14:34 fdfs_file_info -rwxr-xr-x 1 root root 266401 1月 4 14:34 fdfs_monitor -rwxr-xr-x 1 root root 873233 1月 4 14:34 fdfs_storaged -rwxr-xr-x 1 root root 266952 1月 4 14:34 fdfs_test -rwxr-xr-x 1 root root 266153 1月 4 14:34 fdfs_test1 -rwxr-xr-x 1 root root 371336 1月 4 14:34 fdfs_trackerd -rwxr-xr-x 1 root root 251651 1月 4 14:34 fdfs_upload_appender -rwxr-xr-x 1 root root 252781 1月 4 14:34 fdfs_upload_file 二、配置tracker服务器1、 复制tracker样例配置文件，并重命名1cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf 2、 修改tracker配置文件12345vim /etc/fdfs/tracker.conf # 修改的内容如下：disabled=false # 启用配置文件port=22122 # tracker服务器端口（默认22122）base_path=/fastdfs/tracker # 存储日志和数据的根目录 其它参数保留默认配置， 具体配置解释可参考官方文档说明：http://bbs.chinaunix.net/thread-1941456-1-1.html 3、 创建base_path指定的目录1mkdir -p /fastdfs/tracker 4、 防火墙中打开tracker服务器端口（ 默认为 22122）1vi /etc/sysconfig/iptables 附加若/etc/sysconfig目录下没有iptables文件可随便写一条iptables命令配置个防火墙规则，如： iptables -P OUTPUT ACCEPT 然后用命令：service iptables save进行保存，默认就保存到 /etc/sysconfig/iptables文件里。这时既有了这个文件。防火墙也可以启动了。接下来要写策略，也可以直接写在/etc/sysconfig/iptables里了。 添加如下端口行：1-A INPUT -m state --state NEW -m tcp -p tcp --dport 22122 -j ACCEPT 重启防火墙1service iptables restart 如果是CentOS 7.2之后的版本，可以使用如下命令：1234567891011121314# 查询端口是否开放firewall-cmd --query-port=8080/tcp# 开放80端口firewall-cmd --permanent --add-port=80/tcp# 移除端口firewall-cmd --permanent --remove-port=8080/tcp#重启防火墙(修改配置后要重启防火墙)firewall-cmd --reload# 参数解释1、firwall-cmd：是Linux提供的操作firewall的一个工具；2、--permanent：表示设置为持久；3、--add-port：标识添加的端口； 5、 启动tracker服务器1/etc/init.d/fdfs_trackerd start 初次启动，会在/fastdfs/tracker目录下生成logs、data两个目录。12drwxr-xr-x 2 root root 4096 1月 4 15:00 datadrwxr-xr-x 2 root root 4096 1月 4 14:38 logs 检查FastDFS Tracker Server是否启动成功：1ps -ef | grep fdfs_trackerd 三、配置storage服务器1、 复制storage样例配置文件，并重命名1cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf 2、 编辑配置文件12345678vi /etc/fdfs/storage.conf # 修改的内容如下:disabled=false # 启用配置文件port=23000 # storage服务端口base_path=/fastdfs/storage # 数据和日志文件存储根目录store_path0=/fastdfs/storage # 第一个存储目录tracker_server=ip01:22122 # tracker服务器IP和端口http.server_port=8888 # http访问文件的端口 其它参数保留默认配置， 具体配置解释可参考官方文档说明：http://bbs.chinaunix.net/thread-1941456-1-1.html 3、 创建基础数据目录1mkdir -p /fastdfs/storage 4、 防火墙中打开storage服务器端口（ 默认为 23000）1234vi /etc/sysconfig/iptables #添加如下端口行： -A INPUT -m state --state NEW -m tcp -p tcp --dport 23000 -j ACCEPT 重启防火墙1service iptables restart 5、 启动storage服务器1/etc/init.d/fdfs_storaged start 初次启动，会在/fastdfs/storage目录下生成logs、data两个目录。 检查FastDFS Tracker Server是否启动成功：1ps -ef | grep fdfs_storaged 四、文件上传测试（ip01）1、 修改Tracker服务器客户端配置文件123456cp /etc/fdfs/client.conf.sample /etc/fdfs/client.confvim /etc/fdfs/client.conf # 修改以下配置，其它保持默认base_path=/fastdfs/trackertracker_server=ip01:22122 2、 执行文件上传命令123#/usr/local/src/test.png 是需要上传文件路径/usr/bin/fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/test.png 返回文件ID号：group1/M00/00/00/tlxkwlhttsGAU2ZXAAC07quU0oE095.png（能返回以上文件ID，说明文件已经上传成功） 如果上传报如下错误：123[root@localhost opt]# /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /opt/bd.png [2018-07-20 16:49:05] ERROR - file: tracker_proto.c, line: 48, server: 192.168.88.61:22122, response status 2 != 0tracker_query_storage fail, error no: 2, error info: No such file or directory 则关闭storage的防火墙。 五、在所有storage节点安装fastdfs-nginx-module1、 fastdfs-nginx-module 作用说明FastDFS 通过 Tracker 服务器，将文件放在 Storage 服务器存储，但是同组存储服务器之间需要进行文件复制，有同步延迟的问题。假设 Tracker 服务器将文件上传到了 ip01，上传成功后文件 ID 已经返回给客户端。此时 FastDFS 存储集群机制会将这个文件同步到同组存储 ip02，在文件还没有复制完成的情况下，客户端如果用这个文件 ID 在 ip02 上取文件，就会出现文件无法访问的错误。而 fastdfs-nginx-module 可以重定向文件连接到源服务器取文件，避免客户端由于复制延迟导致的文件无法访问错误。(解压后的 fastdfs-nginx-module 在 nginx 安装时使用) 2、 解压fastdfs-nginx-module-master.zip12cd /usr/local/srcunzip fastdfs-nginx-module-master.zip 3、 修改 fastdfs-nginx-module 的 config 配置文件12cd fastdfs-nginx-module/srcvi config 将12CORE_INCS=&quot;$CORE_INCS /usr/local/include/fastdfs /usr/local/include/fastcommon/&quot;CORE_LIBS=&quot;$CORE_LIBS -L/usr/local/lib -lfastcommon -lfdfsclient&quot; 修改为：12CORE_INCS=&quot;$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/&quot;CORE_LIBS=&quot;$CORE_LIBS -L/usr/lib -lfastcommon -lfdfsclient&quot; 4、上传当前的稳定版本 Nginx(nginx-1.6.2.tar.gz)到/usr/local/src 目录 5、安装编译 Nginx 所需的依赖包1yum install gcc gcc-c++ make automake autoconf libtool pcre* zlib openssl openssl-devel 6、编译安装 Nginx (添加 fastdfs-nginx-module 模块)12345cd /usr/local/src/tar -zxvf nginx-1.6.2.tar.gzcd nginx-1.6.2./configure --add-module=/usr/local/src/fastdfs-nginx-module/srcmake &amp;&amp; make install 7、复制 fastdfs-nginx-module 源码中的配置文件到/etc/fdfs 目录,并修改12cp /usr/local/src/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/ vi /etc/fdfs/mod_fastdfs.conf 8、修改以下配置1234567connect_timeout=10 base_path=/tmp tracker_server=ip01:22122 storage_server_port=23000 group_name=group1 url_have_group_name = true store_path0=/fastdfs/storage 9、复制 FastDFS 的部分配置文件到/etc/fdfs 目录12cd /usr/local/src/FastDFS/confcp http.conf mime.types /etc/fdfs/ 10、在/fastdfs/storage文件存储目录下创建软连接,将其链接到实际存放数据的目录1ln -s /fastdfs/storage/data/ /fastdfs/storage/data/M00 11、配置 Nginx（/usr/local/nginx/conf/nginx.conf） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119#user nobody;worker_processes 1; #error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info; #pid logs/nginx.pid; events &#123; worker_connections 1024;&#125; http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; server &#123; listen 8888; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; location ~/group([0-9])/M00 &#123; alias /fastdfs/storage; ngx_fastdfs_module; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&apos;s document root # concurs with nginx&apos;s one # #location ~ /\.ht &#123; # deny all; #&#125; &#125; # another virtual host using mix of IP-, name-, and port-based configuration # #server &#123; # listen 8000; # listen somename:8080; # server_name somename alias another.alias; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125; # HTTPS server # #server &#123; # listen 443 ssl; # server_name localhost; # ssl_certificate cert.pem; # ssl_certificate_key cert.key; # ssl_session_cache shared:SSL:1m; # ssl_session_timeout 5m; # ssl_ciphers HIGH:!aNULL:!MD5; # ssl_prefer_server_ciphers on; # location / &#123; # root html; # index index.html index.htm; # &#125; #&#125;&#125; A、8888端口值是要与/etc/fdfs/storage.conf中的 http.server_port=8888相对应，因为http.server_port默认为8888，如果想改成 80，则要对应修改过来。 B、Storage 对应有多个 group 的情况下，访问路径带 group 名，如/group1/M00/00/00/xxx，对应的 Nginx 配置为：123location ~/group([0-9])/M00 &#123; ngx_fastdfs_module;&#125; C、如查下载时如发现老报 404，将 nginx.conf 第一行 user nobody 修改为 user root 后重新启动。 12、防火墙中打开 Nginx 的 8888 端口1vi /etc/sysconfig/iptables 添加：1234-A INPUT -m state --state NEW -m tcp -p tcp --dport 8888 -j ACCEPT #重启防火墙service iptables restart 启动 Nginx123456 /usr/local/nginx/sbin/nginx(重启 Nginx 的命令为:/usr/local/nginx/sbin/nginx -s reload)# 或者cd /usr/local/nginx/sbin/./nginx -s reload 13、通过浏览器访问测试时上传的文件1http://ip:8888/group1/M00/00/00/tlxkwlhttsGAU2ZXAAC07quU0oE095.png 参考文章：https://www.cnblogs.com/cnmenglang/p/6251696.htmlhttps://www.cnblogs.com/chiangchou/p/fastdfs.html]]></content>
      <categories>
        <category>FastDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[FastDFS集群部署]]></title>
    <url>%2F2019%2F04%2F18%2FFastDFS%2FFastDFS%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[之前介绍过关于FastDFS单机部署，详见博文：FastDFS单节点部署 下面来看下FastDFS集群部署，实现高可用（HA）。 服务器规划跟踪服务器跟踪服务器1【主机】（Tracker Server）：192.100.139.121跟踪服务器2【备机】（Tracker Server）：192.100.139.122group1存储服务器存储服务器1（Storage Server）：192.100.139.123存储服务器2（Storage Server）：192.100.139.124group2存储服务器存储服务器3（Storage Server）：192.100.139.125存储服务器4（Storage Server）：192.100.139.126 操作系统：CentOS7用户：root 数据存储目录：/fastdfs/storage 安装包： FastDFS_v5.08.tar.gz：FastDFS源码 libfastcommon-master.zip：（从 FastDFS 和 FastDHT 中提取出来的公共 C 函数库） fastdfs-nginx-module-master.zip：storage节点http服务nginx模块 nginx-1.10.0.tar.gz：Nginx安装包 ngx_cache_purge-2.3.tar.gz：图片缓存清除Nginx模块（集群环境会用到） 点击这里下载所有安装包。 下载完成后，将压缩包解压到/usr/local/src目录下。 一、所有tracker和storage节点都执行如下操作1、安装所需的依赖包1yum install make cmake gcc gcc-c++ 2、安装libfatscommon1234567cd /usr/local/src#安装unzip 命令： yum install -y unzip zipunzip libfastcommon-master.zipcd libfastcommon-master## 编译、安装./make.sh./make.sh install 3、安装FastDFS12345cd /usr/local/srctar -xzvf FastDFS_v5.08.tar.gzcd FastDFS./make.sh./make.sh install 采用默认安装方式，相应的文件与目录检查如下： 1、服务脚本12/etc/init.d/fdfs_storaged/etc/init.d/fdfs_trackerd 2、配置文件（示例配置文件）1234ll /etc/fdfs/-rw-r--r-- 1 root root 1461 1月 4 14:34 client.conf.sample-rw-r--r-- 1 root root 7927 1月 4 14:34 storage.conf.sample-rw-r--r-- 1 root root 7200 1月 4 14:34 tracker.conf.sample 3、命令行工具（/usr/bin目录下）123456789101112131415ll /usr/bin/fdfs_*-rwxr-xr-x 1 root root 260584 1月 4 14:34 fdfs_appender_test-rwxr-xr-x 1 root root 260281 1月 4 14:34 fdfs_appender_test1-rwxr-xr-x 1 root root 250625 1月 4 14:34 fdfs_append_file-rwxr-xr-x 1 root root 250045 1月 4 14:34 fdfs_crc32-rwxr-xr-x 1 root root 250708 1月 4 14:34 fdfs_delete_file-rwxr-xr-x 1 root root 251515 1月 4 14:34 fdfs_download_file-rwxr-xr-x 1 root root 251273 1月 4 14:34 fdfs_file_info-rwxr-xr-x 1 root root 266401 1月 4 14:34 fdfs_monitor-rwxr-xr-x 1 root root 873233 1月 4 14:34 fdfs_storaged-rwxr-xr-x 1 root root 266952 1月 4 14:34 fdfs_test-rwxr-xr-x 1 root root 266153 1月 4 14:34 fdfs_test1-rwxr-xr-x 1 root root 371336 1月 4 14:34 fdfs_trackerd-rwxr-xr-x 1 root root 251651 1月 4 14:34 fdfs_upload_appender-rwxr-xr-x 1 root root 252781 1月 4 14:34 fdfs_upload_file 二、配置tracker服务器1、复制tracker样例配置文件，并重命名1cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf 2、修改tracker配置文件123456vim /etc/fdfs/tracker.conf# 修改的内容如下：disabled=false # 启用配置文件port=22122 # tracker服务器端口（默认22122）base_path=/fastdfs/tracker # 存储日志和数据的根目录store_lookup=0 # 轮询方式上传 其它参数保留默认配置， 具体配置解释可参考官方文档说明：http://bbs.chinaunix.net/thread-1941456-1-1.html 3、创建base_path指定的目录1mkdir -p /fastdfs/tracker 4、防火墙中打开tracker服务器端口（ 默认为 22122）1vi /etc/sysconfig/iptables 附加：若/etc/sysconfig目录下没有iptables文件可随便写一条iptables命令配置个防火墙规则，如：1iptables -P OUTPUT ACCEPT 然后用命令：service iptables save进行保存，默认就保存到 /etc/sysconfig/iptables文件里。这时既有了这个文件，防火墙也可以启动了。接下来要写策略，也可以直接写在/etc/sysconfig/iptables里了。添加如下端口行：1-A INPUT -m state --state NEW -m tcp -p tcp --dport 22122 -j ACCEPT 重启防火墙1service iptables restart 5、启动tracker服务器1/etc/init.d/fdfs_trackerd start 初次启动，会在/fastdfs/tracker目录下生成logs、data两个目录。12drwxr-xr-x 2 root root 4096 1月 4 15:00 datadrwxr-xr-x 2 root root 4096 1月 4 14:38 logs 检查FastDFS Tracker Server是否启动成功：1ps -ef | grep fdfs_trackerd 三、配置storage服务器1、复制storage样例配置文件，并重命名1cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf 2、编辑配置文件123456789vi /etc/fdfs/storage.conf# 修改的内容如下:disabled=false # 启用配置文件port=23000 # storage服务端口base_path=/fastdfs/storage # 数据和日志文件存储根目录store_path0=/fastdfs/storage # 第一个存储目录tracker_server=192.100.139.121:22122 # tracker服务器IP和端口tracker_server=192.100.139.122:22122 #tracker服务器IP2和端口http.server_port=8888 # http访问文件的端口 配置group_name不同分组配置不同group_name12group_name=group1group_name=group2 其它参数保留默认配置， 具体配置解释可参考官方文档说明：http://bbs.chinaunix.net/thread-1941456-1-1.html 3、创建基础数据目录1mkdir -p /fastdfs/storage 4、防火墙中打开storage服务器端口（ 默认为 23000）1vi /etc/sysconfig/iptables 添加如下端口行：1-A INPUT -m state --state NEW -m tcp -p tcp --dport 23000 -j ACCEPT 重启防火墙1service iptables restart 可以通过/fastdfs/storage/logs/storaged.log查看启动日志。 5、启动storage服务器1/etc/init.d/fdfs_storaged start 初次启动，会在/fastdfs/storage目录下生成logs、data两个目录。12drwxr-xr-x 259 root root 4096 Mar 31 06:22 datadrwxr-xr-x 2 root root 4096 Mar 31 06:22 logs 检查FastDFS Tracker Server是否启动成功：123[root@gyl-test-t9 ~]# ps -ef | grep fdfs_storagedroot 1336 1 3 06:22 ? 00:00:01 /usr/bin/fdfs_storaged /etc/fdfs/storage.confroot 1347 369 0 06:23 pts/0 00:00:00 grep fdfs_storaged 四、文件上传测试（ip01）1、修改Tracker服务器客户端配置文件123456cp /etc/fdfs/client.conf.sample /etc/fdfs/client.confvim /etc/fdfs/client.conf# 修改以下配置，其它保持默认base_path=/fastdfs/trackertracker_server=192.100.139.121:22122 # tracker服务器IP和端口tracker_server=192.100.139.122:22122 #tracker服务器IP2和端口 2、执行文件上传命令1234#/usr/local/src/test.png 是需要上传文件路径 /usr/bin/fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/test.png返回文件ID号：group1/M00/00/00/tlxkwlhttsGAU2ZXAAC07quU0oE095.png（能返回以上文件ID，说明文件已经上传成功） 或者1/usr/bin/fdfs_test /etc/fdfs/client.conf upload client.conf 五、在所有storage节点安装fastdfs-nginx-module1、fastdfs-nginx-module 作用说明FastDFS通过Tracker服务器，将文件放在Storage服务器存储，但是同组存储服务器之间需要进入文件复制，有同步延迟的问题。假设Tracker服务器将文件上传到了ip01，上传成功后文件ID已经返回给客户端。此时FastDFS存储集群机制会将这个文件同步到同组存储ip02，在文件还没有复制完成的情况下，客户端如果用这个文件ID在ip02上取文件，就会出现文件无法访问的错误。而fastdfs-nginx-module可以重定向文件连接到源服务器取文件，避免客户端由于复制延迟导致的文件无法访问错误。(解压后的fastdfs-nginx-module在nginx安装时使用) 2、解压 fastdfs-nginx-module_v1.16.tar.gz12cd /usr/local/srctar -xzvf fastdfs-nginx-module_v1.16.tar.gz 3、修改 fastdfs-nginx-module 的 config 配置文件12cd fastdfs-nginx-module/srcvim config 将1CORE_INCS=&quot;$CORE_INCS /usr/local/include/fastdfs /usr/local/include/fastcommon/&quot; 修改为：1CORE_INCS=&quot;$CORE_INCS /usr/include/fastdfs /usr/include/fastcommon/&quot; 4、安装编译 Nginx 所需的依赖包1yum install gcc gcc-c++ make automake autoconf libtool pcre* zlib openssl openssl-devel 5、上传当前的稳定版本 Nginx(nginx-1.6.2.tar.gz)到 /usr/local/src 目录6、编译安装 Nginx (添加 fastdfs-nginx-module 模块)12345cd /usr/local/src/tar -zxvf nginx-1.10.0.tar.gzcd nginx-1.10.0./configure --prefix=/opt/nginx --add-module=/usr/local/src/fastdfs-nginx-module/srcmake &amp;&amp; make install 7、复制 fastdfs-nginx-module 源码中的配置文件到 /etc/fdfs 目录，并修改12cp /usr/local/src/fastdfs-nginx-module/src/mod_fastdfs.conf /etc/fdfs/vi /etc/fdfs/mod_fastdfs.conf 修改以下配置：123456789101112131415161718192021connect_timeout=10base_path=/tmptracker_server=192.100.139.121:22122 # tracker服务器IP和端口tracker_server=192.100.139.122:22122 #tracker服务器IP2和端口group_name=group1 #当前服务器的group名url_have_group_name=true #url中包含group名称store_path0=/fastdfs/storage #存储路径group_count=2 #设置组的个数#在最后添加 [group1]group_name=group1storage_server_port=23000store_path_count=1store_path0=/fastdfs/storage[group2]group_name=group2storage_server_port=23000store_path_count=1store_path0=/fastdfs/storage 注意group_name的设置，如果是group1就设置为group1，如果是group2就设置为group2 8、复制 FastDFS 的部分配置文件到 /etc/fdfs 目录12cd /usr/local/src/FastDFS/confcp http.conf mime.types /etc/fdfs/ 9、在 /fastdfs/storage 文件存储目录下创建软连接，将其链接到实际存放数据的目录1ln -s /fastdfs/storage/data/ /fastdfs/storage/data/M00 10、配置 Nginx（/usr/local/nginx/conf/nginx.conf）1234567891011121314151617181920212223user nobody;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 8888; server_name localhost; location ~/group[1-2]/M00 &#123; ngx_fastdfs_module; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 说明： A、8888 端口值是要与/etc/fdfs/storage.conf中的 http.server_port=8888 相对应，因为 http.server_port 默认为 8888，如果想改成 80，则要对应修改过来。 B、Storage 对应有多个 group 的情况下，访问路径带 group 名，如/group1/M00/00/00/xxx，对应的 Nginx 配置为：123location ~/group([0-9])/M00 &#123; ngx_fastdfs_module;&#125; C、如查下载时如发现老报 404,将 nginx.conf 第一行 user nobody 修改为 user root 后重新启动。 11、防火墙中打开 Nginx 的 8888 端口1vi /etc/sysconfig/iptables 添加：1-A INPUT -m state --state NEW -m tcp -p tcp --dport 8888 -j ACCEPT 重启防火墙1service iptables restart 启动nginx：12/usr/local/nginx/sbin/nginx(重启 Nginx 的命令为:/usr/local/nginx/sbin/nginx -s reload) 六、验证：通过浏览器访问测试时上传的文件切换追踪服务器IP同样可以访问 http://192.100.139.123:8888/group1/M00/00/00/CmSKtFj13gyAen4oAAH0yXi-HW8296.png http://192.100.139.124:8888/group1/M00/00/00/CmSKtFj13gyAen4oAAH0yXi-HW8296.png 因为缓存问题，125、126两台机器也能通过url访问group1上的文件 七、tracker节点安装nginx1、下载需要的依赖库文件1yum -y install pcre pcre-devel zlib zlib-devel 2、解压并安装nginx，加入ngx_cache_purge（加入缓存模块）123456tar -zxvf nginx-1.10.0.tar.gztar -zxvf ngx_cache_purge-2.3.tar.gzcd nginx-1.10.0./configure --add-module=/usr/local/src/ngx_cache_purge-2.3make &amp;&amp; make install 3、配置nginx负载均衡和缓存123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#user nobody;worker_processes 1;#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#pid logs/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; #access_log logs/access.log main; sendfile on; #tcp_nopush on; #keepalive_timeout 0; keepalive_timeout 65; #gzip on; #设置group1的服务器 upstream fdfs_group1 &#123; server 192.100.139.123:8888 weight=1 max_fails=2 fail_timeout=30s; server 192.100.139.124:8888 weight=1 max_fails=2 fail_timeout=30s; &#125; #设置group2的服务器 upstream fdfs_group2 &#123; server 192.100.139.125:8888 weight=1 max_fails=2 fail_timeout=30s; server 192.100.139.126:8888 weight=1 max_fails=2 fail_timeout=30s; &#125; server &#123; listen 8000; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / &#123; root html; index index.html index.htm; &#125; #设置group1的负载均衡参数 location /group1/M00 &#123; proxy_pass http://fdfs_group1; &#125; #设置group2的负载均衡参数 location /group2/M00 &#123; proxy_pass http://fdfs_group2; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 4、防火墙设置参考上面设置 5、启动nginx1/usr/local/nginx/sbin/nginx 6、上传文件1/usr/bin/fdfs_upload_file /etc/fdfs/client.conf /usr/local/src/test.png 7、通过tracker地址访问http://192.100.139.121:8000/group2/M00/00/00/wKhYdFtdrv-AdFphAAAexdAvMYY481.png http://192.100.139.121:8000/group2/M00/00/00/wKhYdFtdrv-AdFphAAAexdAvMYY481.png 八、Java API 客户端配置1、前往GitHub下载Java_client代码。https://github.com/fzmeng/fastdfs.client2.在你的项目src/java/resources 下加入文件 fastdfs_client.conf注意修改tracker服务器Ip地址12345678connect_timeout = 2network_timeout = 30charset = ISO8859-1http.tracker_http_port = 8888http.anti_steal_token = notracker_server=192.100.139.121:22122tracker_server=192.100.139.122:22122default_group_name=group1]]></content>
      <categories>
        <category>FastDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Github上怎样把新commits的代码合并到自己的fork上]]></title>
    <url>%2F2019%2F04%2F18%2FGit%2FGithub%E4%B8%8A%E6%80%8E%E6%A0%B7%E6%8A%8A%E6%96%B0commits%E7%9A%84%E4%BB%A3%E7%A0%81%E5%90%88%E5%B9%B6%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84fork%E4%B8%8A%2F</url>
    <content type="text"><![CDATA[命令行模式步骤： 配置上游项目地址。即将你 fork 的项目的地址给配置到自己的项目上。比如我 fork 了一个项目，原项目是 wabish/fork-demo.git，我的项目就是 cobish/fork-demo.git。使用以下命令来配置。1git remote add upstream https://github.com/wabish/fork-demo.git 然后可以查看一下配置状况，很好，上游项目的地址已经被加进来了。12345git remote -vorigin git@github.com:cobish/fork-demo.git (fetch)origin git@github.com:cobish/fork-demo.git (push)upstream https://github.com/wabish/fork-demo.git (fetch)upstream https://github.com/wabish/fork-demo.git (push) 获取上游项目更新。使用 fetch 命令更新，fetch 后会被存储在一个本地分支 upstream/master 上。 1git fetch upstream 合并到本地分支。切换到 master 分支，合并 upstream/master 分支。 1git merge upstream/master 提交推送。根据自己情况提交推送自己项目的代码。 1git push origin master 由于项目已经配置了上游项目的地址，所以如果 fork 的项目再次更新，重复步骤 2、3、4即可。 图解模式 https://www.zhihu.com/question/20393785/answer/30725725]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git提交到多个远程仓库]]></title>
    <url>%2F2019%2F04%2F18%2FGit%2FGit%E6%8F%90%E4%BA%A4%E5%88%B0%E5%A4%9A%E4%B8%AA%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[随着github的普及和流行，现在程序员可能都习惯把代码托管到类似github的远程仓库中。毫无疑问，github是最受欢迎的托管平台。但是由于网络等种种原因，github在国内的访问并不稳定。于是国内各种托管平台应运而生，比较知名的有开源中国、coding等。很多国内程序员会把代码托管到多个平台，兼顾稳定性和流行性。 那么如何方便快捷的把代码托管到多个平台呢？ 例如我有下面两个仓库：https://gitee.com/zkzong/mongodb.githttps://github.com/zkzong/mongodb.git 先添加第一个仓库：1git remote add origin https://gitee.com/zkzong/mongodb.git 再添加第二个仓库：1git remote set-url --add origin https://github.com/zkzong/mongodb.git 如果还有其他，则可以像添加第二个一样继续添加其他仓库。 然后使用下面命令提交：1git push origin --all 打开.git/config，可以看到这样的配置1234[remote &quot;origin&quot;] url = https://github.com/zkzong/spring-boot.git fetch = +refs/heads/*:refs/remotes/origin/* url = https://gitee.com/zkzong/spring-boot.git 刚才的命令其实就是添加了这些配置。如果不想用命令行，可以直接编辑该文件，添加对应的url即可。]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Gradle使用Maven仓库]]></title>
    <url>%2F2019%2F04%2F18%2FGradle%2FGradle%E4%BD%BF%E7%94%A8Maven%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[找到maven仓库的位置并复制路径E:\repository。 配置环境变量GRADLE_USER_HOME，将上面的E:\repository配置进去。 重启idea验证是否生效。可以看到Gradle模块的Service directory path变成了我们设置的GRADLE_USER_HOME环境变量的位置，说明gradle用maven的仓库生效了。 配置gradle工程文件加载jar包仓库的优先级，让其先从本地仓库找，如果找不到，再从中央仓库下载。 1234567repositories &#123; /** * 先让gradle从本地仓库找,找不到再从下面的mavenCentral()中央仓库去找jar包 */ mavenLocal() mavenCentral()&#125;]]></content>
      <categories>
        <category>Gradle</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hibernate]]></title>
    <url>%2F2019%2F04%2F18%2FHibernate%2FHibernate%2F</url>
    <content type="text"><![CDATA[1. Hibernate核心组件 Configuration Database Connection Class Mapping SetupSessionFactorySessionTransactionQueryCriteria 2. 注解注解可以放在成员变量或者getter方法上。@Entity@Table@Id @GeneratedValue@Column 注解和xml的区别3. HQL分页Query setFirstResult(int startPosition)Query setMaxResults(int maxResult) 4. Criteria查询 最简单的criteria查询如下，它会查处所有Employee记录。12Criteria cr = session.createCriteria(Employee.class);List results = cr.list(); … 使用Restrictions添加条件进行Criteria查询123Criteria cr = session.createCriteria(Employee.class);cr.add(Restrictions.eq("salary", 2000));List results = cr.list(); 更多例子如下：12345678910111213141516171819202122232425262728Criteria cr = session.createCriteria(Employee.class); // To get records having salary more than 2000cr.add(Restrictions.gt("salary", 2000)); // To get records having salary less than 2000cr.add(Restrictions.lt("salary", 2000)); // To get records having fistName starting with zaracr.add(Restrictions.like("firstName", "zara%")); // Case sensitive form of the above restriction. CHAPTER 14cr.add(Restrictions.ilike("firstName", "zara%")); // To get records having salary in between 1000 and 2000cr.add(Restrictions.between("salary", 1000, 2000)); // To check if the given property is nullcr.add(Restrictions.isNull("salary")); // To check if the given property is not nullcr.add(Restrictions.isNotNull("salary")); // To check if the given property is emptycr.add(Restrictions.isEmpty("salary")); // To check if the given property is not emptycr.add(Restrictions.isNotEmpty("salary")); 使用LogicalExpression创建AND或OR条件查询12345678910111213Criteria cr = session.createCriteria(Employee.class); Criterion salary = Restrictions.gt("salary", 2000);Criterion name = Restrictions.ilike("firstNname","zara%");// To get records matching with OR condistions LogicalExpression orExp = Restrictions.or(salary, name);cr.add( orExp );// To get records matching with AND condistions LogicalExpression andExp = Restrictions.and(salary, name); cr.add( andExp ); List results = cr.list(); 分页方法 public Criteria setFirstResult(int firstResult)public Criteria setMaxResults(int maxResults) 对结果排序12345678910Criteria cr = session.createCriteria(Employee.class); // To get records having salary more than 2000 cr.add(Restrictions.gt("salary", 2000)); // To sort records in descening order crit.addOrder(Order.desc("salary")); // To sort records in ascending order crit.addOrder(Order.asc("salary")); List results = cr.list(); 5. 批量处理首先设置hibernate.jdbc.batch_size为需要批量处理的数字。其次修改代码：12345678910111213'''添加功能'''Session session = SessionFactory.openSession(); Transaction tx = session.beginTransaction(); for ( int i=0; i&lt;100000; i++ ) &#123; Employee employee = new Employee(.....); session.save(employee); if( i % 50 == 0 ) &#123; // Same as the JDBC batch size //flush a batch of inserts and release memory: session.flush(); session.clear(); &#125; &#125; tx.commit(); session.close(); 1234567891011121314151617'''修改功能'''Session session = sessionFactory.openSession(); Transaction tx = session.beginTransaction();ScrollableResults employeeCursor = session.createQuery("FROM EMPLOYEE") .scroll(); int count = 0; while ( employeeCursor.next() ) &#123; Employee employee = (Employee) employeeCursor.get(0); employee.updateEmployee(); seession.update(employee); if ( ++count % 50 == 0 ) &#123; session.flush(); session.clear(); &#125; &#125; tx.commit(); session.close();]]></content>
      <categories>
        <category>Hibernate</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BeanUtils.copyProperties()使用]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2FBeanUtils.copyProperties()%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[大部分Java程序员应该都用过BeanUtils.copyProperties()，起作用就是把两个对象中相同字段进行赋值。123Person p1 = new Person(&quot;zong&quot;, 30);Person p2 = new Person();BeanUtils.copyProperties(p2, p1); 当然p2不一定也是Person对象，其他对象也可以。只要两个对象中有相同的成员变量就可以赋值。 但是如果赋值对象是List等集合类呢？？？答案是否定的，它们之间并不能赋值。1234567891011Person p1 = new Person(&quot;zong&quot;, 30);Person p2 = new Person(&quot;ma&quot;, 25);Person p3 = new Person(&quot;liu&quot;, 20);List&lt;Person&gt; sList = new ArrayList&lt;Person&gt;(3);sList.add(p1);sList.add(p2);sList.add(p3);List&lt;Person&gt; tList = new ArrayList&lt;Person&gt;(3);BeanUtils.copyProperties(tList, sList);System.out.println(tList.size()); 以上代码是想把sList的值赋给tList，但是运行之后发现tList.size()的值为0，赋值失败。 如果要对两个List对象赋值，可以参考如下代码：123456List&lt;Person&gt; tList = new ArrayList&lt;Person&gt;(3);for (int i = 0; i &lt; sList.size(); i++) &#123; Person p = new Person(); BeanUtils.copyProperties(p, sList.get(i)); tList.add(p);&#125; http://stackoverflow.com/questions/19312055/beanutils-copyproperties-to-copy-arraylist]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[BigDecimal使用ROUND_HALF_UP进行四舍五入]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2FBigDecimal%E4%BD%BF%E7%94%A8ROUND_HALF_UP%E8%BF%9B%E8%A1%8C%E5%9B%9B%E8%88%8D%E4%BA%94%E5%85%A5%2F</url>
    <content type="text"><![CDATA[123456BigDecimal bdTest = new BigDecimal(1.745);BigDecimal bdTest1 = new BigDecimal(0.745);bdTest = bdTest.setScale(2, BigDecimal.ROUND_HALF_UP);bdTest1 = bdTest1.setScale(2, BigDecimal.ROUND_HALF_UP);System.out.println("bdTest:" + bdTest); // 1.75System.out.println("bdTest1:" + bdTest1); // 0.74 运行以上代码可以看到，1.745四舍五入的结果是1.75，0.745四舍五入的结果是0.74。 原因：使用参数为float或double的BigDecimal创建对象会丢失精度。因此强烈建议不要使用参数为float或double的BigDecimal创建对象。12System.out.println(new BigDecimal(1.745)); // 1.74500000000000010658141036401502788066864013671875System.out.println(new BigDecimal(0.745)); // 0.74499999999999999555910790149937383830547332763671875 解决办法： 使用BigDecimal(String val)的构造方法创建对象new BigDecimal(&quot;1.745&quot;);new BigDecimal(&quot;0.745&quot;); 使用使用BigDecimal的valueOf(double val)方法创建对象BigDecimal.valueOf(1.745);BigDecimal.valueOf(0.745); http://stackoverflow.com/questions/12460482/how-to-round-0-745-to-0-75-using-bigdecimal-round-half-up]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Class.forName()与ClassLoader.loadClass()的区别]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2FClass.forName()%E4%B8%8EClassLoader.loadClass()%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[1. Java类装载过程 装载：通过累的全限定名获取二进制字节流，将二进制字节流转换成方法区中的运行时数据结构，在内存中生成Java.lang.class对象； 链接：执行下面的校验、准备和解析步骤，其中解析步骤是可以选择的； 校验：检查导入类或接口的二进制数据的正确性；（文件格式验证，元数据验证，字节码验证，符号引用验证）准备：给类的静态变量分配并初始化存储空间；解析：将常量池中的符号引用转成直接引用； 初始化：激活类的静态变量的初始化Java代码和静态Java代码块，并初始化程序员设置的变量值。 2. 分析Class.forName()和ClassLoader.loadClass()Class.forName(className)方法，内部实际调用的方法是Class.forName(className,true,classloader)；第2个boolean参数表示类是否需要初始化，Class.forName(className)默认是需要初始化。一旦初始化，就会触发目标对象的static块代码执行，static参数也也会被再次初始化。 ClassLoader.loadClass(className)方法，内部实际调用的方法是 ClassLoader.loadClass(className,false)；第2个 boolean参数，表示目标对象是否进行链接，false表示不进行链接，由上面介绍可以看出，不进行链接意味着不进行包括初始化等一些列步骤，那么静态块和静态对象就不会得到执行。 3. 数据库链接为什么使用Class.forName(className)JDBC Driver源码如下，因为在JDBC规范中明确要求Driver(数据库驱动)类必须向DriverManager注册自己，所以使用Class.forName(classname)才能在反射回去类的时候执行static块。 1234567static &#123; try &#123; java.sql.DriverManager.registerDriver(new Driver()); &#125; catch (SQLException E) &#123; throw new RuntimeException(&quot;Can&apos;t register driver!&quot;); &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Junit]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2FJUnit%2F</url>
    <content type="text"><![CDATA[测试方法必须使用@Test修饰 测试方法必须使用public void修饰 新建test源代码目录来存放测试代码 测试类的包名应该和被测试类的报名保持一致 测试单元中的每个方法必须可以独立测试，测试方法之间不能有任何依赖 测试类一般使用Test作为后缀 测试方法一般使用test作为前缀 JUnit4中提供的常用注解大致包括： @BeforeClass @AfterClass @Before @After @Test @Ignore @RunWith 注解的执行顺序和作用： @Test将一个普通的方法修饰为一个测试方法 @BeforeClass会在所有方法运行前执行，static修饰 @AfterClass会在所有方法运行结束后执行，static修饰 @Before会在每个测试方法运行前执行一次 @After会在每个测试方法运行后被执行一次]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java equals()和hashCode()]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2FJava%20equals()%E5%92%8ChashCode()%2F</url>
    <content type="text"><![CDATA[https://www.journaldev.com/21095/java-equals-hashcode]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[String]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2FString%2F</url>
    <content type="text"><![CDATA[常用的字符串创建有两种方式：String s = &quot;zong&quot;;String s = new String(&quot;abc&quot;); http://www.cnblogs.com/ydpvictor/archive/2012/09/09/2677260.html]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[equals和==的区别小结]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2Fequals%E5%92%8C%3D%3D%E7%9A%84%E5%8C%BA%E5%88%AB%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[====比较的是变量(栈)内存中存放的对象的(堆)内存地址，用来判断两个对象的地址是否相同，即是否是指相同一个对象。比较的是真正意义上的指针操作。 比较的是操作符两端的操作数是否是同一个对象。 两边的操作数必须是同一类型的（可以是父子类之间）才能编译通过。 比较的是地址，如果是具体的阿拉伯数字的比较，值相等则为true。如：int a=10与long b=10L与double c=10.0都是相同的（为true），因为他们都指向地址为10的堆。 equalsequals用来比较的是两个对象的内容是否相等，由于所有的类都是继承自java.lang.Object类的，所以适用于所有对象，如果没有对该方法进行覆盖的话，调用的仍然是Object类中的方法，而Object中的equals方法返回的却是==的判断。 String s=&quot;abce&quot;是一种非常特殊的形式，和new有本质的区别。它是java中唯一不需要new就可以产生对象的途径。以String s=&quot;abce&quot;;形式赋值在java中叫直接量，它是在常量池中而不是像new一样放在压缩堆中。这种形式的字符串，在JVM内部发生字符串拘留，即当声明这样的一个字符串后，JVM会在常量池中先查找有没有一个值为”abcd”的对象，如果有，就会把它赋给当前引用，即原来那个引用和现在这个引用指向了同一对象，如果没有，则在常量池中新创建一个”abcd”，下一次如果有String s1 = &quot;abcd&quot;;又会将s1指向”abcd”这个对象，即以这种形式声明的字符串，只要值相等，任何多个引用都指向同一对象。而String s = new String(&quot;abcd&quot;);和其它任何对象一样，每调用一次就产生一个对象，只要它们调用。 也可以这么理解：String str = &quot;hello&quot;;先在内存中找是不是有”hello”这个对象，如果有，就让str指向那个”hello”。如果内存里没有”hello”，就创建一个新的对象保存”hello”。String str=new String(&quot;hello&quot;)就是不管内存里是不是已经有”hello”这个对象，都新建一个对象保存”hello”。 具体可以看下面的代码：1234567891011121314151617181920public class EqualsTest &#123; public static void main(String[] args) &#123; String a = new String(&quot;ab&quot;); // a为一个引用 String b = new String(&quot;ab&quot;); // b为另一个引用,对象的内容一样 String aa = &quot;ab&quot;; // 放在常量池中 String bb = &quot;ab&quot;; // 从常量池中查找 if (aa == bb) &#123; // true System.out.println(&quot;aa==bb&quot;); &#125; if (a == b) &#123; // false，非同一对象 System.out.println(&quot;a==b&quot;); &#125; if (a.equals(b)) &#123; // true System.out.println(&quot;aEQb&quot;); &#125; if (42 == 42.0) &#123; // true System.out.println(&quot;true&quot;); &#125; &#125;&#125; equals和==的区别equals方法最初是在所有类的基类Object中进行定义的，源码是：123public boolean equals(Object obj) &#123; return (this == obj);&#125; 由equals的源码可以看出这里定义的equals与==是等效的（Object类中的equals没什么区别），不同的原因就在于有些类（像String、Integer等类）对equals进行了重写，但是没有对equals进行重写的类（比如我们自己写的类）就只能从Object类中继承equals方法，其equals方法与==就也是等效的，除非我们在此类中重写equals。 对equals重新需要注意五点： 自反性：对任意引用值x，x.equals(x)的返回值一定为true； 对称性：对于任何引用值x和y，当且仅当y.equals(x)返回值为true时，x.equals(y)的返回值一定为true； 传递性：如果x.equals(y)=true，y.equals(z)=true，则x.equals(z)=true； 一致性：如果参与比较的对象没任何改变，则对象比较的结果也不应该有任何改变； 非空性：任何非空的引用值x，x.equals(null)的返回值一定为false 。 String类对equals的重写如下：123456789101112131415161718192021public boolean equals(Object anObject) &#123; if (this == anObject) &#123; return true; &#125; if (anObject instanceof String) &#123; String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) &#123; char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false;&#125; 另外，”==”比”equals”运行速度快,因为”==”只是比较引用。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java加密与解密的艺术]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2FJava%E5%8A%A0%E5%AF%86%E4%B8%8E%E8%A7%A3%E5%AF%86%E7%9A%84%E8%89%BA%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[Url Base64算法主要是替换了Base64字符映射表中的第62和63个字符，也就是将“+”和“/”符号替换成了“-”和“_”符号。但对于补位符“=”，一种建议是使用“~”符号，另一种建议是使用“.”符号。其中，由于“~”符号与文件系统冲突，不建议使用；而对于“.”符号，如果出现连续两次，则认为是错误。对于补位符的问题，Bouncy Castle和Commons Codec有差别：Bouncy Castle使用“.”作为补位符，而Commons Codec则完全杜绝使用补位符。 Bouncy Castle和Commons Codec都提供了Base64算法实现，两组织在算法实现上遵循了不同的标准。Bouncy Castle实现了一般Base64编码，而Commons Codec遵循RFC 2045的相关定义做了算法实现。简而言之，RFC 2045要求Base64编码后的字符串每行76个字符，不论每行是否够76个字符，都要在行末添加一个回车换行符（“\r\n”）。Bouncy Castle并没有对于Base64编码字符串的行要求，而Commons Codec遵循了这些要求并提供了Base64算法实现的方法，而且在方法易用性上提供了更广泛的API，方便使用。因此，对于Base64算法的实现，Commons Codec更胜一筹。 如果需要在76个字符后换行，使用如下方法：Base64.encodeBase64(data.getBytes(ENCODING), true);]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[static]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2Fstatic%2F</url>
    <content type="text"><![CDATA[1234567891011121314151617181920212223242526public class StaticSuper &#123; static &#123; System.out.println("super static block"); &#125; StaticSuper() &#123; System.out.println("super constructor"); &#125;&#125;public class StaticTests extends StaticSuper &#123; static int rand; static &#123; rand = (int) Math.random() * 6; System.out.println("static block " + rand); &#125; StaticTests() &#123; System.out.println("constructor"); &#125; public static void main(String[] args) &#123; System.out.println("in main"); StaticTests st = new StaticTests(); &#125;&#125; 输出：12345super static blockstatic block 0in mainsuper constructorconstructor]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[使用pom文件指定JDK编译版本]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2F%E4%BD%BF%E7%94%A8pom%E6%96%87%E4%BB%B6%E6%8C%87%E5%AE%9AJDK%E7%BC%96%E8%AF%91%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[maven项目会用maven-compiler-plugin默认的JDK版本来进行编译，如果不指明版本就容易出现版本不匹配的问题，可能导致编译不通过的问题。 解决办法：在pom文件中配置maven-compiler-plugin插件。 有两种方式：方式一：properties标签添加：12&lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;&lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt; 方式二：12345678910111213&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基础数据类型]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2F%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[注意，Integer、Short、Byte、Character、Long这几个类的valueOf方法的实现是类似的（[-128, 127]之间使用==为true）。Double、Float的valueOf方法的实现是类似的。 1234567891011121314151617public void test() &#123; Integer a = 1; Integer b = 2; Integer c = 3; Integer d = 3; Integer e = 321; Integer f = 321; Long g = 3L; Long h = 2L; System.out.println(c == d); // true System.out.println(e == f); // false System.out.println(c == (a + b)); // true System.out.println(c.equals(a + b)); // true System.out.println(g == (a + b)); // true System.out.println(g.equals(a + b)); // false System.out.println(g.equals(a + h)); // true&#125; 第一个和第二个输出结果没有什么疑问。第三句由于a+b包含了算术运算，因此会触发自动拆箱过程（会调用intValue方法），因此它们比较的是数值是否相等。而对于c.equals(a+b)会先触发自动拆箱过程，再触发自动装箱过程，也就是说a+b，会先各自调用intValue方法，得到了加法运算后的数值之后，便调用Integer.valueOf方法，再进行equals比较。同理对于后面的也是这样，不过要注意倒数第二个和最后一个输出的结果（如果数值是int类型的，装箱过程调用的是Integer.valueOf；如果是long类型的，装箱调用的Long.valueOf方法）。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[文件和byte数组之间相互转换]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2F%E6%96%87%E4%BB%B6%E5%92%8Cbyte%E6%95%B0%E7%BB%84%E4%B9%8B%E9%97%B4%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[文件转换成byte数组文件转换成byte数组有两种方式： 1. 传统方式123456789File file = new File(&quot;/temp/abc.txt&quot;);//init array with file lengthbyte[] bytesArray = new byte[(int) file.length()];FileInputStream fis = new FileInputStream(file);fis.read(bytesArray); //read file into bytes[]fis.close();return bytesArray; 2. NIO方式12345String filePath = &quot;/temp/abc.txt&quot;;byte[] bFile = Files.readAllBytes(new File(filePath).toPath());//or thisbyte[] bFile = Files.readAllBytes(Paths.get(filePath)); 完整实例下面的例子展示了如何把读取的文件内容转换成byte数组，使用了传统的FileInputStream和java.nio两种方式。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;/** * @author zkzong * @date 2017/10/20 */public class FileToArrayOfBytes &#123; public static void main(String[] args) &#123; try &#123; // convert file to byte[] byte[] bFile = readBytesFromFile(&quot;test.txt&quot;); //java nio //byte[] bFile = Files.readAllBytes(new File(&quot;test.txt&quot;).toPath()); //byte[] bFile = Files.readAllBytes(Paths.get(&quot;test.txt&quot;)); // save byte[] into a file Path path = Paths.get(&quot;test2.txt&quot;); Files.write(path, bFile); System.out.println(&quot;Done&quot;); //Print bytes[] for (int i = 0; i &lt; bFile.length; i++) &#123; System.out.print((char) bFile[i]); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; private static byte[] readBytesFromFile(String filePath) &#123; FileInputStream fileInputStream = null; byte[] bytesArray = null; try &#123; File file = new File(filePath); bytesArray = new byte[(int) file.length()]; //read file into bytes[] fileInputStream = new FileInputStream(file); fileInputStream.read(bytesArray); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fileInputStream != null) &#123; try &#123; fileInputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return bytesArray; &#125;&#125; byte数组转换成文件byte数组转换成文件也有两种方式： 1. 传统方式123FileOutputStream fos = new FileOutputStream(fileDest);fos.write(bytesArray);fos.close(); 2. NIO方式12Path path = Paths.get(fileDest);Files.write(path, bytesArray); 完整实例下面的例子展示了如何把读取的文件内容转换成byte数组，并把保存的byte数组转换成一个新的文件，使用了传统的try-catch、JDK 7的try-resources和java.nio两种方式。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;/** * @author zkzong * @date 2017/10/20 */public class ArrayOfBytesToFile &#123; private static final String UPLOAD_FOLDER = &quot;/&quot;; public static void main(String[] args) &#123; FileInputStream fileInputStream = null; try &#123; File file = new File(&quot;test.txt&quot;); byte[] bFile = new byte[(int) file.length()]; //read file into bytes[] fileInputStream = new FileInputStream(file); fileInputStream.read(bFile); //save bytes[] into a file writeBytesToFile(bFile, UPLOAD_FOLDER + &quot;test1.txt&quot;); writeBytesToFileClassic(bFile, UPLOAD_FOLDER + &quot;test2.txt&quot;); writeBytesToFileNio(bFile, UPLOAD_FOLDER + &quot;test3.txt&quot;); System.out.println(&quot;Done&quot;); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fileInputStream != null) &#123; try &#123; fileInputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //Classic, &lt; JDK7 private static void writeBytesToFileClassic(byte[] bFile, String fileDest) &#123; FileOutputStream fileOuputStream = null; try &#123; fileOuputStream = new FileOutputStream(fileDest); fileOuputStream.write(bFile); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; if (fileOuputStream != null) &#123; try &#123; fileOuputStream.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; //Since JDK 7 - try resources private static void writeBytesToFile(byte[] bFile, String fileDest) &#123; try (FileOutputStream fileOuputStream = new FileOutputStream(fileDest)) &#123; fileOuputStream.write(bFile); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //Since JDK 7, NIO private static void writeBytesToFileNio(byte[] bFile, String fileDest) &#123; try &#123; Path path = Paths.get(fileDest); Files.write(path, bFile); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[泛型]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2F%E6%B3%9B%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[当使用泛型类时，必须在创建对象的时候指定类型参数的值，而使用泛型方法的时候，通常不必指明参数类型。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[反射]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2F%E5%8F%8D%E5%B0%84%2F</url>
    <content type="text"><![CDATA[Class类提供了四个public方法，用于获取某个类的构造方法 Constructor getConstructor(Class[] params) 根据构造函数的参数，返回一个具体的具有public属性的构造函数 Constructor getConstructors() 返回所有具有public属性的构造函数数组 Constructor getDeclaredConstructor(Class[] params) 根据构造函数的参数，返回一个具体的构造函数（不分public和非public属性） Constructor getDeclaredConstructors() 返回该类中所有的构造函数数组（不分public和非public属性） 四种获取成员方法的方法 Method getMethod(String name, Class[] params) 根据方法名和参数，返回一个具体的具有public属性的方法 Method[] getMethods() 返回所有具有public属性的方法数组 Method getDeclaredMethod(String name, Class[] params) 根据方法名和参数，返回一个具体的方法（不分public和非public属性） Method[] getDeclaredMethods() 根据方法名和参数，返回一个具体的方法（不分public和非public属性） 四种获取成员属性的方法 Field getField(String name) 根据变量名，返回一个具体的具有public属性的成员变量 Field[] getFields() 返回具有public属性的成员变量的数组 Field getDeclaredField(String name) 根据变量名，返回一个成员变量（不分public和非public属性） Field[] getDeclaredFields() 返回所有成员变量组成的数组（不分public和非public属性）]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[知识点]]></title>
    <url>%2F2019%2F04%2F18%2FJava%2F%E7%9F%A5%E8%AF%86%E7%82%B9%2F</url>
    <content type="text"><![CDATA[foreach循环仅应用于实现了Iterable接口的Java array和Collection类。 在从任何Collection（例如Map、Set或List）中删除对象时总要使用Iterator的remove方法，也请谨记for-each循环只是标准Iterator代码标准用法之上的一种语法糖（syntactic sugar）而已。 使用newInstance()来创建的类，必须带有默认的构造器。]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Jenkins]]></title>
    <url>%2F2019%2F04%2F18%2FJenkins%2FJenkins%2F</url>
    <content type="text"><![CDATA[首先从Jenkins官方网站https://jenkins.io/下载最新的war包。虽然Jenkins提供了Windows、Linux、OS X等各种安装程序，但是，这些安装程序都没有war包好使。只需要运行命令：1java -jar jenkins.war Jenkins就启动成功了！它的war包自带Jetty服务器，剩下的工作我们全部在浏览器中进行。第一次启动Jenkins时，出于安全考虑，Jenkins会自动生成一个随机的按照口令。注意控制台输出的口令，复制下来，然后在浏览器输入：1http://localhost:8080/ 粘贴口令，进入安装界面，如果执行默认的安装，Jenkins就自动配置好了Maven、git等常用插件。如果插件安装失败，可以下载后离线安装。插件下载地址：http://mirrors.jenkins-ci.org/plugins/。最后，创建一个admin用户，完成安装。用管理员账号登录Jenkins后，第一次使用前，需要在“系统管理”-&gt;“Global Tool Configuration”-&gt;“Maven”中新增一个Maven，直接输入一个名字，选中“自动安装”，Jenkins会自动下载并安装Maven： 然后，在Jenkins首页选择“新建”，输入名字，选择“构建一个maven项目”： 在配置页中，源码管理选择Git，填入地址： 默认使用master分支。如果需要口令，在Credentials中添加用户名/口令，或者使用SSH Key。 构建触发器指定了触发一次构建的条件。推荐使用最简单的配置“Poll SCM”，它的意思是，定时检查版本库，发现有新的提交就触发构建。这种方式对git、SVN等所有版本管理系统都是通用的。 我们在日程表中填入：1* * * * * 表示每分钟检查一次。如果你觉得太频繁，可以改成“每3分钟检查一次”：1*/3 * * * * 在“Build”中，默认的Root POM是pom.xml。如果pom.xml不在根目录下，就填入子目录，例如：wxapi/pom.xml。 在Goals and options中，填入需要执行的mvn命令：clean package，Jenkins将执行如下命令：1mvn clean package 特殊参数也在这里填写，如-DskipTests=true clean package。保存后，就可以执行自动化构建了。 点击一个构建任务，可以在Console Output中看到控制台详细输出，便于出错排查： 如何部署如果要部署构建好的war包，可以在Post Steps中填上shell命令，直接用脚本部署。 另一种方式是创建另外一个构建项目，手动触发部署。 无论用哪种方式，都是为了确保编译、部署是通过CI服务器完成的，而不是某台开发机器。 如何创建Linux服务有了Jenkins，我们就可以在内网或者租用一台EC2服务器来搭建CI环境，每月费用不到¥100。推荐Ubuntu Linux系统。因为我们不想每次登录到Linux去启动Jenkins，也不想写脚本来启动服务。推荐安装JDK后，配合supervisor，把Jenkins直接变成一个服务。 可以在Linux上创建一个ci用户，然后，用supervisor启动并指定9001端口： 123456789# /etc/supervisor/conf.d/ci.conf[program:ci]command=java -jar /home/ci/jenkins.war --httpPort=9001user=ciautostart=trueautorestart=truestartsecs=30startretries=5 Jenkins默认在当前用户的主目录下创建.jenkins目录，所有的配置文件、数据库都存放在里面，只需要备份这个目录就备份了整个CI配置。 这样，一个CI环境就搭建完毕。 集成本地tomcat：https://blog.csdn.net/javahighness/article/details/52641694]]></content>
      <categories>
        <category>Jenkins</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot Admin入门指南]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2FSpring%20Boot%20Admin%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Spring Boot ActuatorActuator是Spring Boot的模块，它在应用中添加了REST/JMS端点，方便监控和管理应用。端点提供了健康检查、指标监控、访问日志、线程转储、堆转储和环境信息等等。 Spring Boot AdminActuator功能强大，便于其他应用使用端点（只需要简单的REST调用）。但是开发人员使用时就没那么方便了。对于开发人员，有良好的交互界面会更方便浏览监控数据和管理应用。这正是Spring Boot Admin做的工作。它为actuator端点提供了良好的交互界面，并提供了额外的特性。 Spring Boot Admin不是Spring团队提供的模块，它是由Codecentric公司创建的，代码在Github上公开。 Client And Server不像Actuator，Spring Boot Admin由两部分组成：Client和Server。 Server部分包括Admin用户界面并独立运行于被监控应用。Client部分是包含在被监控应用中，并注册到Admin Server。 这样，即使应用挂掉了或者不能正常运行，监控的Server依然正常运行。假如你有多个应用（比如Spring Boot微服务应用），每个应用运行多个实例。对于传统的Actuator监控，很难单独访问每个应用，因为你要跟踪有多少实例及它们在哪运行。 对于Spring Boot Admin，被监控应用的每个实例（Client）在启动时注册到Server，每个实例在Admin Server就有一个单点，就可以检查它们的状态了。 Server配置首先对Spring Boot Admin Server进行配置。 创建一个Spring Boot工程，可以使用Spring Initializr创建。保证包含web模块。创建工程后，第一件事就是添加Spring Boot Admin Server依赖：12345&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-starter-server&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 注意：尽管Spring Boot Admin不是由Pivotal官方开发的，但是你会发现使用Spring Initializr创建工程时可以选择Spring Boot Admin的Client和Server模块。 接着需要在启动类中加入注解@EnableAdminServer来开启Admin Server。1234567@SpringBootApplication@EnableAdminServerpublic class SpringBootAdminServerApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootAdminServerApplication.class, args); &#125;&#125; 现在运行程序并在浏览器打开http://localhost:8080/，可以看到如下界面： Server运行正常，但是没有Client注册。 Client配置和Server一样，第一步是向新建Client工程添加依赖：12345&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt; 然后指定运行的Admin Server的url，即在application.properties中添加：1spring.boot.admin.client.url=http://localhost:8080 添加Actuator现在可以同时运行Client和Server应用。只要保证没有端口冲突，因为两个应用默认端口都是8080。测试情况下，可以在application.properties中设置server.port=0，这样Client会使用一个随机端口启动。这样就可以测试使用不同的端口启动多个实例。 打开Admin Server界面就可以看到Client应用了。点击应用名称显示应用详情页。 如果你看到上面最少信息的界面，意味着你的项目中没有添加Actuator。记住，Spring Boot Admin使用Actuator端点监控应用运行状况。幸运的是，只需要在项目中添加依赖，自动配置会解决其他问题。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 但是，大部分的端点默认是不对外暴露的，所以需要在application.properties添加配置使它们暴露：1management.endpoints.web.exposure.include=* 暴露Actuator端点后就可以在Admin Server上看到更多的信息了。 更多细节可以查看这篇文章。 安全现在所有服务都能正常运行，但是我们要保证Actuator端点和Admin管理界面的安全性。 Client安全如果你已经使用了Spring Security，上面的内容不会起作用，因为Actuator端点默认是被保护的，Admin Server不能访问它们。如果没有使用Spring Security，首先需要添加依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 为了方便测试，可以配置management.security.enabled=false临时禁用Actuator端点的保护。如果使用基本身份认证，需要在配置文件中提供用户名和密码。Admin Server使用这些凭证来认证Client的Actuator端点。12spring.boot.admin.client.instance.metadata.user.name=clientspring.boot.admin.client.instance.metadata.user.password=client 默认情况下，如果没有配置Spring Boot使用默认用户user并在应用启动时自动生成密码。启动时你可以在控制台找到密码。如果你要在应用中明确指定用户名和密码，可以在配置文件中配置：12spring.security.user.name=clientspring.security.user.password=client Server安全和Client一样，在Admin Server添加依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 在application.properties配置登录Admin Server的用户名和密码：12spring.security.user.name=serverspring.security.user.password=server 然后在Client端也要添加这些凭证，否则不能注册到server。12spring.boot.admin.client.username=serverspring.boot.admin.client.password=server 回到Server模块，最后一件事是添加Spring Security配置来保证Admin管理界面的安全性。1234567891011121314151617181920212223242526@Configurationpublic class SecurityConfig extends WebSecurityConfigurerAdapter &#123; @Override protected void configure(HttpSecurity http) throws Exception &#123; SavedRequestAwareAuthenticationSuccessHandler successHandler = new SavedRequestAwareAuthenticationSuccessHandler(); successHandler.setTargetUrlParameter("redirectTo"); successHandler.setDefaultTargetUrl("/"); http.authorizeRequests() .antMatchers("/assets/**").permitAll() .antMatchers("/login").permitAll() .anyRequest().authenticated().and() .formLogin().loginPage("/login") .successHandler(successHandler).and() .logout().logoutUrl("/logout").and() .httpBasic().and() .csrf() .csrfTokenRepository(CookieCsrfTokenRepository.withHttpOnlyFalse()) .ignoringAntMatchers( "/instances", "/actuator/**" ); &#125;&#125; 这段代码的作用是：限制只有通过HTTP基本认证和登录用户可以使用Admin管理界面。登录界面和静态资源（javascript, HTML, CSS）是公开的，否则无法登录。它是基于cookie的CSRF保护。你可以看到在CSRF保护中有些路径被忽略了，因为Admin Server缺少适当的支持。 重启Server，可以看到更加美观的登录界面。 云服务发现Spring Boot Admin client并不是唯一可以注册应用到Server的方式。Admin Server也支持Spring Cloud Service Discovery。你可以通过官方文档或Spring Cloud Discovery with Spring Boot Admin查看更多信息。 通知服务监控后，如果服务出错当然希望能够得到通知。Spring Boot Admin提供了多种通知选项。 第一次访问Admin Server页面时，它会提示要求显示推送通知的权限。一旦出现问题，你会收到提示信息。 其他通知需要简单配置。一般只需要在application.properties配置就行。目前支持的服务有： Mail Slack HipChat PagerDuty OpsGenie Let’s Chat Telegram Microsoft Team 配置Email通知如果要使用Email通知，需要在Server端添加Spring email依赖：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 然后需要指定SMTP服务器，它用来发送email通知和证书。更新Admin Server的application.properties。123spring.mail.host=smtp.foo.comspring.mail.username=smtp-server-userspring.mail.password=smtp-server-password 然后指定发送者和接收者。123456# 发送邮箱spring.boot.admin.notify.mail.from=&quot;Spring Boot Admin &lt;noreply@foo.com&gt;&quot;# 接收者邮箱列表，以逗号分隔spring.boot.admin.notify.mail.to=alice@foo.com,bob@bar.com# 抄送者邮箱列表，以逗号分隔spring.boot.admin.notify.mail.cc=joe@foo.com 结论Spring Boot Admin为Actuator端点提供了漂亮的管理界面，而且它可以集中监控多个应用，这在云端和微服务中是非常有用的。但是要确保在使用过程中保证Server和Client的安全。更多信息可以查看官方文档。 参考：https://www.vojtechruzicka.com/spring-boot-admin/]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot CommandLineRunner和ApplicationRunner]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2FSpring%20Boot%20CommandLineRunner%E5%92%8CApplicationRunner%2F</url>
    <content type="text"><![CDATA[在spring boot应用中，我们可以在程序启动之前执行任何任务。为了达到这个目的，我们需要使用CommandLineRunner或ApplicationRunner接口创建bean，spring boot会自动监测到它们。这两个接口都有一个run()方法，在实现接口时需要覆盖该方法，并使用@Component注解使其成为bean。CommandLineRunner和ApplicationRunner的作用是相同的。不同之处在于CommandLineRunner接口的run()方法接收String数组作为参数，而ApplicationRunner接口的run()方法接收ApplicationArguments对象作为参数。当程序启动时，我们传给main()方法的参数可以被实现CommandLineRunner和ApplicationRunner接口的类的run()方法访问。我们可以创建多个实现CommandLineRunner和ApplicationRunner接口的类。为了使他们按一定顺序执行，可以使用@Order注解或实现Ordered接口。 CommandLineRunner和ApplicationRunner接口的run()方法在SpringApplication完成启动时执行。启动完成之后，应用开始运行。CommandLineRunner和ApplicationRunner的作用是在程序开始运行前执行任务或记录信息。 下面展示了如何在程序中使用CommandLineRunner和ApplicationRunner。 Maven配置pom.xml12345678910111213141516171819202122232425262728293031323334&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.concretepage&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;spring-demo&lt;/name&gt; &lt;description&gt;Spring Boot Demo Project&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.2.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; CommandLineRunnerCommandLineRunner是个接口，有一个run()方法。为了使用CommandLineRunner我们需要创建一个类实现该接口并覆盖run()方法。使用@Component注解实现类。当SpringApplication.run()启动spring boot程序时，启动完成之前，CommandLineRunner.run()会被执行。CommandLineRunner的run()方法接收启动服务时传过来的参数。1run(String... args) 参数为String数组。 CommandLineRunnerBean.java1234567891011121314151617package com.concretepage.bean;import java.util.Arrays;import java.util.stream.Collectors;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.CommandLineRunner;import org.springframework.stereotype.Component;@Componentpublic class CommandLineRunnerBean implements CommandLineRunner &#123; private static final Logger logger = LoggerFactory.getLogger(CommandLineRunnerBean.class); public void run(String... args) &#123; String strArgs = Arrays.stream(args).collect(Collectors.joining(&quot;|&quot;)); logger.info(&quot;Application started with arguments:&quot; + strArgs); &#125;&#125; MyApplication.java123456789101112131415161718package com.concretepage;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.ConfigurableApplicationContext;import com.concretepage.service.HelloService;@SpringBootApplicationpublic class MyApplication &#123; private static final Logger logger = LoggerFactory.getLogger(MyApplication.class); public static void main(String[] args) &#123; ConfigurableApplicationContext context = SpringApplication.run(MyApplication.class, args); HelloService service = context.getBean(HelloService.class); logger.info(service.getMessage()); &#125; &#125; 我们也可以创建一个service。一旦Spring boot启动完成service就会执行。这意味着SpringApplication.run()执行完成后service的方法就会执行。HelloService.java12345678910package com.concretepage.service;import org.springframework.stereotype.Service;@Servicepublic class HelloService &#123; public String getMessage()&#123; return &quot;Hello World!&quot;; &#125;&#125; 现在使用带有参数的可执行jar运行程序。spring-boot-demo-0.0.1-SNAPSHOT.jar为生成的jar文件。执行命令如下：1java -jar spring-boot-demo-0.0.1-SNAPSHOT.jar data1 data2 data3 输出结果为：1232017-03-19 13:38:38.909 INFO 1036 --- [ main] c.c.bean.CommandLineRunnerBean : Application started with arguments:data1|data2|data32017-03-19 13:38:38.914 INFO 1036 --- [ main] com.concretepage.MyApplication : Started MyApplication in 1.398 seconds (JVM running for 1.82)2017-03-19 13:38:38.915 INFO 1036 --- [ main] com.concretepage.MyApplication : Hello World! ApplicationRunnerApplicationRunner和CommandLineRunner的作用相同。在SpringApplication.run()完成spring boot启动之前，ApplicationRunner的run()方法会被执行。1run(ApplicationArguments args) 可以看到，CommandLineRunner.run()接收String数组作为参数，而ApplicationRunner.run()接收ApplicationArguments作为参数。这些参数是启动spring boot程序时传给main()方法的。 ApplicationRunnerBean.java12345678910111213141516171819package com.concretepage.bean;import java.util.Arrays;import java.util.stream.Collectors;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.boot.ApplicationArguments;import org.springframework.boot.ApplicationRunner;import org.springframework.stereotype.Component;@Componentpublic class ApplicationRunnerBean implements ApplicationRunner &#123; private static final Logger logger = LoggerFactory.getLogger(ApplicationRunnerBean.class); @Override public void run(ApplicationArguments arg0) throws Exception &#123; String strArgs = Arrays.stream(arg0.getSourceArgs()).collect(Collectors.joining(&quot;|&quot;)); logger.info(&quot;Application started with arguments:&quot; + strArgs); &#125;&#125; 创建可执行jar包并使用如下参数运行：1java -jar spring-boot-demo-0.0.1-SNAPSHOT.jar data1 data2 data3 输出结果为：1232017-03-19 16:26:06.952 INFO 5004 --- [ main] c.c.bean.ApplicationRunnerBean : Application started with arguments:data1|data2|data32017-03-19 16:26:06.956 INFO 5004 --- [ main] com.concretepage.MyApplication : Started MyApplication in 1.334 seconds (JVM running for 1.797)2017-03-19 16:26:06.957 INFO 5004 --- [ main] com.concretepage.MyApplication : Hello World! CommandLineRunner和ApplicationRunner的执行顺序在spring boot程序中，我们可以使用不止一个实现CommandLineRunner和ApplicationRunner的bean。为了有序执行这些bean的run()方法，可以使用@Order注解或Ordered接口。例子中我们创建了两个实现CommandLineRunner接口的bean和两个实现ApplicationRunner接口的bean。我们使用@Order注解按顺序执行这四个bean。 CommandLineRunnerBean1.java1234567@Component@Order(1)public class CommandLineRunnerBean1 implements CommandLineRunner &#123; public void run(String... args) &#123; System.out.println(&quot;CommandLineRunnerBean 1&quot;); &#125;&#125; ApplicationRunnerBean1.java12345678@Component@Order(2)public class ApplicationRunnerBean1 implements ApplicationRunner &#123; @Override public void run(ApplicationArguments arg0) throws Exception &#123; System.out.println(&quot;ApplicationRunnerBean 1&quot;); &#125;&#125; CommandLineRunnerBean2.java1234567@Component@Order(3)public class CommandLineRunnerBean2 implements CommandLineRunner &#123; public void run(String... args) &#123; System.out.println(&quot;CommandLineRunnerBean 2&quot;); &#125;&#125; ApplicationRunnerBean2.java12345678@Component@Order(4)public class ApplicationRunnerBean2 implements ApplicationRunner &#123; @Override public void run(ApplicationArguments arg0) throws Exception &#123; System.out.println(&quot;ApplicationRunnerBean 2&quot;); &#125;&#125; 输出结果为：1234CommandLineRunnerBean 1ApplicationRunnerBean 1CommandLineRunnerBean 2ApplicationRunnerBean 2]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot使用Servlet，Filter和Listener-1]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2FSpring%20Boot%E4%BD%BF%E7%94%A8Servlet%EF%BC%8CFilter%E5%92%8CListener-1%2F</url>
    <content type="text"><![CDATA[在Spring Boot应用中，Servlet，Filter和Listener可以通过两种方式注册：一是使用Spring的@Bean，二是使用内置容器通过扫描@WebServlet，@WebFilter和@WebListener注解类。 本文介绍如何使用@Bean的方式注册Servlet，Filter和Listener。 可以使用ServletRegistrationBean，FilterRegistrationBean和ServletListenerRegistrationBean分别注册Servlet，Filter和Listener。 依赖jar包编辑pom.xml文件，添加spring-boot-starter-web依赖。12345678910111213141516171819202122232425262728293031&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.boraji.tutorial.springboot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-servlet-filter-example&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;Spring Boot - Servlet, Filter and Listener example&lt;/name&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 创建Servlet，Filter和ListenerServlet1234567891011121314151617181920212223242526272829303132import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;public class MyServlet extends HttpServlet &#123; private static final long serialVersionUID = 1L; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;MyServlet&apos;s doGet() method is invoked.&quot;); doAction(req, resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;MyServlet&apos;s doPost() method is invoked.&quot;); doAction(req, resp); &#125; private void doAction(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String name = req.getParameter(&quot;name&quot;); resp.setContentType(&quot;text/plain&quot;); resp.getWriter().write(&quot;Hello &quot; + name + &quot;!&quot;); &#125; &#125; Filter123456789101112131415161718192021222324252627282930313233343536import java.io.IOException;import java.util.Enumeration;import javax.servlet.Filter;import javax.servlet.FilterChain;import javax.servlet.FilterConfig;import javax.servlet.ServletException;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;public class MyFilter implements Filter &#123; @Override public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;MyFilter doFilter() is invoked.&quot;); Enumeration&lt;String&gt; params = req.getParameterNames(); while (params.hasMoreElements()) &#123; String param=params.nextElement(); System.out.println(&quot;Parameter:&quot;+param+&quot;\tValue:&quot;+req.getParameter(param)); &#125; chain.doFilter(req, res); &#125; @Override public void init(FilterConfig config) throws ServletException &#123; &#125; @Override public void destroy() &#123; &#125;&#125; Listener123456789101112131415import javax.servlet.ServletContextEvent;import javax.servlet.ServletContextListener;public class MyServletContextListener implements ServletContextListener &#123; @Override public void contextInitialized(ServletContextEvent e) &#123; System.out.println(&quot;MyServletContextListener Context Initialized&quot;); &#125; @Override public void contextDestroyed(ServletContextEvent e) &#123; System.out.println(&quot;MyServletContextListener Context Destroyed&quot;); &#125;&#125; 注册Servlet，Filter和Listener如上所述，可以使用ServletRegistrationBean，FilterRegistrationBean和ServletListenerRegistrationBean注册Servlet，Filter和Listener。 创建SpringBootApp类并如下声明ServletRegistrationBean，FilterRegistrationBean和ServletListenerRegistrationBean的bean方法。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import javax.servlet.ServletContextListener;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.web.servlet.FilterRegistrationBean;import org.springframework.boot.web.servlet.ServletListenerRegistrationBean;import org.springframework.boot.web.servlet.ServletRegistrationBean;import org.springframework.context.annotation.Bean;import com.boraji.tutorial.springboot.filter.MyFilter;import com.boraji.tutorial.springboot.listener.MyServletContextListener;import com.boraji.tutorial.springboot.servlet.MyServlet;@SpringBootApplicationpublic class SpringBootApp &#123; // Register Servlet @Bean public ServletRegistrationBean servletRegistrationBean() &#123; ServletRegistrationBean bean = new ServletRegistrationBean( new MyServlet(), &quot;/myServlet&quot;); return bean; &#125; // Register Filter @Bean public FilterRegistrationBean filterRegistrationBean() &#123; FilterRegistrationBean bean = new FilterRegistrationBean(new MyFilter()); // Mapping filter to a Servlet bean.addServletRegistrationBeans(new ServletRegistrationBean[] &#123; servletRegistrationBean() &#125;); return bean; &#125; // Register ServletContextListener @Bean public ServletListenerRegistrationBean&lt;ServletContextListener&gt; listenerRegistrationBean() &#123; ServletListenerRegistrationBean&lt;ServletContextListener&gt; bean = new ServletListenerRegistrationBean&lt;&gt;(); bean.setListener(new MyServletContextListener()); return bean; &#125; public static void main(String[] args) &#123; SpringApplication.run(SpringBootApp.class, args); &#125;&#125; 运行运行SpringBootApp.java类。可以通过Run -&gt; Run as -&gt; Java Application或者使用mvn spring-boot：run命令运行spring boot应用。 启动成功后，在浏览器输入http://localhost:8080/myServlet?name=Sunil%20Singh%20Bora。控制台输出如下：]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot使用Servlet，Filter和Listener-2]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2FSpring%20Boot%E4%BD%BF%E7%94%A8Servlet%EF%BC%8CFilter%E5%92%8CListener-2%2F</url>
    <content type="text"><![CDATA[上一篇文章介绍了在Spring Boot应用中如何使用@Bean的方式注册Servlet，Filter和Listener。 本文介绍如何使用@WebServlet，@WebFilter和@WebListener注解类分别注册Servlet，Filter和Listener。 依赖jar包编辑pom.xml文件，添加spring-boot-starter-web依赖。12345678910111213141516171819202122232425262728293031&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.boraji.tutorial.springboot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-servlet-filter-example&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;Spring Boot - Servlet, Filter and Listener example&lt;/name&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 创建Servlet，Filter和ListenerServlet12345678910111213141516171819202122232425262728293031323334import java.io.IOException;import javax.servlet.ServletException;import javax.servlet.annotation.WebServlet;import javax.servlet.http.HttpServlet;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@WebServlet(&quot;/myServlet&quot;)public class MyServlet extends HttpServlet &#123; private static final long serialVersionUID = 1L; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;MyServlet&apos;s doGet() method is invoked.&quot;); doAction(req, resp); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println(&quot;MyServlet&apos;s doPost() method is invoked.&quot;); doAction(req, resp); &#125; private void doAction(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; String name = req.getParameter(&quot;name&quot;); resp.setContentType(&quot;text/plain&quot;); resp.getWriter().write(&quot;Hello &quot; + name + &quot;!&quot;); &#125; &#125; Filter1234567891011121314151617181920212223242526272829303132333435363738import java.io.IOException;import java.util.Enumeration;import javax.servlet.Filter;import javax.servlet.FilterChain;import javax.servlet.FilterConfig;import javax.servlet.ServletException;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import javax.servlet.annotation.WebFilter;@WebFilter(&quot;/myServlet&quot;)public class MyFilter implements Filter &#123; @Override public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain) throws IOException, ServletException &#123; System.out.println(&quot;MyFilter doFilter() is invoked.&quot;); Enumeration&lt;String&gt; params = req.getParameterNames(); while (params.hasMoreElements()) &#123; String param=params.nextElement(); System.out.println(&quot;Parameter:&quot;+param+&quot;\tValue:&quot;+req.getParameter(param)); &#125; chain.doFilter(req, res); &#125; @Override public void init(FilterConfig config) throws ServletException &#123; &#125; @Override public void destroy() &#123; &#125;&#125; Listener1234567891011121314151617import javax.servlet.ServletContextEvent;import javax.servlet.ServletContextListener;import javax.servlet.annotation.WebListener;@WebListenerpublic class MyServletContextListener implements ServletContextListener &#123; @Override public void contextInitialized(ServletContextEvent e) &#123; System.out.println(&quot;MyServletContextListener Context Initialized&quot;); &#125; @Override public void contextDestroyed(ServletContextEvent e) &#123; System.out.println(&quot;MyServletContextListener Context Destroyed&quot;); &#125;&#125; 注册Servlet，Filter和Listener注册servlet组件（如Servlet，Filter和Listener），需要使用@ServletComponentScan注解配置类。@ServletComponentScan这个注解会扫描指定包中的servlet组件。 创建SpringBootApp类，并使用注解@ServletComponentScan和@SpringBootApplication。12345678910111213141516import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.web.servlet.ServletComponentScan;@SpringBootApplication@ServletComponentScan(basePackages = &#123; &quot;com.boraji.tutorial.springboot.servlet&quot;, &quot;com.boraji.tutorial.springboot.filter&quot;, &quot;com.boraji.tutorial.springboot.listener&quot; &#125;)public class SpringBootApp &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringBootApp.class, args); &#125;&#125; 运行运行SpringBootApp.java类。可以通过Run -&gt; Run as -&gt; Java Application或者使用mvn spring-boot：run命令运行spring boot应用。 启动成功后，在浏览器输入http://localhost:8080/myServlet?name=Sunil%20Singh%20Bora。]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot依赖引入的多种方式]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2FSpring%20Boot%E4%BE%9D%E8%B5%96%E5%BC%95%E5%85%A5%E7%9A%84%E5%A4%9A%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[使用Spring Boot开发，不可避免的会面临Maven依赖包版本的管理。 有如下几种方式可以管理Spring Boot的版本。 使用parent继承123456789101112131415161718&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.example&lt;/groupId&gt; &lt;artifactId&gt;myproject&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;!-- Additional lines to be added here... --&gt;&lt;/project&gt; 使用parent继承的方式，简单，方便使用。但是有的时候项目又需要继承其他的prarent，这个时候parent继承的方式就满足不了需求了。不过不用担心，还有其他方式。 使用import方式123456789101112&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;!-- Import dependency management from Spring Boot --&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;2.0.0.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; 在parent的pom文件中，声明dependencyManagement，这样在实际的项目pom文件中，直接声明需要的spring boot包就可以，不需要填写version属性。 还有一种是使用maven plugin。 使用Spring boot Maven插件12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; spring boot依赖管理，根据不同的实际需求，选择不同的管理方式，可以大大提高效率。]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot实战]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2FSpring%20Boot%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[第1章 入门1.1 Spring风云再起1.1.1 重新认识Spring1.1.2 Spring Boot精要四个核心： 自动配置：针对很多Spring应用程序常见的应用功能，Spring Boot能自动提供相关配置。 起步依赖：告诉Spring Boot需要什么功能，他就能引入需要的库。 命令行界面：这是Spring Boot的可选特性，借此你只需写代码就能完成完整的应用程序，无需传统项目构建。 Actuator：让你能够深入运行中的Spring Boot应用程序，一探究竟。 1.1.3 Spring Boot不是什么 Spring Boot不是应用服务器，只是嵌入了Servlet容器。 Spring Boot没有实现诸如JPA或JMS之类的企业级Java规范，不过它自动配置了某个JPA实现的Bean，以此支持JPA。 Spring Boot没有引入任何形式的代码生成，而是利用了Spring 4的条件化配置特性，以及Maven和Gradle提供的传递依赖解析，以此实现Spring应用程序上下文里的自动配置。 1.2 Spring Boot入门1.2.1 安装Spring Boot CLISpring Boot CLI有好几种安装方式： 用下载的分发包进行安装 用Groovy Environment Manager进行安装 通过OS X Homebrew进行安装 使用MacPorts进行安装 1.2.2 使用Spring Initializr初始化Spring Boot项目Spring Initializr有几种用法： 通过Web界面使用 通过Spring Tool Suite使用 通过IntelliJ IDEA使用 使用Spring Boot CLI使用 使用Spring Initializr的Web界面http://start.spring.io 在Spring Tool Suite里创建Spring Boot项目 在IntelliJ IDEA里创建Spring Boot项目 在Spring Boot CLI里使用Initializr 1.3 小结第2章 开发第一个应用程序2.1 运用Spring Boot2.1.1 查看初始化的Spring Boot新项目@SpringBootApplication将三个有用的注解组合在一起： Spring的@Configuration Spring的@ComponentScan Spring Boot的@EnableAutoConfiguration 2.1.2 Spring Boot项目构建过程解析Spring Boot为Gradle和Maven提供了构建插件，以便辅助构建Spring Boot项目。 要是选择用Maven来构建应用程序， Initializr会替你生成一个pom.xml文件，其中使用了Spring Boot的Maven插件。123456&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;&#123;springBootVersion&#125;&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt; pom.xml里的没有指定版本。 2.2 使用起步依赖只在构建文件里指定这些功能，让构建过程自己搞明白我们要什么东西，这正是Spring Boot起步依赖的功能。 2.2.1 指定基于功能的依赖Spring Boot通过提供众多起步依赖降低项目依赖的复杂度。起步依赖本质上是一个Maven项目对象模型（Project Object Model， POM），定义了对其他库的传递依赖，这些东西加在一起即支持某项功能。很多起步依赖的命名都暗示了它们提供的某种或某类功能。 我们并不需要指定版本号，起步依赖本身的版本是由正在使用的Spring Boot的版本来决定的，而起步依赖则会决定它们引入的传递依赖的版本。 查看依赖树：12gradle dependenciesmvn dependency:tree 2.2.2 覆盖起步依赖引入的传递依赖Gradle123compile(&quot;org.springframework.boot:spring-boot-starter-web&quot;) &#123; exclude group: &apos;com.fasterxml.jackson.core&apos;&#125; Maven123456789&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; 2.3 使用自动配置2.3.1 专注于应用程序功能2.3.2 运行应用程序2.3.3 刚刚发生了什么Spring条件化配置：实现Condition接口，覆盖它的matches()方法。 2.4 小结起步依赖帮助你专注于应用程序需要的功能类型，而非提供该功能的具体版本。与此同时，自动配置把你从样板式的配置中解放了出来。这些配置在没有Spring Boot的Spring应用程序里非常常见。 第3章 自定义配置两种影响自动配置的方式——使用显式配置进行覆盖和使用属性进行精细化配置 3.1 覆盖Spring Boot自动配置3.1.1 保护应用程序123456compile(&quot;org.springframework.boot:spring-boot-starter-security&quot;)&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-security&lt;/artifactId&gt;&lt;/dependency&gt; 用户名：user，密码：在打印日志中 3.1.2 创建自定义的安全配置3.1.3 掀开自动配置的神秘面纱3.2 通过属性文件外置配置Spring Boot自动配置的Bean提供了300多个用于微调的属性。 Spring Boot应用程序有多种设置途径。Spring Boot能从多种属性源获得属性，包括如下几处： 命令行参数 java:comp/env里的JNDI属性 JVM系统属性 操作系统环境变量 随机生成的带random.*前缀的属性（在设置其他属性时，可以引用它们，比如${random.long}） 应用程序以外的application.properties或者application.yml文件 打包在应用程序内的application.properties或者application.yml文件 通过@PropertySource标注的属性源 默认属性 这个列表按照优先级排序 application.properties和application.yml文件能放在以下四个位置： 外置，在相对于应用程序运行目录的/config子目录里 外置，在应用程序运行的目录里 内置，在config包内 内置，在Classpath根目录 这个列表按照优先级排序 如果在同一优先级位置同时有application.properties和application.yml，那么==application. yml里的属性会覆盖application.properties里的属性==。（==写反了？==） 3.2.1 自动配置微调禁用模板缓存spring.thymeleaf.cache=false 配置嵌入式服务器server.port=8080 HTTPS服务： 用JDK的keytool工具来创建一个密钥存储（keystore）1234server.port = 8443server.ssl.key-store = classpath:mykeys.jksserver.ssl.key-store-password = 123456server.ssl.key-password = 123456 配置日志 配置数据源 3.2.2 应用程序Bean的配置外置Spring Boot的属性解析器会自动把驼峰规则的属性和使用连字符或下划线的同名属性关联起来。 3.2.3 使用Profile进行配置@Profile 设置spring.profiles.active属性就能激活Profile，任意设置配置属性的方式都能用于设置这个值。 3.3 定制应用程序错误页面第4章 测试Spring的SpringJUnit4ClassRunner可以在基于Junit的应用程序测试里加载Spring应用程序上下文。 使用特定于Profile的属性文件使用application.properties可以创建额外的属性文件，遵循application-{profile}.properties这种命名格式，就能提供特定于Profile的属性。 使用多Profile YAML文件进行配置如果使用YAML来配置属性，则可以遵循与配置文件相同的命名规范，即创建application-{profile}.yml这样的YAML文件，并将与Profile无关的属性继续放在application.yml里。但既然用了YAML，就可以把所有Profile的配置属性都放在一个application.yml文件里。使用一组三个连字符（—）作为分隔符。 4.1 集成测试自动配置在大多数情况下，为Spring Boot应用程序编写测试时应该用@SpringApplicationConfiguration代替@ContextConfiguration。 4.2 测试Web应用程序两个方案： Spring Mock MVC Web集成测试 4.2.1 模拟Spring MVC要在测试里设置Mock MVC，可以使用MockMvcBuilders，该类提供了两个静态方法。 standaloneSetup()：构建一个Mock MVC，提供一个或多个手工创建并配置的控制器。 webAppContextSetup()：使用Spring应用程序上下文来构建Mock MVC，该上下文里可以包含一个或多个配置好的控制器。 两者的主要区别在于， standaloneSetup()希望你手工初始化并注入你要测试的控制器，而webAppContextSetup()则基于一个WebApplicationContext的实例，通常由Spring加载。前者同单元测试更加接近，你可能只想让它专注于单一控制器的测试，而后者让Spring加载控制器及其依赖，以便进行完整的集成测试。 4.2.2 测试Web安全经过身份验证的请求又该如何发起呢？ Spring Security提供了两个注解。 @WithMockUser：加载安全上下文，其中包含一个UserDetails，使用了给定的用户名、密码和授权。 @WithUserDetails：根据给定的用户名查找UserDetails对象，加载安全上下文。 4.3 测试运行中应用程序@WebIntegrationTest 4.3.1 用随机端口启动服务器@SpringApplicationConfiguration和@WebIntegrationTest已经被@SpringBootTest替代。 4.3.2 使用Selenium测试HTML页面testCompile(&quot;org.seleniumhq.selenium:selenium-java:2.45.0&quot;) 第5章 Spring Boot CLISpring Boot CLI施展了很多技能。 CLI可以利用Spring Boot的自动配置和起步依赖。 CLI可以检测到正在使用的特定类，自动解析合适的依赖库来支持那些类。 CLI知道多数常用类都在哪些包里，如果用到了这些类，它会把那些包加入Groovy的默认包里。 应用自动依赖解析和自动配置后， CLI可以检测到当前运行的是一个Web应用程序，并自动引入嵌入式Web容器（默认是Tomcat）供应用程序使用。 第7章 深入Actuator7.1 揭秘Actuator的端点Actuator的端点 HTTP方法 路 径 描 述 GET /autoconfig 提供了一份自动配置报告，记录哪些自动配置条件通过了，哪些没通过 GET /configprops 描述配置属性（包含默认值）如何注入Bean GET /beans 描述应用程序上下文里全部的Bean，以及它们的关系 GET /dump 获取线程活动的快照 GET /env 获取全部环境属性 GET /env/{name} 根据名称获取特定的环境属性值 GET /health 报告应用程序的健康指标，这些值由HealthIndicator的实现类提供 GET /info 获取应用程序的定制信息，这些信息由info打头的属性提供 GET /mappings 描述全部的URI路径，以及它们和控制器（包含Actuator端点）的映射关系 GET /metrics 报告各种应用程序度量信息，比如内存用量和HTTP请求计数 GET /metrics/{name} 报告指定名称的应用程序度量值 POST /shutdown 关闭应用程序，要求endpoints.shutdown.enabled设置为true GET /trace 提供基本的HTTP请求跟踪信息（时间戳、 HTTP头等） 7.2 连接Actuator的远程shellGradle：1compile(&quot;org.springframework.boot:spring-boot-starter-remote-shell&quot;) Maven：1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-remote-shell&lt;/artifactId&gt;&lt;/dependency&gt; ssh user@localhost -p 2000 密码在日志中 第8章 部署Spring Boot应用程序8.1 衡量多种部署方式Spring Boot应用程序有多种构建和运行方式： 在IDE中运行应用程序（涉及Spring ToolSuite或IntelliJ IDEA）。 使用Maven的spring-boot:run或Gradle的bootRun，在命令行里运行。 使用Maven或Gradle生成可运行的JAR文件，随后在命令行中运行。 使用Spring Boot CLI在命令行中运行Groovy脚本。 使用Spring Boot CLI来生成可运行的JAR文件，随后在命令行中运行。 8.2 部署到应用服务器8.2.1 构建WAR文件Gradle：1234567apply plugin: &apos;war&apos;war &#123; baseName = &apos;test&apos; version = &apos;0.0.1-SNAPSHOT&apos; archiveName = &apos;demo.war&apos;&#125; archiveName：打包后的包名 Maven：&lt;packaging&gt;war&lt;/packaging&gt; 要使用SpringBootServletInitializer，只需创建一个子类，覆盖configure()方法来指定Spring配置类。 123456public class TestServletInitializer extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(Ch08DeployApplication.class); &#125;&#125; 构建命令： gradle build mvn package 如果没有删除Application里的main()方法，构建过程生成的WAR文件仍可直接运行，一如可执行的JAR文件： java -jar readinglist-0.0.1-SNAPSHOT.war 这样一来，同一个部署产物就能有两种部署方式了！ 如果war包放在外部容器中运行，需要使用SpringBootServletInitializer，如果通过java -jar运行则不需要。 Spring Boot开发者工具1compile &quot;org.springframework.boot:spring-boot-devtools&quot; 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt;&lt;/dependency&gt; 自动重启IDEA不能自动重启的解决办法：http://blog.csdn.net/u012410733/article/details/54316548 可以设置spring.devtools.restart.exclude属性来覆盖默认的重启排除目录。 如果想彻底关闭自动重启，可以将spring.devtools.restart.enabled设置为false。 还可以设置一个触发文件，必须修改这个文件才能触发重启。只需设置spring.devtools.restart.triggerfile属性。 全局配置开发者工具可以在主目录（home directory）里创建一个名为.spring-boot-devtools.properties的文件。（请注意，文件名用“ .”开头。） 要是想覆盖这些配置，可以在每个项目的application.properties或application.yml文件里设置特定于每个项目的属性。 Spring Boot依赖如果你正在使用的起步依赖没有覆盖到某个库，而你需要使用这个库，那就得在Maven或Gradle的构建说明里显式地声明这个依赖。 请注意，在这两种情况下，都不需要指定版本号， Spring Boot的依赖管理会替你处理这个问题的。但是，如果想覆盖Spring Boot选择的版本，你也可以显式地提供一个版本号。]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[如何把Spring Boot项目的war包部署到tomcat]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2F%E5%A6%82%E4%BD%95%E6%8A%8ASpring%20Boot%E9%A1%B9%E7%9B%AE%E7%9A%84war%E5%8C%85%E9%83%A8%E7%BD%B2%E5%88%B0tomcat%2F</url>
    <content type="text"><![CDATA[Spring Boot默认提供内嵌的tomcat，所以打包直接生成jar包，用java -jar命令就可以启动。但是，有时候我们更希望一个tomcat来管理多个项目，这种情况下就需要项目是war格式的包而不是jar格式的包。 本文介绍如何把Spring Boot项目的war包部署到tomcat。 把Spring Boot的war包部署到tomcat，需要三步： 继承SpringBootServletInitializer 标记嵌入的servlet容器为provided 更新packaging为war 1. 继承SpringBootServletInitializer修改已存在的@SpringBootApplication类，使其继承SpringBootServletInitializer。 1.1 典型的Spring Boot JAR部署（更新该文件以支持war包部署）1234567891011import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class DeployWarApplication extends SpringBootServletInitializer &#123; public static void main(String[] args) &#123; SpringApplication.run(DeployWarApplication.class, args); &#125;&#125; 1.2 Spring Boot war包部署1234567891011121314151617import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.boot.web.support.SpringBootServletInitializer;@SpringBootApplicationpublic class DeployWarApplication extends SpringBootServletInitializer &#123; public static void main(String[] args) &#123; SpringApplication.run(DeployWarApplication.class, args); &#125; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder builder) &#123; return builder.sources(DeployWarApplication.class); &#125;&#125; 如果创建了额外的DeployWarApplication类部署，确保高速Spring Boot启动哪个main类。123&lt;properties&gt; &lt;start-class&gt;com.zkzong.NewSpringBootWebApplicationForWAR&lt;/start-class&gt;&lt;/properties&gt; 2. 标记内嵌的servlet容器为provided12345&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 3. 更新packaging为war1&lt;packaging&gt;war&lt;/packaging&gt; 需要注意的是这样部署的request url需要在端口后加上项目的名字才能正常访问。spring-boot更加强大的一点就是：即便项目是以上配置，依然可以用内嵌的tomcat来调试，启动命令和以前没变，还是：mvn spring-boot:run。 如果需要在spring boot中加上request前缀，需要在application.properties中添加server.contextPath=/prefix/即可。其中prefix为前缀名。这个前缀会在war包中失效，取而代之的是war包名称，如果war包名称和prefix相同的话，那么调试环境和正式部署环境就是一个request地址了。 参考：https://zhidao.baidu.com/question/1897916337731983300.htmlhttps://www.mkyong.com/spring-boot/spring-boot-deploy-war-file-to-tomcat/]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[实践Druid作为Spring Boot工程的数据源添加SQL监控]]></title>
    <url>%2F2019%2F04%2F18%2FSpring%20Boot%2F%E5%AE%9E%E8%B7%B5Druid%E4%BD%9C%E4%B8%BASpring%20Boot%E5%B7%A5%E7%A8%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E6%BA%90%E6%B7%BB%E5%8A%A0SQL%E7%9B%91%E6%8E%A7%2F</url>
    <content type="text"><![CDATA[在大型业务系统上线后，为了保证系统能够更好地持续稳定运行，及时发现各种故障（代码缺陷、SQL性能问题、服务器CPU/磁盘参数指标和各类业务异常等），因此需要针对系统开发各种监控功能。在微服务架构下的各类业务平台中，针对SQL进行监控，并根据业务的发展情况及时进行调优尤为重要。如果让中间件或者业务研发团队自己根据业务特征定制化开发一套SQL的监控系统，可能既费时费力，又不一定能够达到预定的结果。本文将介绍业界较为流行的Druid数据源连接池插件，并跟其他几款热门的数据源连接池进行对比分析，最后给出在Spring Boot工程中集成该数据源连接池的实践方法。 1. Druid数据库连接池介绍Druid数据源连接池来源于阿里巴巴，是淘宝和支付宝专用数据库连接池。事实上，它不仅仅是一个数据库连接池，还包含一个ProxyDriver、一系列内置的JDBC组件库、一个 SQL Parser。支持所有JDBC兼容的数据库，包括Oracle、MySql、Derby、Postgresql、SQL Server、H2等等。Druid针对Oracle和MySql做了特别优化，比如Oracle的PSCache内存占用优化，MySql的ping检测优化。Druid提供了诸如MySql、Oracle、Postgresql、SQL-92等SQL语句的完美支持，是一个手写的高性能SQL Parser，支持Visitor模式，使得分析SQL的抽象语法树很方便。它执行简单SQL语句耗时在10微秒以内，对于复杂的SQL语句耗时也在30微秒左右。另外，通过Druid提供的SQL Parser可以在JDBC层面上拦截SQL并进行相应处理，比如说分库分表、SQL安全审计等。Druid也能防御SQL注入攻击，WallFilter就是通过Druid的SQL Parser分析语义实现的。 2. 与其他几种数据库连接池进行对比通过下面的表格先来看下Druid与当前比较流行的其他几款数据源连接池的对比： 功能 dbcp druid c3p0 tomcat-jdbc HikariCP 支持PSCache 是 是 是 否 否 监控 jmx jmx/log/http jmx,log jmx jmx 扩展 弱 好 弱 弱 弱 sql拦截及解析 无 支持 无 无 无 代码 简单 中等 复杂 简单 简单 特点 依赖common-pool 阿里开源，功能全面 代码逻辑复杂，且不易维护 功能简单，起源于boneCP 连接池管理 LinkedBlockingDeque 数组 更新 FairBlockingQueue threadlocal+CopyOnWriteArrayList 从上面对比的表格中，可以看到druid功能最为全面，具备sql拦截等功能，其中统计数据较为全面，具有良好的扩展性。虽然在性能方面比HikariCP略差，但是综合其他方面来考虑在做技术选型的时候，可以选择Druid作为数据源连接池组件来用。 3. 动手在Spring-Boot工程中添加Druid实践本文前面两节都是主要讲了理论，相对比较枯燥。下面这一节将从实践的角度，来一步一步向大家展示如何在Spring Boot工程中添加Druid连接池进行业务级的SQL监控。 版本环境Spring Boot 1.4.1.RELEASE、Druid 1.0.12、JDK 1.8 在工程中添加Druid的pom依赖因为阿里开源了Druid的数据源连接池源码，我们可以通过maven仓库可以获得jar包依赖。访问http://mvnrepository.com/artifact/com.alibaba/druid选择自己项目需要的版本（在本次集成中选择的是1.0.12），点击进入后复制maven内容到pom.xml内即可，如下所示：12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.12&lt;/version&gt;&lt;/dependency&gt; 在自己工程中添加完以上Druid数据源连接池的依赖后，记得在Intellij中点击下”Enable Auto import”选项即可自动下载maven依赖的jar到本地.m2目录并构建到项目中。添加Druid至Spring Boot工程中就这么Easy，这么快捷。 在Spring Boot工程中添加Druid配置在上面我们已经将Druid添加至项目中，接下来需要修改Spring Boot的application.yml配置文件，来添加Druid数据源连接池的支持，如下所示：123456789101112131415161718192021222324server.port=8888# 数据库访问配置spring.datasource.type=com.alibaba.druid.pool.DruidDataSourcespring.datasource.driver-class-name=com.mysql.jdbc.Driverspring.datasource.url=jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=gbk&amp;zeroDateTimeBehavior=convertToNull&amp;useSSL=falsespring.datasource.username=rootspring.datasource.password=root# 下面为连接池的补充设置，应用到上面所有数据源中spring.datasource.initialSize=5spring.datasource.minIdle=5spring.datasource.maxActive=20# 配置获取连接等待超时的时间spring.datasource.maxWait=60000# 配置间隔多久才进行一次检测，检测需要关闭的空闲连接，单位是毫秒spring.datasource.timeBetweenEvictionRunsMillis=60000# 配置一个连接在池中最小生存的时间，单位是毫秒spring.datasource.minEvictableIdleTimeMillis=300000spring.datasource.validationQuery=SELECT 1 FROM DUALspring.datasource.testWhileIdle=truespring.datasource.testOnBorrow=falsespring.datasource.testOnReturn=false# 配置监控统计拦截的filters，去掉后监控界面sql无法统计，&apos;wall&apos;用于防火墙spring.datasource.filters=stat,wall,log4jspring.datasource.logSlowSql=true 需要说明的是，上面配置中的filters：stat表示已经可以使用监控过滤器，这时结合定义一个过滤器，我们就可以用其来监控SQL的执行情况。 开启Druid的SQL监控功能在工程中开启监控功能后，可以在工程应用运行过程中，通过Druid数据源连接池自带SQL监控提供的多维度数据，分析出业务SQL执行的情况，从而可以调整和优化代码以及SQL，方便业务开发同事调优数据库的访问性能。 要达到开启SQL监控的效果，还需在Spring Boot工程中还实现Druid数据源连接池的Serlvet以及Filter，其Bean的初始化代码如下（下面给出两种配置方式）： 第一种方式@Confing注解的配置类：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/** * druid 配置. * &lt;p&gt; * 这样的方式不需要添加注解：@ServletComponentScan * * @author Administrator */@Configurationpublic class DruidConfiguration &#123; /** * 注册一个StatViewServlet * * @return */ @Bean public ServletRegistrationBean DruidStatViewServle2() &#123; //org.springframework.boot.context.embedded.ServletRegistrationBean提供类的进行注册. ServletRegistrationBean servletRegistrationBean = new ServletRegistrationBean(new StatViewServlet(), &quot;/druid/*&quot;); servletRegistrationBean.addUrlMappings(&quot;/druid/*&quot;); //添加初始化参数：initParams //白名单： servletRegistrationBean.addInitParameter(&quot;allow&quot;, &quot;127.0.0.1&quot;); //IP黑名单 (存在共同时，deny优先于allow) : 如果满足deny的话提示:Sorry, you are not permitted to view this page. servletRegistrationBean.addInitParameter(&quot;deny&quot;, &quot;192.168.1.73&quot;); //登录查看信息的账号密码. servletRegistrationBean.addInitParameter(&quot;loginUsername&quot;, &quot;admin&quot;); servletRegistrationBean.addInitParameter(&quot;loginPassword&quot;, &quot;admin&quot;); //是否能够重置数据. servletRegistrationBean.addInitParameter(&quot;resetEnable&quot;, &quot;false&quot;); return servletRegistrationBean; &#125; /** * 注册一个：filterRegistrationBean * * @return */ @Bean public FilterRegistrationBean druidStatFilter2() &#123; FilterRegistrationBean filterRegistrationBean = new FilterRegistrationBean(new WebStatFilter()); //添加过滤规则. filterRegistrationBean.addUrlPatterns(&quot;/*&quot;); //添加不需要忽略的格式信息. filterRegistrationBean.addInitParameter(&quot;exclusions&quot;, &quot;*.js,*.gif,*.jpg,*.png,*.css,*.ico,/druid2/*&quot;); return filterRegistrationBean; &#125;&#125; 第二种方式基于注解的配置：123456789101112131415161718/** * druid数据源状态监控. * * @author Administrator */@WebServlet(urlPatterns = &quot;/druid/*&quot;, initParams = &#123; @WebInitParam(name = &quot;allow&quot;, value = &quot;192.168.1.72,127.0.0.1&quot;),// IP白名单(没有配置或者为空，则允许所有访问) @WebInitParam(name = &quot;deny&quot;, value = &quot;192.168.1.73&quot;),// IP黑名单 (存在共同时，deny优先于allow) @WebInitParam(name = &quot;loginUsername&quot;, value = &quot;admin&quot;),// 用户名 @WebInitParam(name = &quot;loginPassword&quot;, value = &quot;admin&quot;),// 密码 @WebInitParam(name = &quot;resetEnable&quot;, value = &quot;false&quot;)// 禁用HTML页面上的“Reset All”功能 &#125;)public class DruidStatViewServlet extends StatViewServlet &#123; private static final long serialVersionUID = 1L;&#125; 12345678910111213/** * druid过滤器. * * @author Administrator */@WebFilter(filterName = &quot;druidWebStatFilter&quot;, urlPatterns = &quot;/*&quot;, initParams = &#123; @WebInitParam(name = &quot;exclusions&quot;, value = &quot;*.js,*.gif,*.jpg,*.bmp,*.png,*.css,*.ico,/druid/*&quot;)//忽略资源 &#125;)public class DruidStatFilter extends WebStatFilter &#123;&#125; 使用上面第二种方式的话，还需要在Spring Boot工程的启动类上添加注解：@ServletComponentScan，这样使Spring能够扫描到我们自己编写的servlet和filter。 使用Druid进行SQL监控的效果我们已经配置完成了Druid的监控，在本地运行Spring Boot的Jar包，运行成功后即可访问Druid监控界面，默认访问地址为：http://localhost:8080/druid/，最终的效果图如下所示： 可以看到了我们成功的访问了Druid的监控页面，那么现在输入我们在Bean初始化时候设置的用户名、密码（admin/admin）登录监控平台，进入监控平台首页，如下所示： 有了Web UI我们就可以方便的从这个UI上看到该工程部署起来后数据源初始化配置以及业务级SQL的执行情况。 4. 总结本文围绕Druid数据源连接池为主题，先简要地介绍了该连接池的功能，然后通过与业界几款较为流行的数据源连接池进行横向对比，分析出Druid连接池的特色和优势。最后通过实践，进一步向大家阐述如何在一个Spring Boot工程中添加Druid连接池进行业务SQL级别的监控。]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JDK中涉及的设计模式]]></title>
    <url>%2F2019%2F04%2F18%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2FJDK%E4%B8%AD%E6%B6%89%E5%8F%8A%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[工厂方法模式java.util.Calendar.getInstancejava.text.NumberFormat.getInstance 单例模式java.lang.Runtime.getRuntime()java.awt.Desktop 代理模式java.lang.reflect.Proxyjava.rmi.*包下的所有类]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[正则表达式必知必会]]></title>
    <url>%2F2019%2F04%2F16%2F%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%BF%85%E7%9F%A5%E5%BF%85%E4%BC%9A%2F</url>
    <content type="text"><![CDATA[正则表达式必知必会 正则表达式的两种基本用途：搜索和替换。 正则表达式是一些用来匹配和处理文本的字符串。 正则表达式是区分字母大小写的。 .字符（英文句号）可以匹配任何一个单个的字符。正则表达式里的.字符相当于DOS的?字符，相当于SQL中的_（下划线）字符。 \是一个元字符（metacharacter，表示“这个字符有特殊含义，而不是字符本身含义”）。 [和]定义一个字符集合。 字符区间可以用-（连字符）来定义。 ^ 取非匹配 匹配数字（与非数字）|元字符|说明||:–:|:–:||\d|任何一个数字字符（等价于[0-9]）||\D|任何一个非数字字符（等价于[^0-9]）| 匹配字母和数字（与非字母和数字）|元字符|说明||:–:|:–:||\w|任何一个字母数字字符（大小写均可）或下划线字符（等价于[a-zA-Z0-9_]）||\W|任何一个非字母数字或下划线字符（等价于[^a-zA-Z0-9_]）| 匹配空白字符（与非空白字符）|元字符|说明||:–:|:–:||\s|任何一个空白字符（等价于[\f\n\r\t\v]）||\S|任何一个非空白字符（等价于[^\f\n\r\t\v]）| 匹配十六进制或八进制数值在正则表达式里，十六进制数值要用前缀\x来给出，八进制数值要用前缀\0来给出。 使用POSIX字符类（JavaScript不支持） +匹配一个或多个字符（至少一个；不匹配零个字符的情况）。 当在字符集合里使用的时候，像.和+这样的元字符将被解释为普通字符，不需要被转义。 *匹配字符（或字符集合连续出现零次或多次的情况）。 ?只能匹配一个字符（或字符集合）的零次或一次出现，最多不超过一次。 重复次数要用{和}字符来给出——把数值写在它们之间。 防止过度匹配|贪婪型元字符|懒惰型元字符||:–:|:–:|| | ?||+|+?||{n,}|{n,}?| 单词边界：\b用来匹配一个单词的开始或结尾，不匹配一个单词边界，使用\B。 字符串边界：用来定义字符串边界的元字符有两个：一个是用来定义字符串开头的^，另一个是用来定义字符串结尾的$。 子表达式必须用(和)括起来。 一般来说，元字符.不匹配换行符。 回溯引用指的是模式的后半部分引用在前半部分中定义的子表达式。 回溯引用只能用来引用模式里的子表达式（用(和)括起来的正则表达式片段）。 回溯引用匹配通常从1开始计数（\1、\2，等等）。在许多实现里，第0个匹配（\0）可以用来代表整个正则表达式。 回溯引用在替换操作中的应用() $ 大小写转换|元字符|说明||:–:|:–:||\E|结束\L或\U转换||\l|把下一个字符转换为小写||\L|把\L到\E之间的字符全部转换为小写||\u|把下一个字符转换为大写||\U|把\U到\E之间的字符全部转换为大写| 向前查找模式就是一个以?=开头的子表达式，需要匹配的文本跟在=的后面。 向后查找操作符时?&lt;=。 向前查找模式的长度是可变的，它们可以包含.和+之类的元字符。而向后查找模式只能是固定长度。 对前后查找取非|操作符|说明||:–:|:–:||(?=)|正向前查找||(?!)|负向前查找||(?&lt;=)|正向后查找||(?&lt;!)|负向后查找|]]></content>
      <categories>
        <category>正则表达式</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Git Command]]></title>
    <url>%2F2019%2F04%2F16%2FGit%2FGit%20Command%2F</url>
    <content type="text"><![CDATA[github-git-cheat-sheet Git Reference git-cheat-sheet-detail]]></content>
      <categories>
        <category>Git</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Java解惑]]></title>
    <url>%2F2019%2F04%2F16%2FJava%2FJava%E8%A7%A3%E6%83%91%2F</url>
    <content type="text"><![CDATA[Java解惑 第2章 表达式之谜 奇数性123public static boolean isOdd(int i) &#123; return i % 2 == 1;&#125; 当取余操作返回一个非零的结果时，它与左操作数具有相同的正负符号。当i是一个负奇数时，i%2等于-1而不是1。123public static boolean isOdd(int i) &#123; return i % 2 != 0;&#125; 找零时刻12345public class Change &#123; public static void main(String[] args) &#123; System.out.println(2.00-1.10); &#125;&#125; 问题在于1.1这个数字不能被精确表示为一个double，因此被表示为最接近它的double值。并不是所有的小数都可以用二进制浮点数精确表示。二进制浮点对于货币计算是非常不合适的。解决该问题的一种方式是使用某种整数类型。解决该问题的另一种方式是使用执行精确小数运算的BigDecimal。一定要用BigDecimal(String)构造器，而千万不要用BigDecimal(double)。在需要精确答案的地方，要避免使用float和double；对于货币计算，要使用int、long或BigDecimal。 长整除 1234567public class LongDivision &#123; public static void main(String[] args) &#123; final long MICROS_PER_DAY = 24 * 60 * 60 * 1000 * 1000; final long MILLIS_PER_DAY = 24 * 60 * 60 * 1000; System.out.println(MICROS_PER_DAY/MILLIS_PER_DAY); &#125;&#125; final long MICROS_PER_DAY = 24L 60 60 1000 1000; final long MILLIS_PER_DAY = 24L 60 60 * 1000;当在操作很大的数字时，千万要提防溢出——它可是一个缄默杀手。 初级问题12345public class Elementary &#123; public static void main(String[] args) &#123; System.out.println(12345 + 5432l); &#125;&#125; 在long类型字面常量中，一定要用大写的L，千万不要用小写的l。要避免使用单个l字母作为变量名。 十六进制的趣事12345public class JoyOfHex &#123; public static void main(String[] args) &#123; System.out.println(Long.toHexString(0x100000000L + 0xcafebabe)); &#125;&#125; 负的十进制常量可以很明确地用一个减号来标识。如果十六进制和八进制字面常量的最高位被置位了，那么它们就是负数。System.out.println(Long.toHexString(0x100000000L + 0xcafebabeL));通常最好是避免混合类型的计算。 多重转型12345public class Multicast &#123; public static void main(String[] args) &#123; System.out.println((int)(char)(byte)-1); &#125;&#125; 如果最初的数值类型是有符号的，就执行符号扩展；如果它是char，那么不管它将要被转换成什么类型，都执行零扩展。 互换内容12345678public class CleverSwap &#123; public static void main(String[] args) &#123; int x = 1984; int y = 2001; x ^= y ^= x ^= y; System.out.println("x="+x+";y="+y); &#125;&#125; C/C++中运行正确。Java中不正确。操作符的操作数是从左向右求值的。实际运行过程如下：1234567int tmp1 = x;int tmp2 = y;int tmp3 = x ^ y;x = tmp3;y = tmp2 ^ tmp3;x = tmp1 ^ y;y = (x ^= (y ^= x)) ^ y; 在单个表达式中不要对相同的变量赋值两次。 Dos Equis12345678public class DosEquis &#123; public static void main(String[] args) &#123; char x= 'X'; int i = 0; System.out.println(true ? x : 0); System.out.println(false ? i : x); &#125;&#125; 混合类型的计算会引起混乱，而这一点在条件表达式中比在其他任何地方都表现得更明显。确定条件表达式结果类型的规则：（1） 如果第二个和第三个操作数具有相同的类型，那么它就是条件表达式的类型。（2） 如果一个操作数的类型是T，T表示byte、short、或char，而另一个操作数是一个int类型的常数表达式，它的值可以用类型T表示，那么条件表达式的类型就是T。（3） 否则，将对操作数类型进行二进制数字提升，而条件表达式的类型就是第二个和第三个操作数被提升之后的类型。将final修饰符用于i的声明可以把i转变为一个常量表达式。在条件表达式中使用类型相同的第二个和第三个操作数。 半斤12x += i;x = x + i; 复合赋值E1 op= E2等价于简单赋值E1=(T)((E1)op(E2))，其中T是E1的类型。复合赋值表达式自动将所执行计算的结果转型为其左侧变量的类型。请不要将复合赋值操作符作用于byte、short或char类型的变量。在将复合赋值操作符作用于int类型的变量上时，要确保表达式右侧不是long、float或double类型。在将复合赋值操作符作用于float类型的变量上时，要确保表达式右侧不是double类型。 八两12x = x + i;x += i; 如果在+=操作符左侧的操作数是String类型的，那么它允许右侧的操作数是任意类型。简单赋值操作符（=）允许其左侧的是对象引用类型，只要表达式的右侧与左侧的变量时赋值兼容的即可。 第3章 字符之谜 最后的笑声123456public class LastLaugh &#123; public static void main(String[] args) &#123; System.out.println("H" + "a"); System.out.println('H' + 'a'); &#125;&#125; 当且仅当+操作符的操作数中至少有一个是String类型时，才会执行字符串连接操作。 System.out.println(“2+2=”+2+2); ABC 1234567public class Abc &#123; public static void main(String[] args) &#123; String letters = "ABC"; char[] numbers = &#123;'1', '2', '3'&#125;; System.out.println(letters + " easy as " + numbers); &#125;&#125; System.out.println(numbers);要想将一个char数组转换成一个字符串，就要调用String.valueOf(char[])方法。 动物庄园1234567public class AnimalFarm &#123; public static void main(String[] args) &#123; final String pig = "length:10"; final String dog = "length:" + pig.length(); System.out.println("Animals are equal:" + pig == dog); &#125;&#125; +比==的优先级高。在使用字符串连接操作符时，总是将重要的操作数用括号括起来。在比较对象引用时，应该优先使用equals方法而不是==操作符，除非需要比较的是对象的标识而不是对象的值。 转义字符的溃败12345public class EscapeRout &#123; public static void main(String[] args) &#123; // \u0022 is the Unicode escape for double quote (") System.out.println("a\u0022.length()+\u0022b".length()); &#125;&#125; Java对在字符串字面常量中的Unicode转义字符没有提供任何特殊处理。在字符串和字符字面常量中优先选择的是转义字符序列，而不是Unicode转义字符。不要使用Unicode转义字符来表示ASCII字符。 令人晕头转向的Hello1234567891011/** * Generated by the IBM IDL-to-Java compiler, version 1.0 * from F:\TestRoot\apps\a1\units\include\PolicyHome.idl * Wednesday, June 17, 1998 6:44:40 o'clock AM GMT+00:00 */public class Test &#123; public static void main(String[] args) &#123; System.out.print("Hell"); System.out.println("o world"); &#125;&#125; 问题在于注释的第三行，它包含了字符\units。Unicode转义字符必须是良构的，即使出现在注释中也是如此。在Javadoc注释中，应该使用HTML实体转义字符来代替Unicode转义字符。工具应该确保不将Windows文件名置于所生成的Java源文件的注释中。 行打印程序1234567public class LinePrinter &#123; public static void main(String[] args) &#123; // Note: \u000A is Unicode representation of linefeed (LF) char c = 0x000A; System.out.println(c); &#125;&#125; 第三行的注释除非确实是必需的，否则就不要用使用Unicode转义字符。 嗯？只有在你要向程序中插入用其他任何方式都无法表示的字符时，Unicode转义字符才是必需的，除此之外的任何情况都应该避免使用它们。 字符串奶酪12345678910public class StringCheese &#123; public static void main(String[] args) &#123; byte[] bytes = new byte[256]; for (int i = 0; i &lt; 256; i++) bytes[i] = (byte) i; String str = new String(bytes); for (int i = 0, n = str.length(); i &lt; n; i++) System.out.print((int) str.charAt(i) + " "); &#125;&#125; 在char序列和byte序列之间转换时，可以且通常应该显式地指定字符集。每当要将一个byte序列转换成一个String时，你都在使用一个字符集，不管是否显式指定了它。 漂亮的火花（块注释符）1234567891011121314151617public class Classifier &#123; public static void main(String[] args) &#123; System.out.println(classify('n') + classify('+') + classify('2')); &#125; static String classify(char ch) &#123; if ("0123456789".indexOf(ch) &gt;= 0) return "NUMERAL "; if ("abcdefghijklmnopqrstuvwxyz".indexOf(ch) &gt;= 0) return "LETTER "; /* (Operators not supported yet) if ("+-*/&amp;|!=".indexOf(ch) &gt;= 0) return "OPERATOR "; */ return "UNKNOWN "; &#125;&#125; 在注释中没有特殊处理字符串字面常量。块注释不能嵌套。注释掉代码段的最好方式是使用单行的注释序列。块注释不能可靠地注释掉代码段。 我的类是什么12345public class Me &#123; public static void main(String[] args) &#123; System.out.println(Me.class.getName().replaceAll(".", "/") + ".class"); &#125;&#125; String.replaceAll接受了一个正则表达式作为它的第一个参数。正则表达式”.”可以匹配任何单个的字符。12Me.class.getName().replaceAll("\\.", "/");System.out.println(Me.class.getName().replaceAll(Pattern.quote("."), "/") + ".class"); 我的类是什么？镜头21234567public class MeToo &#123; public static void main(String[] args) &#123; System.out.println(MeToo.class.getName().replaceAll("\\.", File.separator) + ".class"); &#125;&#125; 第二个参数不是一个普通的字符串，而是一个替代字符串。在替代字符串中出现的反斜杠会把紧随其后的字符进行转义，从而导致其被按字面含义而处理了。12345System.out.println(MeToo.class.getName().replaceAll("\\.", Matcher.quoteReplacement(File.separator)) + ".class");System.out.println(MeToo.class.getName().replace(".", File.separator) + ".class");System.out.println(MeToo.class.getName().replace('.', File.separatorChar) + ".class"); 在使用不熟悉的类库方法时一定要格外小心。 URL的愚弄1234567public class BrowserTest &#123; public static void main(String[] args) &#123; System.out.print("iexplore:"); http: // www.google.com; System.out.println(":maximize"); &#125;&#125; 在程序中间出现的URL是一个语句标号（statement label）后面跟着尾注释（end-of-line comment）。仔细地写注释，并让它们跟上时代；去除那些已遭废弃的代码。 不劳而获12345678910111213141516171819public class Rhymes &#123; private static Random rnd = new Random(); public static void main(String[] args) &#123; StringBuffer word = null; switch (rnd.nextInt(2)) &#123; case 1: word = new StringBuffer('P'); case 2: word = new StringBuffer('G'); default: word = new StringBuffer('M'); &#125; word.append('a'); word.append('i'); word.append('n'); System.out.println(word); &#125;&#125; 要当心栅栏柱错误。不要从一个非空的case向下进入另一个case。不管在什么时候，都要尽可能使用熟悉的惯用法和API。如果必须使用不熟悉的API，那么请仔细阅读其文档。1234567891011121314151617181920212223242526 switch (rnd.nextInt(3)) &#123; case 1: word = new StringBuffer("P"); break; case 2: word = new StringBuffer("G"); break; default: word = new StringBuffer("M"); break; &#125;System.out.println("PGM".charAt(rnd.nextInt(3)) + "ain");public class Rhymes &#123; public static void main(String args[]) &#123; String a[] = &#123; "Main", "Pain", "Gain" &#125;; System.out.println(randomElement(a)); &#125; private static Random rnd = new Random(); private static String randomElement(String[] a) &#123; return a[rnd.nextInt(a.length)]; &#125;&#125; char不是String，而是更像int。要提防各种诡异的谜题。 第4章 循环之谜 尽情享受每一个字节123456789101112131415public class BigDelight &#123; public static void main(String[] args) &#123; for (byte b = Byte.MIN_VALUE; b &lt; Byte.MAX_VALUE; b++) &#123; if (b == 0x90) System.out.print("Joy!"); &#125; &#125;&#125;if (b == (byte)0x90) System.out.print("Joy!");if ((b &amp; 0xff) == 0x90) System.out.print("Joy!");private static final byte TARGET = (byte) 0x90; if (b == TARGET) System.out.print("Joy!"); 要避免混合类型比较，因为它们内在地容易引起混乱。请使用声明的常量替代“魔数”。 无情的增量操作12345678public class Increment &#123; public static void main(String[] args) &#123; int j = 0; for (int i = 0; i &lt; 100; i++) j = j++; System.out.println(j); &#125;&#125; 不要在单个表达式中对相同的变量赋值超过一次。 在循环中12345678910111213public class InTheLoop &#123; public static final int END = Integer.MAX_VALUE; public static final int START = END - 100; public static void main(String[] args) &#123; int count = 0; for (int i = START; i &lt;= END; i++) count++; System.out.println(count); &#125;&#125;for (long i = START; i &lt;= END; i++) 无论你在何时使用了一个整数类型，都要意识到其边界条件。1234int i = START;do &#123; count++;&#125; while (i++ != END); 变幻莫测的i值12345678public class Shifty &#123; public static void main(String[] args) &#123; int i = 0; while(-1 &lt;&lt; i != 0) i++; System.out.println(i); &#125;&#125; 移位操作符只使用其右操作数的低5位作为移位长度。或者是低6位，如果其左操作数是一个long类型数值。 就 Gf D G G G G G第5章 异常之谜 F F F F F F F F F F 第6章 类之谜 令人混淆的构造器案例12345678910111213public class Confusing &#123; private Confusing(Object o) &#123; System.out.println("Object"); &#125; private Confusing(double[] dArray) &#123; System.out.println("double array"); &#125; public static void main(String[] args) &#123; new Confusing(null); &#125;&#125; Java的重载解析过程是分两阶段运行的。第一阶段选取所有可获得并且可应用的方法或构造器。第二阶段在第一阶段选取的方法或构造器中选取最精确的一个。在测试哪一个方法或构造器最精确时，并没有使用实参。要想强制要求编译器选择一个精确的重载版本，需要将实参转型为形参所声明的类型。避免使用重载。 啊呀！狸猫变犬子123456789101112131415161718192021222324252627282930313233343536373839404142class Counter &#123; private static int count; public static void increment() &#123; count++; &#125; public static int getCount() &#123; return count; &#125;&#125;class Dog extends Counter &#123; public Dog() &#123; &#125; public void woof() &#123; increment(); &#125;&#125;class Cat extends Counter &#123; public Cat() &#123; &#125; public void meow() &#123; increment(); &#125;&#125;public class Ruckus &#123; public static void main(String[] args) &#123; Dog[] dogs = &#123; new Dog(), new Dog() &#125;; for (int i = 0; i &lt; dogs.length; i++) dogs[i].woof(); Cat[] cats = &#123; new Cat(), new Cat(), new Cat() &#125;; for (int i = 0; i &lt; cats.length; i++) cats[i].meow(); System.out.print(Dog.getCount() + " woofs and "); System.out.println(Cat.getCount() + " meows"); &#125;&#125; 每一个静态字段在声明它的类及其所有子类中共享一份单一的副本。优选组合而不是继承。 我所得到的都是静态的12345678910111213141516171819class Dog &#123; public static void bark() &#123; System.out.print("woof "); &#125;&#125;class Basenji extends Dog &#123; public static void bark() &#123; &#125;&#125;public class Bark &#123; public static void main(String args[]) &#123; Dog woofer = new Dog(); Dog nipper = new Basenji(); woofer.bark(); nipper.bark(); &#125;&#125; 对静态方法的调用不存在任何动态的分派机制。千万不要用一个表达式来标识一个静态方法调用。千万不要隐藏静态方法。 比生命更大12345678910111213141516171819public class Elvis &#123; public static final Elvis INSTANCE = new Elvis(); private final int beltSize; private static final int CURRENT_YEAR = Calendar.getInstance().get( Calendar.YEAR); private Elvis() &#123; beltSize = CURRENT_YEAR - 1930; &#125; public int beltSize() &#123; return beltSize; &#125; public static void main(String[] args) &#123; System.out.println("Elvis wears a size " + INSTANCE.beltSize() + " belt."); &#125;&#125; 在final类型的静态字段被初始化之前，存在着读取其值的可能。要想改正一个类初始化循环，需要重新对静态字段的初始器进行排序，使得每一个初始器都出现在任何依赖于它的初始器之前。要当心类初始化循环。 不是你的类型123456public class Type1 &#123; public static void main(String[] args) &#123; String s = null; System.out.println(s instanceof String); &#125;&#125; instanceof操作符被定义为在其左操作数为null时返回false。12345public class Type2 &#123; public static void main(String[] args) &#123; System.out.println(new Type2() instanceof String); &#125;&#125; instanceof操作符有这样的要求：如果两个操作数的类型都是类，其中一个必须是另一个的子类型。12345public class Type3 &#123; public static void main(String args[]) &#123; Type3 t3 = (Type3) new Object(); &#125;&#125; 要点何在1234567891011121314151617181920212223242526272829303132333435class Point &#123; private final int x, y; private final String name; // Cached at construction time Point(int x, int y) &#123; this.x = x; this.y = y; name = makeName(); &#125; protected String makeName() &#123; return "[" + x + "," + y + "]"; &#125; public final String toString() &#123; return name; &#125;&#125;public class ColorPoint extends Point &#123; private final String color; ColorPoint(int x, int y, String color) &#123; super(x, y); this.color = color; &#125; protected String makeName() &#123; return super.makeName() + ":" + color; &#125; public static void main(String[] args) &#123; System.out.println(new ColorPoint(4, 2, "purple")); &#125;&#125; 在一个final类型的实例字段被赋值之前，存在着取用其值的可能。循环的实例初始化时可以且总是应该避免的。千万不要在构造器中调用可覆写的方法。 总和的玩笑123456789101112131415161718192021222324252627class Cache &#123; static &#123; initializeIfNecessary(); &#125; private static int sum; public static int getSum() &#123; initializeIfNecessary(); return sum; &#125; private static boolean initialized = false; private static synchronized void initializeIfNecessary() &#123; if (!initialized) &#123; for (int i = 0; i &lt; 100; i++) sum += i; initialized = true; &#125; &#125;&#125;public class Client &#123; public static void main(String[] args) &#123; System.out.println(Cache.getSum()); &#125;&#125; 要么使用积极初始化，要么使用延迟初始化，千万不要同时使用二者。请考虑类初始化的顺序，特别是当初始化显得很重要时更是如此。 做你的事吧 Null与Void123456789public class Null &#123; public static void greet() &#123; System.out.println("Hello world!"); &#125; public static void main(String[] args) &#123; ((Null) null).greet(); &#125;&#125; 静态方法调用的限定表达式是可以计算的，但是它的值将被忽略。12Null.greet();greet(); 特创论12345678910111213141516171819public class Creator &#123; public static void main(String[] args) &#123; for (int i = 0; i &lt; 100; i++) Creature creature = new Creature(); System.out.println(Creature.numCreated()); &#125;&#125;class Creature &#123; private static long numCreated = 0; public Creature() &#123; numCreated++; &#125; public static long numCreated() &#123; return numCreated; &#125;&#125; 一个局部变量声明作为一条语句只能直接出现在一个语句块中。（一个语句块是由一对花括号以及包含在这对花括号的语句和声明构成的）123456789101112131415161718192021222324252627282930313233 for (int i = 0; i &lt; 100; i++) &#123; Creature creature = new Creature(); &#125; for (int i = 0; i &lt; 100; i++) new Creature();// Thread-safe creation counterclass Creature &#123; private static long numCreated; public Creature() &#123; synchronized (Creature.class) &#123; numCreated++; &#125; &#125; public static synchronized long numCreated() &#123; return numCreated; &#125;&#125;class Creature &#123; private static AtomicLong numCreated = new AtomicLong(); public Creature() &#123; numCreated.incrementAndGet(); &#125; public static long numCreated() &#123; return numCreated.get(); &#125;&#125; 在使用一个变量来对实例的创建进行计数时，要使用long类型而不是int类型的变量，以防止溢出。 第7章 库之谜 大问题123456789101112public class BigProblem &#123; public static void main(String[] args) &#123; BigInteger fiveThousand = new BigInteger("5000"); BigInteger fiftyThousand = new BigInteger("50000"); BigInteger fiveHundredThousand = new BigInteger("500000"); BigInteger total = BigInteger.ZERO; total.add(fiveThousand); total.add(fiftyThousand); total.add(fiveHundredThousand); System.out.println(total); &#125;&#125; BigInteger实例是不可变的。123total = total.add(fiveThousand);total = total.add(fiftyThousand);total = total.add(fiveHundredThousand); 名字里有什么123456789101112131415161718192021public class Name &#123; private final String first, last; public Name(String first, String last) &#123; this.first = first; this.last = last; &#125; public boolean equals(Object o) &#123; if (!(o instanceof Name)) return false; Name n = (Name) o; return n.first.equals(first) &amp;&amp; n.last.equals(last); &#125; public static void main(String[] args) &#123; Set&lt;Name&gt; s = new HashSet&lt;Name&gt;(); s.add(new Name("Mickey", "Mouse")); System.out.println(s.contains(new Name("Mickey", "Mouse"))); &#125;&#125; 无论何时，只要覆写了equals方法，就必须同时覆写hashCode方法。 产生它的散列码12345678910111213141516171819202122public class Name &#123; private final String first, last; public Name(String first, String last) &#123; this.first = first; this.last = last; &#125; public boolean equals(Name n) &#123; return n.first.equals(first) &amp;&amp; n.last.equals(last); &#125; public int hashCode() &#123; return 31 * first.hashCode() + last.hashCode(); &#125; public static void main(String[] args) &#123; Set&lt;Name&gt; s = new HashSet&lt;Name&gt;(); s.add(new Name("Donald", "Duck")); System.out.println(s.contains(new Name("Donald", "Duck"))); &#125;&#125; 重载为错误和混乱提供了机会。为了避免无意识地重载，应该机械地对你想要覆写的每一个超类方法都复制其声明。 差是什么以0开头的整型字面常量将被解释成为八进制数值。千万不要在一个整型字面常量前面加上一个0。 一行以毙之了解类库中有些什么可以为你节省大量的时间和精力，并且可以提高程序的速度和质量。 日期游戏Date将一月表示为0，而Calendar延续了这个错误。Date.getDay返回的是Date实例所表示的星期日期，而不是月份日期。在使用Calendar或Date的时候一定要当心，千万要记着查阅API文档。 名字游戏不要使用IdentityHashMap，除非你需要其基于标识的语义，它不是一个通用目的的Map实现。 更多同样的问题不要因为偶然地添加了一个返回类型，而将一个构造器声明变成了一个方法声明。要遵守标准的命名约定。 按余数编组Math.abs不能保证一定会返回非负的结果。 疑似配需的惊人传奇不要使用基于减法的比较器，除非你能够确保要比较的数值之间的差永远不会大于Integer.MAX_VALUE。第8章 更多类之谜 一件私事12345678910111213class Base &#123; public String className = "Base";&#125;class Derived extends Base &#123; private String className = "Derived";&#125;public class PrivateMatter &#123; public static void main(String[] args) &#123; System.out.println(new Derived().className); &#125;&#125; 避免隐藏。一个覆写方法的访问修饰符所提供的访问权限要大于等于被覆写方法的访问修饰符所提供的访问权限。而字段则不必。 对字符串上瘾要避免重用平台类的名字，并且千万不要难过重用java.lang中的类名。 灰色的阴影1234567891011121314151617public class ShadesOfGray &#123; public static void main(String[] args) &#123; System.out.println(X.Y.Z); &#125;&#125;class X &#123; static class Y &#123; static String Z = "Black"; &#125; static C Y = new C();&#125;class C &#123; String Z = "White";&#125; 当一个变量和一个类型具有相同的名字，并且它们位于相同的作用域时，变量名具有优先权。相似地，变量名和类型名可以遮掩包名。 黑色的渐隐 一揽子交易12345678910111213141516171819202122232425package click;public class CodeTalk &#123; public void doIt() &#123; printMessage(); &#125; void printMessage() &#123; System.out.println("Click"); &#125;&#125;package hack;import click.CodeTalk;public class TypeIt &#123; private static class ClickIt extends CodeTalk &#123; void printMessage() &#123; System.out.println("Hack"); &#125; &#125; public static void main(String[] args) &#123; ClickIt clickit = new ClickIt(); clickit.doIt(); &#125;&#125; 一个包内私有的方法不能被位于另一个包中的某个方法直接覆写。 进口税 终极危难final修饰符对方法和字段而言，意味着某些完全不同的事情。对于方法，final意味着该方法不能被覆写（对实例方法而言）或者隐藏（对静态方法而言）。对于字段，final意味着该字段不能被赋值超过一次。 隐私在公开重用名字是危险的；应该避免隐藏、遮蔽和遮掩。 同一性的危机 头还是尾名字重用的术语表第9章 更多库之谜 乒乓123456789101112131415public class PingPong &#123; public static synchronized void main(String[] a) &#123; Thread t = new Thread() &#123; public void run() &#123; pong(); &#125; &#125;; t.run(); System.out.print("Ping"); &#125; static synchronized void pong() &#123; System.out.print("Pong"); &#125;&#125; 当你想调用一个线程的start方法时要多加小心，别弄错成调用这个线程的run方法了。 乱锁之妖 反射的影响123456789public class Reflector &#123; public static void main(String[] args) throws Exception &#123; Set&lt;String&gt; s = new HashSet&lt;String&gt;(); s.add("foo"); Iterator it = s.iterator(); Method m = it.getClass().getMethod("hasNext"); System.out.println(m.invoke(it)); &#125;&#125; 访问位于其它包中的非公共类型的成员是不合法的。Object.getClass().getMethod(“methodName”)这种惯用法虽然很常见，但是却有问题，它不应该被使用。在使用反射访问某个类型时，请使用表示某种可访问类型的Class对象。Method m = Iterator.class.getMethod(“hasNext”); 狗狗的幸福生活避免遮蔽。使用Thread(Runnable)构造器来替代对Thread的继承。 更深层的反射123456789101112131415public class Outer &#123; public static void main(String[] args) throws Exception &#123; new Outer().greetWorld(); &#125; private void greetWorld() throws Exception &#123; System.out.println(Inner.class.newInstance()); &#125; public class Inner &#123; public String toString() &#123; return "Hello world"; &#125; &#125;&#125; 除非你确实需要一个外围实例i，否则应该优先使用静态成员类而不是非静态成员类。请避免使用反射类实例化内部类。 无法识别的字符化1234567public class Greeter &#123; public static void main(String[] args) &#123; String greeting = "Hello world"; for (int i = 0; i &lt; greeting.length(); i++) System.out.write(greeting.charAt(i)); &#125;&#125; write(int)是唯一一个在自动刷新功能开启的情况下不刷新PrintStream的输出方法。尽可能使用熟悉的惯用法，如果不得不使用陌生的API，请一定要参考相关的文档。 啤酒爆炸 诵读困难者的一神论 戛然而止12345678910public class SelfInterruption &#123; public static void main(String[] args) &#123; Thread.currentThread().interrupt(); if (Thread.interrupted()) &#123; System.out.println("Interrupted: " + Thread.interrupted()); &#125; else &#123; System.out.println("Not interrupted: " + Thread.interrupted()); &#125; &#125;&#125; 调用Thread.interrupted方法总是会清除当前线程的中断状态。Thread.currentThread().isInterrupted()不要使用Thread.interrupted方法，除非你想要清除当前线程的中断状态。 延迟初始化]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA Default Keymap]]></title>
    <url>%2F2019%2F04%2F16%2FIDE%2FIDEA%2FIntelliJ%20IDEA%20Default%20Keymap%2F</url>
    <content type="text"><![CDATA[IntelliJ IDEA Default Keymap]]></content>
      <categories>
        <category>IDEA</category>
      </categories>
  </entry>
</search>
